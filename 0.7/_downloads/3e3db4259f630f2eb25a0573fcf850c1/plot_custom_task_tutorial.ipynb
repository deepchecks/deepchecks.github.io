{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Custom Task Tutorial\n====================\n\nComputer vision is an umbrella term for a wide spectrum of objectives\nmodels are trained for. These objective reflect on the structure of the\ndata and the possible actions on it.\n\nThe first step before running any Deepchecks checks is to create an\nimplementation of\n`VisionData <vision_data.VisionData>`{.interpreted-text role=\"class\"}.\nEach implementation represents and standardize a computer vision task\nand allows to run a more complex checks which relates to the given\ntask\\'s characteristics. There are default base classes for a few known\ntasks like object detection and classification, however not all tasks\nhave a base implementation, meaning you will have to create your own\ntask.\n\nWhen creating your own task you will be limited to run checks which are\nagnostic to the specific task type. For example performance checks that\nuses IOU works only on object detection tasks, since they need to know\nthe exact bounding box format in order to run, while other checks that\nuses `/user-guide/vision/vision_properties`{.interpreted-text\nrole=\"doc\"} or custom metrics are agnostic to the task type.\n\nIn this guide we will implement a custom instance segmentation task and\nrun checks on it.\n\n1.  [Defining the Data](#defining-the-data)\n2.  [Implement Custom Task](#implement-custom-task)\n3.  [Implement Custom Properties](#implement-custom-properties)\n4.  [Implement Custom Metric](#implement-custom-metric)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the Data\n=================\n\nFirst we will define a [PyTorch\nDataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\nof COCO-128 segmentation task. This part represents your own code, and\nis not yet Deepchecks related.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import contextlib\nimport os\nimport typing as t\nfrom pathlib import Path\n\nimport albumentations as A\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torchvision.transforms.functional as F\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom PIL import Image, ImageDraw\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import VisionDataset\nfrom torchvision.datasets.utils import download_and_extract_archive\nfrom torchvision.utils import draw_segmentation_masks\n\n\nclass CocoSegmentDataset(VisionDataset):\n    \"\"\"An instance of PyTorch VisionData the represents the COCO128-segments dataset.\n\n    Parameters\n    ----------\n    root : str\n        Path to the root directory of the dataset.\n    name : str\n        Name of the dataset.\n    train : bool\n        if `True` train dataset, otherwise test dataset\n    transforms : Callable, optional\n        A function/transform that takes in an PIL image and returns a transformed version.\n        E.g, transforms.RandomCrop\n    \"\"\"\n\n    TRAIN_FRACTION = 0.5\n\n    def __init__(\n            self,\n            root: str,\n            name: str,\n            train: bool = True,\n            transforms: t.Optional[t.Callable] = None,\n    ) -> None:\n        super().__init__(root, transforms=transforms)\n\n        self.train = train\n        self.root = Path(root).absolute()\n        self.images_dir = Path(root) / 'images' / name\n        self.labels_dir = Path(root) / 'labels' / name\n\n        images: t.List[Path] = sorted(self.images_dir.glob('./*.jpg'))\n        labels: t.List[t.Optional[Path]] = []\n\n        for image in images:\n            label = self.labels_dir / f'{image.stem}.txt'\n            labels.append(label if label.exists() else None)\n\n        assert len(images) != 0, 'Did not find folder with images or it was empty'\n        assert not all(l is None for l in labels), 'Did not find folder with labels or it was empty'\n\n        train_len = int(self.TRAIN_FRACTION * len(images))\n\n        if self.train is True:\n            self.images = images[0:train_len]\n            self.labels = labels[0:train_len]\n        else:\n            self.images = images[train_len:]\n            self.labels = labels[train_len:]\n\n    def __getitem__(self, idx: int) -> t.Tuple[Image.Image, np.ndarray]:\n        \"\"\"Get the image and label at the given index.\"\"\"\n        image = Image.open(str(self.images[idx]))\n        label_file = self.labels[idx]\n\n        masks = []\n        classes = []\n        if label_file is not None:\n            for label_str in label_file.open('r').read().strip().splitlines():\n                label = np.array(label_str.split(), dtype=np.float32)\n                class_id = int(label[0])\n                # Transform normalized coordinates to un-normalized\n                coordinates = (label[1:].reshape(-1, 2) * np.array([image.width, image.height])).reshape(-1).tolist()\n                # Create mask image\n                mask = Image.new('L', (image.width, image.height), 0)\n                ImageDraw.Draw(mask).polygon(coordinates, outline=1, fill=1)\n                # Add to list\n                masks.append(np.array(mask, dtype=bool))\n                classes.append(class_id)\n\n        if self.transforms is not None:\n            # Albumentations accepts images as numpy\n            transformed = self.transforms(image=np.array(image), masks=masks)\n            image = transformed['image']\n            masks = transformed['masks']\n            # Transform masks to tensor of (num_masks, H, W)\n            if masks:\n                masks = torch.stack([torch.from_numpy(m) for m in masks])\n            else:\n                masks = torch.empty((0, 3))\n\n        return image, classes, masks\n\n    def __len__(self):\n        return len(self.images)\n\n    @classmethod\n    def load_or_download(cls, root: Path, train: bool) -> 'CocoSegmentDataset':\n        coco_dir = root / 'coco128'\n        folder = 'train2017'\n\n        if not coco_dir.exists():\n            url = 'https://ultralytics.com/assets/coco128-segments.zip'\n            md5 = 'e29ec06014d1e06b58b6ffe651c0b34f'\n\n            with open(os.devnull, 'w', encoding='utf8') as f, contextlib.redirect_stdout(f):\n                download_and_extract_archive(\n                    url,\n                    download_root=str(root),\n                    extract_root=str(root),\n                    md5=md5\n                )\n            \n            try:\n                # remove coco128's README.txt so that it does not come in docs\n                os.remove(\"coco128/README.txt\")\n            except:\n                pass\n        return CocoSegmentDataset(coco_dir, folder, train=train, transforms=A.Compose([ToTensorV2()]))\n\n\n# Download and load the datasets\ncurr_dir = Path('.')\ntrain_ds = CocoSegmentDataset.load_or_download(curr_dir, train=True)\ntest_ds = CocoSegmentDataset.load_or_download(curr_dir, train=False)\n\n\ndef batch_collate(batch):\n    \"\"\"Function which gets list of samples from `CocoSegmentDataset` and combine them to a batch.\"\"\"\n    images, classes, masks = zip(*batch)\n    return list(images), list(classes), list(masks)\n\n\n# Create DataLoaders\ntrain_data_loader = DataLoader(\n    dataset=train_ds,\n    batch_size=32,\n    shuffle=False,\n    collate_fn=batch_collate\n)\n\ntest_data_loader = DataLoader(\n    dataset=test_ds,\n    batch_size=32,\n    shuffle=False,\n    collate_fn=batch_collate\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizing that we loaded our datasets correctly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "masked_images = [draw_segmentation_masks(train_ds[i][0], masks=train_ds[i][2], alpha=0.7)\n                 for i in range(5)]\n\nfix, axs = plt.subplots(ncols=len(masked_images), figsize=(20, 20))\nfor i, img in enumerate(masked_images):\n    img = img.detach()\n    img = F.to_pil_image(img)\n    axs[i].imshow(np.asarray(img))\n    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\nfix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implement Custom Task\n=====================\n\nWith our data and model ready we can write the task class.\n\nWith the built-in base classes, the checks are accessing directly the\nvalues which return from the functions [infer\\_on\\_batch]{.title-ref}\nand [batch\\_to\\_labels]{.title-ref} and therefore they require a\nstandard format. But with custom task, these functions\\' values are not\nused directly, so we can just return our own data as is. On the other\nhand the functions [batch\\_to\\_images]{.title-ref} and\n[get\\_classes]{.title-ref} are used so we will need to make sure our\ndata is in the expected format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import List, Sequence\n\nfrom deepchecks.vision import VisionData\n\n\nclass MyCustomSegmentationData(VisionData):\n    \"\"\"Class for loading the COCO segmentation dataset.\"\"\"\n\n    def get_classes(self, batch_labels) -> List[List[int]]:\n        \"\"\"Return per label a list of classes (by id) in it.\"\"\"\n        # The input `batch_labels` is the result of `batch_to_labels` function.\n        return batch_labels[0]\n\n    def batch_to_labels(self, batch):\n        \"\"\"Extract from the batch only the labels. No standard format is needed here.\"\"\"\n        _, classes, masks = batch\n        return classes, masks\n\n    def infer_on_batch(self, batch, model, device):\n        \"\"\"Infer on a batch of images. No standard format is needed here.\"\"\"\n        predictions = model.to(device)(batch[0])\n        return predictions\n\n    def batch_to_images(self, batch) -> Sequence[np.ndarray]:\n        \"\"\"Convert the batch to a list of images as (H, W, C) 3D numpy array per image.\"\"\"\n        return [tensor.numpy().transpose((1, 2, 0)) for tensor in batch[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are able to run checks that use only the image data, since it\\'s\nin the standard Deepchecks format. Let\\'s run\nFeatureLabelCorrelationChange check with our task\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.checks import FeatureLabelCorrelationChange\n\n# Create our task with the `DataLoader`s we defined before.\ntrain_task = MyCustomSegmentationData(train_data_loader)\ntest_task = MyCustomSegmentationData(test_data_loader)\n\nresult = FeatureLabelCorrelationChange().run(train_task, test_task)\nresult\n\n# Now in order to run more check, we'll need to define custom properties or metrics.\n#\n# Implement Custom Properties\n# ===========================\n#\n# In order to run checks that are using label or prediction properties we'll have to implement\n# a custom :doc:`properties </user-guide/vision/vision_properties>`. We'll write label properties and run a label drift\n# check.\n\n\nfrom itertools import chain\n\nfrom deepchecks.vision.checks import TrainTestLabelDrift\n\n# The labels object is the result of `batch_to_labels` function we defined earlier. The property should return a flat\n# list of values.\n\n\ndef number_of_detections(labels) -> List[int]:\n    \"\"\"Return a list containing the number of detections per sample in batch.\"\"\"\n    classes, all_masks = labels\n    return [sample_masks.shape[0] for sample_masks in all_masks]\n\n\ndef classes_in_labels(labels: List[torch.Tensor]) -> List[int]:\n    \"\"\"Return a list containing the classes in batch.\"\"\"\n    classes, all_masks = labels\n    # Flatten list of lists into a single list\n    return list(chain.from_iterable(classes))\n\n\n# We will pass this object as parameter to checks that are using label properties\nlabel_properties = [\n    {'name': '# Detections per Label', 'method': number_of_detections, 'output_type': 'discrete'},\n    {'name': 'Classes in Labels', 'method': classes_in_labels, 'output_type': 'class_id'}\n]\n\n\nresult = TrainTestLabelDrift(label_properties=label_properties).run(train_task, test_task)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implement Custom Metric\n=======================\n\nSome checks test the model performance and requires a metric in order to\nrun. When using a custom task you will also have to create a custom\nmetric in order for those checks to work, since the built-in metrics\ndon\\'t know to handle your data structure. The metrics need to conform\nto the API of [pytorch-ignite](https://pytorch.org/ignite/metrics.html).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}