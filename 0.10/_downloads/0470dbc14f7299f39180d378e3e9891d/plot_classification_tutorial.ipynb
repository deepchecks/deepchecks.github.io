{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classification Model Validation Tutorial {#vision_classification_tutorial}\n========================================\n\nIn this tutorial, you will learn how to validate your **classification\nmodel** using deepchecks test suites. You can read more about the\ndifferent checks and suites for computer vision use cases at the\n`examples section  </checks_gallery/vision>`{.interpreted-text\nrole=\"doc\"}.\n\nA classification model is usually used to classify an image into one of\na number of classes. Although there are multi label use-cases, in which\nthe model is used to classify an image into multiple classes, most\nuse-cases require the model to classify images into a single class.\nCurrently deepchecks supports only single label classification (either\nbinary or multi-class).\n\n``` {.bash}\n# Before we start, if you don't have deepchecks vision package installed yet, run:\nimport sys\n!{sys.executable} -m pip install \"deepchecks[vision]\" --quiet --upgrade # --user\n\n# or install using pip from your python environment\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the data and model\n===========================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Importing the required packages\n\nimport os\nimport urllib.request\nimport zipfile\n\nimport albumentations as A\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL.Image\nimport torch\nimport torchvision\nfrom albumentations.pytorch import ToTensorV2\nfrom torch import nn\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\n\nfrom deepchecks.vision.classification_data import ClassificationData"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloading the dataset\n=======================\n\nThe data is available from the torch library. We will download and\nextract it to the current directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = 'https://download.pytorch.org/tutorial/hymenoptera_data.zip'\nurllib.request.urlretrieve(url, 'hymenoptera_data.zip')\n\nwith zipfile.ZipFile('hymenoptera_data.zip', 'r') as zip_ref:\n    zip_ref.extractall('.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Data\n=========\n\nWe will use torchvision and torch.utils.data packages for loading the\ndata. The model we are building will learn to classify **ants** and\n**bees**. We have about 120 training images each for ants and bees.\nThere are 75 validation images for each class. This dataset is a very\nsmall subset of imagenet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class AntsBeesDataset(ImageFolder):\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Overrides initialization method to replace default loader with OpenCV loader\n        :param args:\n        :param kwargs:\n        \"\"\"\n        super(AntsBeesDataset, self).__init__(*args, **kwargs)\n\n    def __getitem__(self, index: int):\n        \"\"\"\n        overrides __getitem__ to be compatible to albumentations\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (sample, target) where target is class_index of the target class.\n        \"\"\"\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        sample = self.get_cv2_image(sample)\n        if self.transforms is not None:\n            transformed = self.transforms(image=sample, target=target)\n            sample, target = transformed[\"image\"], transformed[\"target\"]\n        else:\n            if self.transform is not None:\n                sample = self.transform(image=sample)['image']\n            if self.target_transform is not None:\n                target = self.target_transform(target)\n\n        return sample, target\n\n    def get_cv2_image(self, image):\n        if isinstance(image, PIL.Image.Image):\n            image_np = np.array(image).astype('uint8')\n            return image_np\n        elif isinstance(image, np.ndarray):\n            return image\n        else:\n            raise RuntimeError(\"Only PIL.Image and CV2 loaders currently supported!\")\n\n# Just normalization for validation\ndata_transforms = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\ndata_dir = 'hymenoptera_data'\n# Just normalization for validation\ndata_transforms = A.Compose([\n    A.Resize(height=256, width=256),\n    A.CenterCrop(height=224, width=224),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\ntrain_dataset = AntsBeesDataset(root=os.path.join(data_dir,'train'))\ntrain_dataset.transforms = data_transforms\n\nval_dataset = AntsBeesDataset(root=os.path.join(data_dir,'val'))\nval_dataset.transforms = data_transforms\n\ndataloaders = {\n    'train':torch.utils.data.DataLoader(train_dataset, batch_size=4,\n                                                shuffle=True),\n    'val': torch.utils.data.DataLoader(val_dataset, batch_size=4,\n                                                shuffle=True)\n}\n\nclass_names = ['ants', 'bees']\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize a Few Images\n======================\n\nLet\\'s visualize a few training images so as to understand the data\naugmentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloading a pre-trained model\n===============================\n\nNow, we will download a pre-trained model from torchvision, that was\ntrained on the ImageNet dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = torchvision.models.resnet18(pretrained=True)\nnum_ftrs = model.fc.in_features\n# We have only 2 classes\nmodel.fc = nn.Linear(num_ftrs, 2)\nmodel = model.to(device)\n_ = model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validating the Model with Deepchecks\n====================================\n\nNow, after we have the training data, validation data and the model, we\ncan validate the model with deepchecks test suites.\n\nVisualize the data loader and the model outputs\n-----------------------------------------------\n\nFirst we\\'ll make sure we are familiar with the data loader and the\nmodel outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch = next(iter(dataloaders['train']))\n\nprint(\"Batch type is: \", type(batch))\nprint(\"First element is: \", type(batch[0]), \"with len of \", len(batch[0]))\nprint(\"Example output of an image shape from the dataloader \", batch[0][0].shape)\nprint(\"Image values\", batch[0][0])\nprint(\"-\"*80)\n\nprint(\"Second element is: \", type(batch[1]), \"with len of \", len(batch[1]))\nprint(\"Example output of a label shape from the dataloader \", batch[1][0].shape)\nprint(\"Image values\", batch[1][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementing the ClassificationData class\n=========================================\n\nThe first step is to implement a class that enables deepchecks to\ninteract with your model and data. The appropriate class to implement\nshould be selected according to you models task type. In this tutorial,\nwe will implement the classification task type by implementing a class\nthat inherits from the\n`deepchecks.vision.classification_data.ClassificationData`{.interpreted-text\nrole=\"class\"} class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The goal of this class is to make sure the outputs of the model and of the dataloader are in the correct format.\n# To learn more about the expected format please visit the API reference for the\n# :class:`deepchecks.vision.classification_data.ClassificationData` class.\n\nclass AntsBeesData(ClassificationData):\n\n    def __init__(self, *args, **kwargs):\n      super().__init__(*args, **kwargs)\n\n    def batch_to_images(self, batch):\n        \"\"\"\n        Convert a batch of data to images in the expected format. The expected format is an iterable of cv2 images,\n        where each image is a numpy array of shape (height, width, channels). The numbers in the array should be in the\n        range [0, 255]\n        \"\"\"\n        inp = batch[0].detach().numpy().transpose((0, 2, 3, 1))\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n        inp = std * inp + mean\n        inp = np.clip(inp, 0, 1)\n        return inp*255\n\n    def batch_to_labels(self, batch):\n        \"\"\"\n        Convert a batch of data to labels in the expected format. The expected format is a tensor of shape (N,),\n        where N is the number of samples. Each element is an integer representing the class index.\n        \"\"\"\n        return batch[1]\n\n    def infer_on_batch(self, batch, model, device):\n        \"\"\"\n        Returns the predictions for a batch of data. The expected format is a tensor of shape (N, n_classes),\n        where N is the number of samples. Each element is an array of length n_classes that represent the probability of\n        each class.\n        \"\"\"\n        logits = model.to(device)(batch[0].to(device))\n        return nn.Softmax(dim=1)(logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After defining the task class, we can validate it by running the\nfollowing code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "LABEL_MAP = {\n    0: 'ants',\n    1: 'bees'\n  }\ntraining_data = AntsBeesData(data_loader=dataloaders[\"train\"], label_map=LABEL_MAP)\nval_data = AntsBeesData(data_loader=dataloaders[\"val\"], label_map=LABEL_MAP)\n\ntraining_data.validate_format(model)\nval_data.validate_format(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And observe the output:\n\nRunning Deepchecks\\' suite on our data and model!\n=================================================\n\nNow that we have defined the task class, we can validate the train and\ntest data with deepchecks\\' train test validation suite. This can be\ndone with this simple few lines of code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.suites import train_test_validation\n\nsuite = train_test_validation()\nresult = suite.run(training_data, val_data, model, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also have suites for:\n`data integrity <deepchecks.vision.suites.default_suites.data_integrity>`{.interpreted-text\nrole=\"func\"} - validating a single dataset and\n`model evaluation <deepchecks.vision.suites.default_suites.model_evaluation>`{.interpreted-text\nrole=\"func\"} -evaluating the model\\'s performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observing the results:\n======================\n\nThe results can be saved as a html file with the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.save_as_html('output.html')\n\n# Or displayed in a new window in an IDE like Pycharm:\n# result.show_in_window()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or, if working inside a notebook, the output can be displayed directly\nby simply printing the result object:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}