{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating a Custom Check {#vision__custom_check}\n=======================\n\nDeepchecks offers a wide range of checks for computer vision problems,\naddressing distribution issues, performance checks and more.\nNevertheless, in order to fully validate your ML pipeline you often need\nto write your own checks. This guide will walk you through the basics of\nwriting your own checks.\n\nWe recommend writing a single check for each aspect of the model or data\nyou would like to validate. As explained in\n`general__deepchecks_hierarchy`{.interpreted-text role=\"ref\"}, the role\nof the check is to run the logic and output a display and a pythonic\nvalue. Then, a condition can be defined on that value to determine if\nthe check is successful or not.\n\n1.  [Vision Checks Structure](#vision-checks-structure)\n2.  [Write a Basic Check](#write-a-basic-check)\n3.  [Check Display](#check-display)\n4.  [Defining a Condition](#defining-a-condition)\n5.  [Base Checks Types](#base-checks-types)\n6.  `vision__custom_check_templates`{.interpreted-text role=\"ref\"}\n\nVision Checks Structure\n-----------------------\n\nThe first step when writing a vision check is to decide what check base\nclass to use. You can read more in the [Base Checks\nTypes](#base-checks-types) section. In this case, we wish to compare\ntrain and test dataset, so we select the\n`deepchecks.core.checks.TrainTestBaseCheck`{.interpreted-text\nrole=\"class\"}. This type of check must implement the following three\nmethods:\n\n-   initialize\\_run - Actions to be performed before starting to iterate\n    over the dataloader batches.\n-   update - Actions to be performed on each batch.\n-   compute - Actions to be performed after iterating over all the\n    batches. Returns the check display and the return value.\n\nWhile [ModelOnlyCheck]{.title-ref} alone do not implement the update\nmethod. Apart from that, the check init should recipient and handle\ncheck parameters.\n\nWrite a Basic Check\n-------------------\n\nLet\\'s implement a simple check, comparing the average of each color\nchannel between the train and the test dataset.\n\nWe\\'ll start by writing the simplest possible example, returning only a\ndict of the color averages. We\\'ll use external functions when\nimplementing the check in order to be able to reuse them later.\n\n**Good to know: the return value of a check can be any object, a number,\ndictionary, string, etc...**\n\n### The Context and Batch Objects\n\nThe three methods of the vision check - initialize\\_run, update and\ncompute, make use of the Context object and the Batch object.\n\nThe context object contains the basic objects deepchecks uses - the\ntrain and test [VisionData](/user-guide/vision/VisionData) objects, and\nthe use model itself. The Batch objects contains processed data from the\ndataloader, such as the images, labels and model predictions. For some\nchecks, such as the one shown in this example, the Context object is not\nneeded.\n\nFor more examples of using the Context and Batch objects for different\ntypes of base checks, see the\n`vision__custom_check_templates`{.interpreted-text role=\"ref\"} guide.\n\n### Check Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import typing as t\n\nimport numpy as np\n\nfrom deepchecks.core.check_result import CheckResult\nfrom deepchecks.core.checks import DatasetKind\nfrom deepchecks.core.condition import ConditionCategory\nfrom deepchecks.vision.base_checks import TrainTestCheck\nfrom deepchecks.vision.context import Context\nfrom deepchecks.vision.vision_data.batch_wrapper import BatchWrapper\n\n\ndef init_color_averages_dict() -> t.Dict[str, np.array]:\n    \"\"\"Initialize the color averages dicts.\"\"\"\n    return {\n            DatasetKind.TRAIN.value: np.zeros((3,), dtype=np.float64),\n            DatasetKind.TEST.value: np.zeros((3,), dtype=np.float64),\n    }\n\n\ndef init_pixel_counts_dict() -> t.Dict[str, int]:\n    \"\"\"Initialize the pixel counts dicts.\"\"\"\n    return {\n            DatasetKind.TRAIN.value: 0,\n            DatasetKind.TEST.value: 0,\n    }\n\n\ndef sum_pixel_values(batch: BatchWrapper) -> np.array:\n    \"\"\"Sum the values of all the pixels in the batch, returning a numpy array with an entry per channel.\"\"\"\n    images = batch.original_images\n    return sum(image.sum(axis=(0, 1)) for image in images)  # sum over the batch and pixels\n\n\ndef count_pixels_in_batch(batch: BatchWrapper) -> int:\n    \"\"\"Count the pixels in the batch.\"\"\"\n    return sum((image.shape[0] * image.shape[1] for image in batch.original_images))\n\n\nclass ColorAveragesCheck(TrainTestCheck):\n    \"\"\"Check if the average of each color channel is the same between the train and test dataset.\"\"\"\n\n    def __init__(self, channel_names: t.Tuple[str] = None, **kwargs):\n        \"\"\"Init the check and enable customization of the channel_names.\"\"\"\n        super().__init__(**kwargs)\n        if channel_names is None:\n            self.channel_names = ('R', 'G', 'B')\n\n    def initialize_run(self, context: Context):\n        \"\"\"Initialize the color_averages dict and pixel counter dict.\"\"\"\n        self._color_averages = init_color_averages_dict()\n        self._pixel_count = init_pixel_counts_dict()\n\n    def update(self, context: Context, batch: BatchWrapper, dataset_kind: DatasetKind):\n        \"\"\"Add the batch color counts to the color_averages dict, and update counter.\"\"\"\n\n        self._color_averages[dataset_kind.value] += sum_pixel_values(batch)  # add to the color_averages dict\n        # count the number of pixels we have summed\n        self._pixel_count[dataset_kind.value] += count_pixels_in_batch(batch)\n\n    def compute(self, context: Context):\n        \"\"\"Compute the color averages and return them.\"\"\"\n        # Divide by the number of pixels to get the average pixel value per color channel\n        for dataset_kind in DatasetKind:\n            self._color_averages[dataset_kind.value] /= self._pixel_count[dataset_kind.value]\n        # Return the color averages in a dict by channel name\n        return_value = {d_kind: dict(zip(self.channel_names, color_averages))\n                        for d_kind, color_averages in self._color_averages.items()}\n        return CheckResult(return_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hooray! we just implemented a custom check. Next, we will run it on the\nCOCO128 dataset:\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nIn this tutorial, we use the pytorch to create the dataset and model. To\nsee how this can be done using tensorflow or other frameworks, please\nvisit the `vision__vision_data_class`{.interpreted-text role=\"ref\"}\nguide.\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.datasets.detection.coco_torch import load_dataset\n\ntrain_ds = load_dataset(train=True, object_type='VisionData')\ntest_ds = load_dataset(train=False, object_type='VisionData')\n\nresult = ColorAveragesCheck().run(train_ds, test_ds)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our check ran successfully, but we got the print \"Nothing found\". This\nis because we haven't defined to the check anything to display, so the\ndefault behavior is to print \"Nothing found\". In order to access the\nvalue that we have defined earlier we can use the \"value\" property on\nthe result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "favorite checks from our\n`API ref <../../../api/deepchecks.vision>`{.interpreted-text\nrole=\"doc\"}.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check Display\n=============\n\nMost of the times we will want our checks to have a visual display that\nwill quickly summarize the check result. We can pass objects for display\nto the CheckResult. Objects for display should be of type: html string,\ndataframe or a function that plots a graph. Let's define a graph that\nwill be displayed using [Plotly](https://plotly.com/). We will inherit\nfrom the original check to shorten the code an update only the compute\nmethod.\n\n**Good to know: \\`\\`display\\`\\` can receive a single object to display\nor a list of objects**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nimport plotly.express as px\n\n\nclass ColorAveragesCheck(TrainTestCheck):\n    \"\"\"Check if the average of each color channel is the same between the train and test dataset.\"\"\"\n\n    def __init__(self, channel_names: t.Tuple[str] = None, **kwargs):\n        \"\"\"Init the check and enable customization of the channel_names.\"\"\"\n        super().__init__(**kwargs)\n        if channel_names is None:\n            self.channel_names = ('R', 'G', 'B')\n\n    def initialize_run(self, context: Context):\n        \"\"\"Initialize the color_averages dict and pixel counter dict.\"\"\"\n        self._color_averages = init_color_averages_dict()\n        self._pixel_count = init_pixel_counts_dict()\n\n    def update(self, context: Context, batch: BatchWrapper, dataset_kind: DatasetKind):\n        \"\"\"Add the batch color counts to the color_averages dict, and update counter.\"\"\"\n\n        self._color_averages[dataset_kind.value] += sum_pixel_values(batch)  # add to the color_averages dict\n        # count the number of pixels we have summed\n        self._pixel_count[dataset_kind.value] += count_pixels_in_batch(batch)\n\n    def compute(self, context: Context):\n        \"\"\"Compute the color averages and return them. Also display a histogram comparing train and test.\"\"\"\n        # Divide by the number of pixels to get the average pixel value per color channel\n        for dataset_kind in DatasetKind:\n            self._color_averages[dataset_kind.value] /= self._pixel_count[dataset_kind.value]\n        # Return the color averages in a dict by channel name\n        return_value = {d_kind: dict(zip(self.channel_names, color_averages))\n                        for d_kind, color_averages in self._color_averages.items()}\n\n        # **New Code Here**!!!\n        # ========================\n        # Display a histogram comparing train and test\n        color_averages_df = pd.DataFrame(return_value).unstack().reset_index()\n        color_averages_df.columns = ['Dataset', 'Channel', 'Pixel Value']\n        fig = px.histogram(color_averages_df, x='Dataset', y='Pixel Value', color='Channel', barmode='group',\n                           histfunc='avg', color_discrete_sequence=['rgb(255,0,0)', 'rgb(0,255,0)', 'rgb(0,0,255)'],\n                           title='Color Averages Histogram')\n        return CheckResult(return_value, display=[fig])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let check it out:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result = ColorAveragesCheck().run(train_ds, test_ds)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Voil\u00e0! Now we have a check that prints a graph and has a value. We can\nadd this check to any Suite, and it will run within it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining a Condition\n====================\n\nFinally, we can add a condition to our check. A condition is a function\nthat receives the result of the check and returns a condition result\nobject. To read more on conditions, check out the condition [user\nguide](../../user-guide/general/customizations/configure_check_conditions).\nIn this case, we\\'ll define a condition verifying that the color\naverages haven\\'t changed by more than 10%.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.core import ConditionResult\n\n\nclass ColorAveragesCheck(TrainTestCheck):\n    \"\"\"Check if the average of each color channel is the same between the train and test dataset.\"\"\"\n\n    def __init__(self, channel_names: t.Tuple[str] = None, **kwargs):\n        \"\"\"Init the check and enable customization of the channel_names.\"\"\"\n        super().__init__(**kwargs)\n        if channel_names is None:\n            self.channel_names = ('R', 'G', 'B')\n\n    def initialize_run(self, context: Context):\n        \"\"\"Initialize the color_averages dict and pixel counter dict.\"\"\"\n        self._color_averages = init_color_averages_dict()\n        self._pixel_count = init_pixel_counts_dict()\n\n    def update(self, context: Context, batch: BatchWrapper, dataset_kind: DatasetKind):\n        \"\"\"Add the batch color counts to the color_averages dict, and update counter.\"\"\"\n\n        self._color_averages[dataset_kind.value] += sum_pixel_values(batch)  # add to the color_averages dict\n        # count the number of pixels we have summed\n        self._pixel_count[dataset_kind.value] += count_pixels_in_batch(batch)\n\n    def compute(self, context: Context):\n        \"\"\"Compute the color averages and return them. Also display a histogram comparing train and test.\"\"\"\n        # Divide by the number of pixels to get the average pixel value per color channel\n        for dataset_kind in DatasetKind:\n            self._color_averages[dataset_kind.value] /= self._pixel_count[dataset_kind.value]\n        # Return the color averages in a dict by channel name\n        return_value = {d_kind: dict(zip(self.channel_names, color_averages))\n                        for d_kind, color_averages in self._color_averages.items()}\n\n        # Display a histogram comparing train and test\n        color_averages_df = pd.DataFrame(return_value).unstack().reset_index()\n        color_averages_df.columns = ['Dataset', 'Channel', 'Pixel Value']\n        fig = px.histogram(color_averages_df, x='Dataset', y='Pixel Value', color='Channel', barmode='group',\n                           histfunc='avg', color_discrete_sequence=['rgb(255,0,0)', 'rgb(0,255,0)', 'rgb(0,0,255)'],\n                           title='Color Averages Histogram')\n        return CheckResult(return_value, display=[fig])\n\n    # **New Code Here**!!!\n    # ========================\n    def add_condition_color_average_change_not_greater_than(self, change_ratio: float = 0.1) -> ConditionResult:\n        \"\"\"Add a condition verifying that the color averages haven't changed by more than change_ratio%.\"\"\"\n\n        def condition(check_result: CheckResult) -> ConditionResult:\n            failing_channels = []\n            # Iterate over the color averages and verify that they haven't changed by more than change_ratio\n            for channel in check_result.value[DatasetKind.TRAIN.value].keys():\n                if abs(check_result.value[DatasetKind.TRAIN.value][channel] -\n                       check_result.value[DatasetKind.TEST.value][channel]) > change_ratio:\n                    failing_channels.append(channel)\n\n            # If there are failing channels, return a condition result with the failing channels\n            if failing_channels:\n                return ConditionResult(ConditionCategory.FAIL, f'The color averages have changes by more than threshold in the channels'\n                                              f' {failing_channels}.')\n            else:\n                return ConditionResult(ConditionCategory.PASS)\n\n        return self.add_condition(f'Change in color averages not greater than {change_ratio:.2%}', condition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let check it out:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result = ColorAveragesCheck().run(train_ds, test_ds)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now our check we will alert us automatically if the color averages\nhave changed by more than 10%!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Base Checks Types\n=================\n\nVision checks all inherit from one of the following classes:\n\n-   `~deepchecks.vision.base_checks.SingleDatasetCheck`{.interpreted-text\n    role=\"class\"} - Check that runs on a single dataset and model.\n-   `~deepchecks.vision.base_checks.TrainTestCheck`{.interpreted-text\n    role=\"class\"} - Check that runs on a train and test dataset and\n    model.\n-   `~deepchecks.vision.base_checks.ModelOnlyCheck`{.interpreted-text\n    role=\"class\"} - Check that runs on only a model .\n\nAll three classes inherit from the\n`~deepchecks.core.checks.BaseCheck`{.interpreted-text role=\"class\"}\nBaseCheck, same as checks in any other deepchecks subpackage. Each has\nits own run signature, according to the objects on which it will run.\n\nThe first two classes of checks run some logic on the image data, and so\nthe check structure is designed to enable accumulating and computation\non batches outputted by the dataloader.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}