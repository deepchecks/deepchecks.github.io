{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confusion Matrix Report {#nlp__confusion_matrix_report}\n=======================\n\nThis notebook provides an overview for using and understanding the\nConfusion Matrix Report check for NLP tasks.\n\n**Structure:**\n\n-   [What is the Confusion Matrix\n    Report?](#what-is-the-confusion-matrix-report)\n-   [Generate data & model](#generate-data-model)\n-   [Run the check](#run-the-check)\n\nWhat is the Confusion Matrix Report?\n------------------------------------\n\nThe `ConfusionMatrixReport` produces a confusion matrix visualization\nwhich summarizes the performance of the model. The confusion matrix\ncontains the TP (true positive), FP (false positive), TN (true negative)\nand FN (false negative), from which we can derive the relevant metrics,\nsuch as accuracy, precision, recall etc. ([confusion\nmatrix](https://en.wikipedia.org/wiki/Confusion_matrix)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate data & model\n=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp import TextData\nfrom deepchecks.nlp.checks import ConfusionMatrixReport\nfrom deepchecks.nlp.datasets.classification.tweet_emotion import load_data, load_precalculated_predictions\n\ntweets_data = load_data(data_format='DataFrame', as_train_test=False)\ntweets_dataset = TextData(tweets_data.text, label=tweets_data['label'],\n                          task_type='text_classification')\n\npredictions = load_precalculated_predictions(as_train_test=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = ConfusionMatrixReport()\nresult = check.run(tweets_dataset, predictions=predictions)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nWe can define our check a condition that will validate if all the\nmisclassified cells/samples in the confusion matrix is below a certain\nthreshold. Using the `misclassified_samples_threshold` argument, we can\nspecify what percentage of the total samples our condition should use to\nvalidate the misclassified cells.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Let's add a condition and re-run the check:\n\ncheck = ConfusionMatrixReport()\ncheck.add_condition_misclassified_samples_lower_than_condition(misclassified_samples_threshold=0.1)\nresult = check.run(tweets_dataset, predictions=predictions)\nresult.show()\n\n#%%"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}