{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mean Average Precision Report\n=============================\n\nThis notebooks provides an overview for using and understanding the mean\naverage precision report check.\n\n**Structure:**\n\n-   [What is the purpose of the\n    check?](#what-is-the-purpose-of-the-check)\n-   [Generate data & model](#generate-data-and-model)\n-   [Run the check](#run-the-check)\n-   [Define a condition](#define-a-condition)\n\nWhat Is the Purpose of the Check?\n---------------------------------\n\nThe Mean Average Precision Report evaluates the [mAP\nmetric](https://manalelaidouni.github.io/Evaluating-Object-Detection-Models-Guide-to-Performance-Metrics.html)\non the given model and data, plots the AP on graph, and returns the mAP\nvalues per bounding box size category (small, medium, large). This check\nonly works on the Object Detection task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate Data and Model\n=======================\n\nWe generate a sample dataset of 128 images from the [COCO\ndataset](https://cocodataset.org/#home), and using the [YOLOv5\nmodel](https://github.com/ultralytics/yolov5).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.checks.performance import MeanAveragePrecisionReport\nfrom deepchecks.vision.datasets.detection import coco\n\nyolo = coco.load_model(pretrained=True)\ntest_ds = coco.load_dataset(train=False, object_type='VisionData')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = MeanAveragePrecisionReport()\nresult = check.run(test_ds, yolo)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observe the check's output\n==========================\n\nThe result value is a dataframe that has the Mean Average Precision\nscore for different bounding box area sizes. We report the mAP for\ndifferent IoU thresholds: 0.5, 0.75 and an average of mAP values for IoU\nthresholds between 0.5 and 0.9 (with a jump size of 0.05).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nWe can define a condition that checks whether our model\\'s mean average\nprecision score is not less than a given threshold for all bounding box\nsizes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = MeanAveragePrecisionReport().add_condition_average_mean_average_precision_not_less_than(0.4)\nresult = check.run(test_ds, yolo)\nresult.show(show_additional_outputs=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}