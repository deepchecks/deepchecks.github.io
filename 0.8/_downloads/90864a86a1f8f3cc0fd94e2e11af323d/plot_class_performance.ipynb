{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Class Performance\n=================\n\nThis notebooks provides an overview for using and understanding the\nclass performance check.\n\n**Structure:**\n\n-   [What is the purpose of the\n    check?](#what-is-the-purpose-of-the-check)\n-   [Classification](#classification-performance-report)\n    -   [Generate data & model](#generate-data-and-model)\n    -   [Run the check](#run-the-check)\n-   [Object Detection](#object-detection-class-performance)\n    -   [Generate data & model](#id1)\n    -   [Run the check](#id2)\n\nWhat Is the Purpose of the Check?\n---------------------------------\n\nThe class performance check evaluates several metrics on the given model\nand data and returns all of the results in a single check. The check\nuses the following default metrics:\n\n  Task Type          Property name\n  ------------------ ----------------------------------------------------------------------------------------------------------------------------\n  Classification     Precision\n  Classification     Recall\n  Object Detection   [Average Precision](https://manalelaidouni.github.io/Evaluating-Object-Detection-Models-Guide-to-Performance-Metrics.html)\n  Object Detection   [Average Recall](https://manalelaidouni.github.io/Evaluating-Object-Detection-Models-Guide-to-Performance-Metrics.html)\n\nIn addition to the default metrics, the check supports custom metrics\nthat should be implemented using the\n[torch.ignite.Metric](https://pytorch.org/ignite/metrics.html#how-to-create-a-custom-metric)\nAPI. These can be passed as a list using the alternative\\_metrics\nparameter of the check, which will override the default metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports\n=======\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.checks import ClassPerformance\nfrom deepchecks.vision.datasets.classification import mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classification Performance Report\n=================================\n\nGenerate data and model:\n------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mnist_model = mnist.load_model()\ntrain_ds = mnist.load_dataset(train=True, object_type='VisionData')\ntest_ds = mnist.load_dataset(train=False, object_type='VisionData')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = ClassPerformance()\ncheck.run(train_ds, test_ds, mnist_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Object Detection Class Performance\n==================================\n\nFor object detection tasks - the default metric that is being calculated\nit the Average Precision. The definition of the Average Precision is\nidentical to how the COCO dataset defined it - mean of the average\nprecision per class, over the range \\[0.5, 0.95, 0.05\\] of IoU\nthresholds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.datasets.detection import coco"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate Data and Model\n=======================\n\nWe generate a sample dataset of 128 images from the [COCO\ndataset](https://cocodataset.org/#home), and using the [YOLOv5\nmodel](https://github.com/ultralytics/yolov5).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "yolo = coco.load_model(pretrained=True)\n\ntrain_ds = coco.load_dataset(train=True, object_type='VisionData')\ntest_ds = coco.load_dataset(train=False, object_type='VisionData')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = ClassPerformance(show_only='best')\ncheck.run(train_ds, test_ds, yolo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a Condition\n==================\n\nWe can also define a condition to validate that our model performance is\nabove a certain threshold. The condition is defined as a function that\ntakes the results of the check as input and returns a ConditionResult\nobject.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = ClassPerformance(show_only='worst')\ncheck.add_condition_test_performance_greater_than(0.2)\nresult = check.run(train_ds, test_ds, yolo)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We detected that for several classes our model performance is below the\nthreshold.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}