{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calibration Score {#tabular__calibration_score}\n=================\n\nThis notebook provides an overview for using and understanding the\nCalibration Score check.\n\n**Structure:**\n\n-   [What is the Calibration Score\n    check?](#what-is-the-calibration-score-check)\n-   [Binary Classification](#binary-classification)\n-   [Multi-class classification](#multi-class-classification)\n\nWhat is the Calibration Score check?\n------------------------------------\n\nThe `CalibrationScore` check calculates the calibration curve with brier\nscore for each class. Calibration refers to the relationship between the\nmodel probabilities for one label to the ground truth (the label). For\ninstance, a probability of 0.7 for class A represents that there is 70%\nchance the true label of this sample is actually class A.\n\nCalibration curves (also known as reliability diagrams) compare how well\nthe probabilistic predictions of the classifier are calibrated by\nplotting the true frequency of one label against its predicted\nprobability.\n\nThe Brier score metric may be used to assess how well a classifier is\ncalibrated ([Brier score](https://en.wikipedia.org/wiki/Brier_score)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports\n=======\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\n\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom deepchecks.tabular import Dataset\nfrom deepchecks.tabular.checks import CalibrationScore\nfrom deepchecks.tabular.datasets.classification import adult\n\n\ndef custom_formatwarning(msg, *args, **kwargs):\n    # ignore everything except the message\n    return str(msg) + '\\n'\n\nwarnings.formatwarning = custom_formatwarning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Binary Classification\n=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate data & model\n# -----------------------\n# The dataset is the adult dataset which can be downloaded from the UCI machine\n# learning repository.\n#\n# Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml].\n# Irvine, CA: University of California, School of Information and Computer Science.\ntrain_ds, test_ds = adult.load_data()\nmodel = adult.load_fitted_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = CalibrationScore()\ncheck.run(test_ds, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Multi-class classification\n==========================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate data & model\n# -----------------------\niris = load_iris(as_frame=True)\nclf = LogisticRegression()\nframe = iris.frame\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=55)\nclf.fit(X_train, y_train)\nds = Dataset(pd.concat([X_test, y_test], axis=1), \n            features=iris.feature_names,\n            label='target')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = CalibrationScore()\ncheck.run(ds, clf)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}