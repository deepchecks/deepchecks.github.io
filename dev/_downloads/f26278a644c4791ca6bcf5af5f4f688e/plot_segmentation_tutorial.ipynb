{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Semantic Segmentation Tutorial {#vision_segmentation_tutorial}\n==============================\n\nIn this tutorial, you will learn how to validate your **semantic\nsegmentation model** using deepchecks test suites. You can read more\nabout the different checks and suites for computer vision use cases at\nthe `examples section  </checks_gallery/vision/index>`{.interpreted-text\nrole=\"doc\"}.\n\nIf you just want to see the output of this tutorial, jump to\n`observing_the_result`{.interpreted-text role=\"ref\"} section.\n\nA semantic segmentation task is a task where every pixel of the image is\nlabeled with a single class. Therefore, a common output of these tasks\nis an image of identical size to the input, with a vector for each pixel\nof the probability for each class.\n\n``` {.bash}\n# Before we start, if you don't have deepchecks vision package installed yet, run:\nimport sys\n!{sys.executable} -m pip install \"deepchecks[vision]\" --quiet --upgrade # --user\n\n# or install using pip from your python environment\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the data and model\n===========================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Importing the required packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torchvision.transforms.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Data\n=========\n\nThe model in this tutorial is used to detect different objects in images\n(labels based on the Pascal VOC dataset). The model is trained to\nidentify 20 different objects (person, bicycle etc.) and background. The\ndataset itself is the COCO128 dataset with semantic segmentation labels,\nmapped to the Pascal VOC labels (Originally, the COCO dataset includes\nmore labels, but those has been filtered out) The dataset can be loaded\nas a pytorch DataLoader object from\ndeepchecks.vision.datasets.segmentation, as is done in this tutorial,\nbut can also be loaded as a SegmentationData object\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The full pascal VOC data and information can be found here: http://host.robots.ox.ac.uk/pascal/VOC/voc2012/\n# And the COCO128 dataset can be found here: https://www.kaggle.com/datasets/ultralytics/coco128\n\nfrom torchvision.utils import draw_segmentation_masks\n\nfrom deepchecks.vision.datasets.segmentation.segmentation_coco import load_dataset, load_model\nfrom deepchecks.vision.segmentation_data import SegmentationData\n\ntrain_loader = load_dataset(object_type='DataLoader', train=True)\ntest_loader = load_dataset(object_type='DataLoader', train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize a Few Images\n======================\n\nLet\\'s visualize a few images with their segmentation, to understand the\ndata augmentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch = next(iter(test_loader))\n\nmasked_images = [draw_segmentation_masks(batch[0][i], masks=torch.stack([batch[1][i] == j for j in range(20)]),\n                                         alpha=0.6) for i in range(5)]\n\nfix, axs = plt.subplots(ncols=len(masked_images), figsize=(20, 20))\nfor i, img in enumerate(masked_images):\n    img = img.detach()\n    img = F.to_pil_image(img)\n    axs[i].imshow(np.asarray(img))\n    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\nfix.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloading a Pre-trained Model\n===============================\n\nIn this tutorial, we will download a pre-trained LRSAPP model and a\nMobileNetV3 Large backbone from the official PyTorch repository. For\nmore details, please refer to the [official\ndocumentation](https://pytorch.org/vision/main/models/generated/torchvision.models.segmentation.lraspp_mobilenet_v3_large.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel = load_model(pretrained=True, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validating the Model With Deepchecks\n====================================\n\nNow, after we have the training data, test data and the model, we can\nvalidate the model with deepchecks test suites.\n\nVisualize the Data Loader and the Model Outputs\n-----------------------------------------------\n\nFirst we\\'ll make sure we are familiar with the data loader and the\nmodel outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_loader))\n\nprint(\"Batch type is: \", type(batch))\nprint(\"First element is: \", type(batch[0]), \"with len of \", len(batch[0]))\nprint(\"Example output of an image shape from the dataloader \", batch[0][0].shape)\nprint(\"Image values\", batch[0][0])\nprint(\"-\" * 80)\n\nprint(\"Second element is: \", type(batch[1]), \"with len of \", len(batch[1]))\nprint(\"Example output of a label shape from the dataloader \", batch[1][0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementing the SegmentationData class\n=======================================\n\nThe checks in the package validate the model & data by calculating\nvarious quantities over the data, labels and predictions. In order to do\nthat, those must be in a pre-defined format, according to the task type.\nThe first step is to implement a class that enables deepchecks to\ninteract with your model and data and transform them to this pre-defined\nformat, which is set for each task type. In this tutorial, we will\nimplement the semantic segmentation task type by implementing a class\nthat inherits from the\n`deepchecks.vision.segmentation_data.SegmentationData`{.interpreted-text\nrole=\"class\"} class.\n\nThe SegmentationData class contains additional data and general methods\nintended for easy access to relevant metadata for semantic segmentation\nML models validation. To learn more about the expected format please\nvisit the API reference for the\n`deepchecks.vision.segmentation_data.SegmentationData`{.interpreted-text\nrole=\"class\"} class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class CocoSegmentationData(SegmentationData):\n    \"\"\"Class for loading the COCO segmentation dataset, inherits from :class:`~deepchecks.vision.SegmentationData`.\n\n    Implement the necessary methods to load the dataset.\n    \"\"\"\n\n    def batch_to_labels(self, batch):\n        \"\"\"Extract from the batch only the labels and return the labels in format (H, W).\n\n        See SegmentationData for more details on format.\n        \"\"\"\n        return batch[1]\n\n    def infer_on_batch(self, batch, model, device):\n        \"\"\"Infer on a batch of images and return predictions in format (C, H, W), where C is the class_id dimension.\n\n        See SegmentationData for more details on format.\n        \"\"\"\n        normalized_batch = [F.normalize(img.unsqueeze(0).float() / 255,\n                                        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) for img in batch[0]]\n\n        predictions = [model(img)[\"out\"].squeeze(0).detach() for img in normalized_batch]\n        predictions = [torch.nn.functional.softmax(pred, dim=0) for pred in predictions]\n\n        return predictions\n\n    def batch_to_images(self, batch):\n        \"\"\"Convert the batch to a list of images, where each image is a 3D numpy array in the format (H, W, C).\"\"\"\n        return [tensor.numpy().transpose((1, 2, 0)) for tensor in batch[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After defining the task class, we can validate it by running the\nfollowing code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The label_map is a dictionary that maps the class id to the class name, for display purposes.\nLABEL_MAP = {0: 'background', 1: 'airplane', 2: 'bicycle', 3: 'bird', 4: 'boat', 5: 'bottle', 6: 'bus', 7: 'car',\n             8: 'cat', 9: 'chair', 10: 'cow', 11: 'dining table', 12: 'dog', 13: 'horse', 14: 'motorcycle',\n             15: 'person', 16: 'potted plant', 17: 'sheep', 18: 'couch', 19: 'train', 20: 'tv'}\n\ntraining_data = CocoSegmentationData(data_loader=train_loader, label_map=LABEL_MAP)\ntest_data = CocoSegmentationData(data_loader=test_loader, label_map=LABEL_MAP)\n\ntraining_data.validate_format(model, device=device)\ntest_data.validate_format(model, device=device)\n\n# And observe the output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running Deepchecks\\' model evaluation suite on our data and model!\n==================================================================\n\nNow that we have defined the task class, we can validate the model with\nthe model evaluation suite of deepchecks. This can be done with this\nsimple few lines of code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.suites import model_evaluation\n\nsuite = model_evaluation()\nresult = suite.run(training_data, test_data, model, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observing the results: {#vision_segmentation_tutorial__observing_the_result}\n======================\n\nThe results can be saved as a html file with the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.save_as_html('output.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or, if working inside a notebook, the output can be displayed directly\nby simply printing the result object:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}