{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ROC Report {#tabular__roc_report}\n==========\n\nThis notebook provides an overview for using and understanding the ROC\nReport check.\n\n**Structure:**\n\n-   [What is the ROC Report check?](#what-is-the-roc-report-check)\n-   [Generate data & model](#generate-data-model)\n-   [Run the check](#run-the-check)\n-   [Define a condition](#define-a-condition)\n\nWhat is the ROC Report check?\n-----------------------------\n\nThe `ROCReport` check calculates the ROC curve for each class. The ROC\ncurve is a plot of TPR (true positive rate) with respect to FPR (false\npositive rate) at various thresholds ([ROC\ncurve](https://deepchecks.com/glossary/roc-receiver-operating-characteristic-curve)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports\n=======\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\n\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom deepchecks.tabular import Dataset\nfrom deepchecks.tabular.checks import RocReport\n\n\ndef custom_formatwarning(msg, *args, **kwargs):\n    return str(msg) + '\\n'\n\nwarnings.formatwarning = custom_formatwarning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate data & model\n=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "iris = load_iris(as_frame=True)\nclf = LogisticRegression(penalty='none')\nframe = iris.frame\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=55)\nclf.fit(X_train, y_train)\nds = Dataset(pd.concat([X_test, y_test], axis=1), \n            features=iris.feature_names,\n            label='target')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the Check\n=============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = RocReport()\ncheck.run(ds, clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nA condition for minimum allowed AUC score per class can be defined.\nHere, we define minimum AUC score to be 0.7.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = RocReport()\ncheck.add_condition_auc_greater_than(0.7).run(ds, clf)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}