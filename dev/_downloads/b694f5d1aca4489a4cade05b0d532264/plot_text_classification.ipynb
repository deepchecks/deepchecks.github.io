{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test NLP Classification Tasks - Quickstart\n==========================================\n\nIn order to run deepchecks for NLP all you need to have are the\nfollowing for both your train and test data:\n\n1.  Your text data - a list of strings, each string is a single sample\n    (can be a sentence, paragraph, document etc.).\n2.  Your labels - either a\n    `` Text Classification <nlp_supported_text_classification> label or a\n    :ref:`Token Classification <nlp_supported_token_classification> ``{.interpreted-text\n    role=\"ref\"} label.\n3.  Your models predictions (see\n    `Supported Tasks </user-guide/nlp/supported_tasks>`{.interpreted-text\n    role=\"doc\"} for info on supported formats).\n\nIf you don\\'t have deepchecks installed yet:\n\n``` {.python}\nimport sys\n!{sys.executable} -m pip install deepchecks[nlp] -U --quiet #--user\n```\n\nSome properties calculated by `deepchecks.nlp` require additional\npackages to be installed. You can install them by running:\n\n``` {.python}\nimport sys\n!{sys.executable} -m pip install langdetect>=1.0.9 textblob>=0.17.1 -U --quiet #--user\n```\n\nFinally, we\\'ll be using the CatBoost model in this guide, so we\\'ll\nalso need to install it:\n\n``` {.python}\nimport sys\n!{sys.executable} -m pip install catboost -U --quiet #--user\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Data & Create TextData Objects\n===================================\n\nFor the purpose of this guide we\\'ll use a small subset of the [tweet\nemotion](https://github.com/cardiffnlp/tweeteval) dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Imports\nfrom deepchecks.nlp import TextData\nfrom deepchecks.nlp.datasets.classification import tweet_emotion\n\n# Load Data\ntrain, test = tweet_emotion.load_data(data_format='DataFrame')\ntrain.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that we have the tweet text itself, the label (the emotion)\nand then some additional metadata columns.\n\nWe can now create a\n`TextData <deepchecks.nlp.TextData>`{.interpreted-text role=\"class\"}\nobject for the train and test dataframes. This object is used to pass\nyour data to the deepchecks checks.\n\nTo create a TextData object, the only required argument is the text\nitself, but passing only the text will prevent multiple checks from\nrunning. In this example we\\'ll pass the label as well and also provide\nmetadata (the other columns in the dataframe) which we\\'ll use later on\nin the guide. Finally, we\\'ll also explicitly set the index.\n\n::: {.note}\n::: {.title}\nNote\n:::\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#    The label column is optional, but if provided you must also pass the ``task_type`` argument, so that deepchecks\n#    will know how to interpret the label column.\n#\n\ntrain = TextData(train.text, label=train['label'], task_type='text_classification',\n                 index=train.index, metadata=train.drop(columns=['label', 'text']))\ntest = TextData(test.text, label=test['label'], task_type='text_classification',\n                index=test.index, metadata=test.drop(columns=['label', 'text']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Building a Model\n================\n\nIn this example we\\'ll train a very basic model for simplicity, using a\nCatBoostClassifier trained over the embeddings of the tweets. In this\ncase these embeddings were created using the OpenAI GPT-3 model. If you\nwant to reproduce this kind of basic model for your own task, you can\ncalculate your own embeddings, or use our\n`calculate_embeddings_for_text <deepchecks.nlp.utils.calculate_embeddings_for_text>`{.interpreted-text\nrole=\"func\"} function to create embeddings from a generic model. Note\nthat in order to run it you need either an OpenAI API key or have\nHuggingFace\\'s transformers installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\nfrom catboost import CatBoostClassifier\n\n# Load Embeddings and Split to Train and Test\nembeddings = tweet_emotion.load_embeddings()\ntrain_embeddings, test_embeddings = embeddings[train.index, :], embeddings[test.index, :]\n\nmodel = CatBoostClassifier(max_depth=2, n_estimators=50, random_state=42)\nmodel.fit(embeddings[train.index, :], train.label, verbose=0)\nprint(roc_auc_score(test.label,\n                    model.predict_proba(embeddings[test.index, :]),\n                    multi_class=\"ovr\", average=\"macro\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running Deepchecks\n==================\n\nNow that we have our data and model, we can run our first checks! We\\'ll\nrun two types of checks:\n\n1.  [Model Evaluation Checks](#model-evaluation-checks) - checks to run\n    once we have trained our model.\n2.  [Data Integrity Checks]() - checks to run on our dataset, before we\n    train our model.\n\nYou can read more about when should you use deepchecks\n`here <when_should_you_use_deepchecks>`{.interpreted-text role=\"ref\"}.\n\nModel Evaluation Checks\n-----------------------\n\nWe\\'ll start by running the\n`TrainTestPredictionDrift <deepchecks.nlp.checks.model_evaluation.train_test_prediction_drift.TrainTestPredictionDrift>`{.interpreted-text\nrole=\"class\"} check, which will let us know if there has been a\nsignificant change in the model\\'s predictions between the train and\ntest data. Such a change may imply that something has changed in the\ndata distribution between the train and test data in a way that affects\nthe model\\'s predictions.\n\nWe\\'ll also add a condition to the check, which will make it fail if the\ndrift score is higher than 0.1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Start by computing the predictions for the train and test data:\ntrain_preds, train_probas = model.predict(embeddings[train.index, :]), model.predict_proba(embeddings[train.index, :])\ntest_preds, test_probas = model.predict(embeddings[test.index, :]), model.predict_proba(embeddings[test.index, :])\n\n# Run the check\nfrom deepchecks.nlp.checks import PredictionDrift\n\ncheck = PredictionDrift().add_condition_drift_score_less_than(0.1)\nresult = check.run(train, test, train_predictions=list(train_preds), test_predictions=list(test_preds))\n\n# Note: the result can be saved as html using suite_result.save_as_html()\n# or exported to json using suite_result.to_json()\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the check passed, and that the drift score is quite low.\n\nNext, we\\'ll run the\n`MetadataSegmentsPerformance <deepchecks.nlp.checks.model_evaluation.weak_segments_performance.MetadataSegmentsPerformance>`{.interpreted-text\nrole=\"class\"} check, which will check the performance of the model on\ndifferent segments of the metadata that we provided earlier when\ncreating the `TextData <deepchecks.nlp.TextData>`{.interpreted-text\nrole=\"class\"} objects, and report back on any segments that have\nsignificantly lower performance than the rest of the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.checks import MetadataSegmentsPerformance\n\ncheck = MetadataSegmentsPerformance()\n\nresult = check.run(test, predictions=list(test_preds), probabilities=test_probas)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the check found a segment that has significantly lower\nperformance than the rest of the data. In the first tab of the display\nwe can see that there is a large segment of young Europeans that have\nsignificantly lower performance than the rest of the data. Perhaps there\nis some language gap here? We should probably collect and annotate more\ndata from this segment.\n\nProperties\n==========\n\nProperties are one-dimension values that are extracted from the text.\nAmong their uses, they can be used to segment the data, similar to the\nmetadata segments that we saw in the previous check.\n\nBefore we can run the\n`PropertySegmentsPerformance <deepchecks.nlp.checks.model_evaluation.weak_segments_performance.PropertySegmentsPerformance>`{.interpreted-text\nrole=\"class\"} check, we need to make sure that our\n`TextData <deepchecks.nlp.TextData>`{.interpreted-text role=\"class\"}\nobjects have the properties that we want to use. Properties can be added\nto the TextData objects in one of the following ways:\n\n1.  Calculated automatically by deepchecks. Deepchecks has a set of\n    predefined properties that can be calculated automatically. They can\n    be added to the TextData object either by passing\n    `properties='auto'` to the TextData constructor, or by calling the\n    `calculate_default_properties <deepchecks.nlp.TextData.calculate_default_properties>`{.interpreted-text\n    role=\"meth\"} method anytime later.\n2.  You can calculate your own properties and then add them to the\n    TextData object. This can be done by passing a DataFrame of\n    properties to the TextData [properties]{.title-ref} argument, or by\n    calling the\n    `set_properties <deepchecks.nlp.TextData.set_properties>`{.interpreted-text\n    role=\"meth\"} method anytime later with such a DataFrame. You\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Calculate properties\ntrain.calculate_default_properties()\ntest.calculate_default_properties()\n\n# Run the check\nfrom deepchecks.nlp.checks import PropertySegmentsPerformance\n\ncheck = PropertySegmentsPerformance(segment_minimum_size_ratio=0.07)\nresult = check.run(test, predictions=list(test_preds), probabilities=test_probas)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the check found some segments that have lower performance\ncompared to the rest of the dataset. It seems that the model has a\nharder time predicting the emotions in the \\\"neutral-positive\\\"\nsentiment range (in our case, between around 0 and 0.45).\n\nData Integrity Checks\n=====================\n\nThese previous checks were all about the model\\'s performance. Now\nwe\\'ll run a check that attempts to find instances of shortcut learning\n- cases in which the label can be predicted by simple aspects of the\ndata, which in many cases can be an indication that the model has used\nsome information that won\\'t generalize to the real world.\n\nThis check is the\n`PropertyLabelCorrelation <deepchecks.nlp.checks.data_integrity.property_label_correlation.PropertyLabelCorrelation>`{.interpreted-text\nrole=\"class\"} check, which will check the correlation between the\nproperties and the labels, and report back on any properties that have a\nhigh correlation with the labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.checks import PropertyLabelCorrelation\n\ncheck = PropertyLabelCorrelation(n_top_features=10)\nresult = check.run(test)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case the check didn\\'t find any properties that have a high\ncorrelation with the labels. Apart from the sentiment property, which is\nexpected to have high relevance to the emotion of the tweet, the other\nproperties have very low correlation to the label.\n\nYou can find the full list of available NLP checks in the\n`nlp.checks api documentation <deepchecks.nlp.checks>`{.interpreted-text\nrole=\"mod\"}.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}