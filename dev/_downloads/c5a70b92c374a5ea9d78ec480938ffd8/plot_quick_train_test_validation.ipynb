{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quickstart - Train-Test Validation Suite {#quick_train_test_validation}\n========================================\n\nThe deepchecks train-test validation suite is relevant any time you wish\nto validate two data subsets. For example:\n\n-   Comparing distributions across different train-test splits (e.g.\n    before training a model or when splitting data for cross-validation)\n-   Comparing a new data batch to previous data batches\n\nHere we\\'ll use a loans\\' dataset\n(`deepchecks.tabular.datasets.classification.lending_club`{.interpreted-text\nrole=\"mod\"}), to demonstrate how you can run the suite with only a few\nsimple lines of code, and see which kind of insights it can find.\n\n``` {.sourceCode .bash}\n# Before we start, if you don't have deepchecks installed yet, run:\nimport sys\n!{sys.executable} -m pip install deepchecks -U --quiet\n\n# or install using pip from your python environment\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Data and Prepare Data\n==========================\n\nLoad Data\n---------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.datasets.classification import lending_club\nimport pandas as pd\n\ndata = lending_club.load_data(data_format='Dataframe', as_train_test=False)\ndata.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split Data to Train and Test\n============================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# convert date column to datetime, `issue_d`` is date column\ndata['issue_d'] = pd.to_datetime(data['issue_d'])\n\n# Use data from June and July for train and August for test:\ntrain_df = data[data['issue_d'].dt.month.isin([6, 7])]\ntest_df = data[data['issue_d'].dt.month.isin([8])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run Deepchecks for Train Test Validation\n========================================\n\nDefine a Dataset Object\n-----------------------\n\nCreate a deepchecks Dataset, including the relevant metadata (label,\ndate, index, etc.). Check out\n`deepchecks.tabular.Dataset`{.interpreted-text role=\"class\"} to see all\nof the columns and types that can be declared.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Lending Club Metadata\n============================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "categorical_features = ['addr_state', 'application_type', 'home_ownership', \\\n  'initial_list_status', 'purpose', 'term', 'verification_status', 'sub_grade']\nindex_name = 'id'\nlabel = 'loan_status' # 0 is DEFAULT, 1 is OK\ndatetime_name = 'issue_d'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create Dataset\n==============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular import Dataset\n\n# Categorical features can be heuristically inferred, however we\n# recommend to state them explicitly to avoid misclassification.\n\n# Metadata attributes are optional. Some checks will run only if specific attributes are declared.\n\ntrain_ds = Dataset(train_df, label=label,cat_features=categorical_features, \\\n                   index_name=index_name, datetime_name=datetime_name)\ntest_ds = Dataset(test_df, label=label,cat_features=categorical_features, \\\n                   index_name=index_name, datetime_name=datetime_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# for convenience lets save it in a dictionary so we can reuse them for future Dataset initializations\ncolumns_metadata = {'cat_features' : categorical_features, 'index_name': index_name,\n                    'label':label, 'datetime_name':datetime_name}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the Deepchecks Suite\n========================\n\nValidate your data with the\n`deepchecks.tabular.suites.train_test_validation`{.interpreted-text\nrole=\"class\"} suite. It runs on two datasets, so you can use it to\ncompare any two batches of data (e.g. train data, test data, a new batch\nof data that recently arrived)\n\nCheck out the\n`\"when should you use deepchecks guide\" </getting-started/when_should_you_use>`{.interpreted-text\nrole=\"doc\"} for some more info about the existing suites and when to use\nthem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.suites import train_test_validation\n\nvalidation_suite = train_test_validation()\nsuite_result = validation_suite.run(train_ds, test_ds)\n# Note: the result can be saved as html using suite_result.save_as_html()\n# or exported to json using suite_result.to_json()\nsuite_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see in the suite\\'s results: the Date Train-Test Leakage\ncheck failed, indicating that we may have a problem in the way we\\'ve\nsplit our data! We\\'ve mixed up data from two years, causing a leakage\nof future data in the training dataset. Let\\'s fix this.\n\nFix Data\n========\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dt_col = data[datetime_name]\ntrain_df = data[dt_col.dt.year.isin([2017]) & dt_col.dt.month.isin([6,7,8])]\ntest_df = data[dt_col.dt.year.isin([2018]) & dt_col.dt.month.isin([6,7,8])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular import Dataset\n\n# Create the new Datasets\ntrain_ds = Dataset(train_df, **columns_metadata)\ntest_ds = Dataset(test_df, **columns_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Re-run Validation Suite\n=======================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "suite_result = validation_suite.run(train_ds, test_ds)\nsuite_result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ok, the date leakage doesn\\'t happen anymore!\n\nHowever, in the current split after the fix, we can see that we have a\nmultivariate drift, detected by the\n`</checks_gallery/tabular/train_test_validation/plot_whole_dataset_drift>`{.interpreted-text\nrole=\"doc\"} check. The drift is caused mainly by a combination of\nfeatures representing the loan\\'s interest rate (`int_rate`) and its\ngrade (`sub_grade`). In order to proceed, we should think about the two\noptions we have: To split the data in a different manner, or to stay\nwith the current split.\n\nFor working with different data splits: We can consider examining other\nsampling techniques (e.g. using only data from the same year), ideally\nachieving one in which the training data\\'s univariate and multivariate\ndistribution is similar to the data on which the model will run (test /\nproduction data). Of course, we can use deepchecks to validate the new\nsplits.\n\nIf the current split is representative and we are planning on training a\nmodel with it, it is worth understanding this drift (do we expect this\nkind of drift in the model\\'s production environment? can we do\nsomething about it?).\n\nFor more details about drift, see the\n`</user-guide/general/drift_guide>`{.interpreted-text role=\"doc\"}.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run a Single Check\n==================\n\nWe can run a single check on a dataset, and see the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# If we want to run only that check (possible with or without condition)\nfrom deepchecks.tabular.checks import WholeDatasetDrift\n\ncheck_with_condition = WholeDatasetDrift().add_condition_overall_drift_value_less_than(0.4)\n# or just the check without the condition:\n# check = WholeDatasetDrift()\ndataset_drift_result = check_with_condition.run(train_ds, test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also inspect and use the result\\'s value:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_drift_result.value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and see if the conditions have passed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_drift_result.passed_conditions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a Custom Suite\n=====================\n\nTo create our own suite, we can simply write all of the checks, and add\noptional conditions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular import Suite\nfrom deepchecks.tabular.checks import TrainTestFeatureDrift, WholeDatasetDrift, \\\n TrainTestPredictionDrift, TrainTestLabelDrift\n\ndrift_suite = Suite('drift suite',\nTrainTestFeatureDrift().add_condition_drift_score_less_than(\n  max_allowed_categorical_score=0.2, max_allowed_numeric_score=0.1),\nWholeDatasetDrift().add_condition_overall_drift_value_less_than(0.4),\nTrainTestLabelDrift(),\nTrainTestPredictionDrift()\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we can run our new suite using:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result = drift_suite.run(train_ds, test_ds)\nresult.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}