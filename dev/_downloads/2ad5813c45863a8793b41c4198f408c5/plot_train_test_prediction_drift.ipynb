{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Test Prediction Drift\n===========================\n\nThis notebooks provides an overview for using and understanding the\nvision prediction drift check.\n\n**Structure:**\n\n-   [What Is Prediction Drift?](#what-is-prediction-drift)\n-   [Which Prediction Properties Are\n    Used?](#which-prediction-properties-are-used)\n-   [Run Check on a Classification\n    Task](#run-the-check-on-a-classification-task-mnist)\n-   [Run Check on an Object Detection\n    Task](#run-the-check-on-an-object-detection-task-coco)\n\nWhat Is Prediction Drift? ======================== Drift is simply a\nchange in the distribution of data over time, and it is also one of the\ntop reasons why machine learning model\\'s performance degrades over\ntime.\n\nPrediction drift is when drift occurs in the prediction itself.\nCalculating prediction drift is especially useful in cases in which\nlabels are not available for the test dataset, and so a drift in the\npredictions is a direct indication that a change that happened in the\ndata has affected the model\\'s predictions. If labels are available,\nit\\'s also recommended to run the\n`Label Drift check </checks_gallery/vision/train_test_validation/plot_train_test_label_drift>`{.interpreted-text\nrole=\"doc\"}.\n\nFor more information on drift, please visit our\n`drift guide </user-guide/general/drift_guide>`{.interpreted-text\nrole=\"doc\"}\n\nHow Deepchecks Detects Prediction Drift\n\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n\nThis check detects prediction drift by using\n`univariate measures <drift_detection_by_univariate_measure>`{.interpreted-text\nrole=\"ref\"} on the prediction properties.\n\nUsing Prediction Properties to Detect Prediction Drift\n------------------------------------------------------\n\nIn computer vision specifically, our predictions may be complex, and\nmeasuring their drift is not a straightforward task. Therefore, we\ncalculate drift on different\n`properties of the prediction</user-guide/vision/vision_properties>`{.interpreted-text\nrole=\"doc\"}, on which we can directly measure drift.\n\nWhich Prediction Properties Are Used? =================================\n\n  Task Type          Property name                        What is it\n  ------------------ ------------------------------------ ----------------------------------------------\n  Classification     Samples Per Class                    Number of images per class\n  Object Detection   Samples Per Class                    Number of bounding boxes per class\n  Object Detection   Bounding Box Area                    Area of bounding box (height \\* width)\n  Object Detection   Number of Bounding Boxes Per Image   Number of bounding box objects in each image\n\n### Run the Check on a Classification Task (MNIST)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports\n=======\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.checks import TrainTestPredictionDrift\nfrom deepchecks.vision.datasets.classification.mnist import (load_dataset,\n                                                             load_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading data and model:\n=======================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_ds = load_dataset(train=True, batch_size=64, object_type='VisionData')\ntest_ds = load_dataset(train=False, batch_size=64, object_type='VisionData')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = load_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running TrainTestLabelDrift on classification\n=============================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = TrainTestPredictionDrift()\ncheck.run(train_ds, test_ds, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding the results\n=========================\n\nWe can see there is almost no drift between the train & test labels.\nThis means the split to train and test was good (as it is balanced and\nrandom). Let\\'s check the performance of a simple model trained on\nMNIST.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.checks import ClassPerformance\n\nClassPerformance().run(train_ds, test_ds, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MNIST with label drift\n======================\n\nNow, let\\'s try to separate the MNIST dataset in a different manner that\nwill result in a prediction drift, and see how it affects the\nperformance. We are going to create a custom collate\\_fn in the test\ndataset, that will select samples with class 0 in a 1/10 chances.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\nmnist_dataloader_train = load_dataset(train=True, batch_size=64, object_type='DataLoader')\nmnist_dataloader_test = load_dataset(train=False, batch_size=64, object_type='DataLoader')\nfull_mnist = torch.utils.data.ConcatDataset([mnist_dataloader_train.dataset, mnist_dataloader_test.dataset])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_dataset, test_dataset = torch.utils.data.random_split(full_mnist, [60000,10000], generator=torch.Generator().manual_seed(42))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inserting drift to the test set\n===============================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom torch.utils.data._utils.collate import default_collate\n\nnp.random.seed(42)\n\n\ndef collate_test(batch):\n    modified_batch = []\n    for item in batch:\n        image, label = item\n        if label == 0:\n            if np.random.randint(5) == 0:\n                modified_batch.append(item)\n            else:\n                modified_batch.append((image, 1))\n        else:\n            modified_batch.append(item)\n            \n    return default_collate(modified_batch)\n\nmod_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\nmod_test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, collate_fn=collate_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.datasets.classification.mnist import MNISTData\n\nmod_train_ds = MNISTData(mod_train_loader)\nmod_test_ds = MNISTData(mod_test_loader)\n\n# Run the check\n# -------------\n\ncheck = TrainTestPredictionDrift()\ncheck.run(mod_train_ds, mod_test_ds, model)\n\n# Add a condition\n# ---------------\n# We could also add a condition to the check to alert us to changes in the prediction\n# distribution, such as the one that occurred here.\n\ncheck = TrainTestPredictionDrift().add_condition_drift_score_not_greater_than()\ncheck.run(mod_train_ds, mod_test_ds, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the condition alerts us to the present of drift in the\nprediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results\n=======\n\nWe can see the check successfully detects the (expected) drift in class\n0 distribution between the train and test sets. It means the the model\ncorrectly predicted 0 for those samples and so we\\'re seeing drift in\nthe predictions as well as the labels. We note that this check enabled\nus to detect the presence of label drift (in this case) without needing\nactual labels for the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But how does this affect the performance of the model?\n======================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ClassPerformance().run(mod_train_ds, mod_test_ds, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inferring the results\n=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We can see the drop in the precision of class 0, which was caused by the class\n# imbalance indicated earlier by the label drift check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the Check on an Object Detection Task (COCO)\n================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.datasets.detection.coco import load_dataset, load_model\n\ntrain_ds = load_dataset(train=True, object_type='VisionData')\ntest_ds = load_dataset(train=False, object_type='VisionData')\nmodel = load_model(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = TrainTestPredictionDrift()\ncheck.run(train_ds, test_ds, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediction drift is detected!\n=============================\n\nWe can see that the COCO128 contains a drift in the out of the box\ndataset. In addition to the prediction count per class, the prediction\ndrift check for object detection tasks include drift calculation on\ncertain measurements, like the bounding box area and the number of\nbboxes per image.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}