{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Boosting Overfit\n================\n\nThis notebooks provides an overview for using and understanding the\nboosting overfit check.\n\n**Structure:**\n\n-   [What is a boosting overfit?](#what-is-a-boosting-overfit)\n-   [Generate data & model](#generate-data-model)\n-   [Run the check](#run-the-check)\n-   [Define a condition](#define-a-condition)\n\nWhat is A Boosting Overfit?\n---------------------------\n\nA boosting algorithm is a machine learning algorithm that uses a\ncombination of weak learners to predict a target variable. The mechanism\nof boosting is to increase the number of weak learners in the ensemble\nby iteratively adding a new weak learner. The new weak learner uses the\nerror of the ensemble from the previous iterations as its training data.\nThis mechanism continues until the ensemble reaches a certain\nperformance level or until the given maximum number of iterations is\nreached.\n\nThanks to its mechanism, boosting algorithms are usually less prone to\noverfitting than other traditional algorithms like single decision\ntrees. However, the number of weak learners in the ensemble can be too\nlarge making the ensemble too complex given the amount of data it was\ntrained on. In this case, the ensemble may be overfitted on the training\ndata.\n\n### How deepchecks detects a boosting overfit?\n\nThe check runs for a pre-defined number of iterations, and in each step\nit uses only the first X estimators from the boosting model when\npredicting the target variable (number of estimators X is monotonic\nincreasing). It plots the given score calculated for each iteration for\nboth the train dataset and the test dataset.\n\nIf the ratio of decline between the maximal test score achieved in any\nboosting iteration and the test score achieved in the last iteration\n(\\\"full\\\" model score) is above a given threshold (0.05 by default), it\nmeans the model is overfitted and the default condition, if added, will\nfail.\n\n### Supported Models\n\nCurrently the check supports the following models: - AdaBoost (sklearn)\n- GradientBoosting (sklearn) - XGBoost (xgboost) - LGBM (lightgbm) -\nCatBoost (catboost)\n\nGenerate data & model\n---------------------\n\nThe dataset is the adult dataset which can be downloaded from the UCI\nmachine learning repository.\n\nDua, D. and Graff, C. (2019). UCI Machine Learning Repository\n\\[<http://archive.ics.uci.edu/ml>\\]. Irvine, CA: University of\nCalifornia, School of Information and Computer Science.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom deepchecks.tabular.datasets.classification import adult\nfrom deepchecks.tabular import Dataset\n\ntrain_df, val_df = adult.load_data(data_format='Dataframe')\n\n# Run label encoder on all categorical columns\nfor column in train_df.columns:\n    if train_df[column].dtype == 'object':\n        le = LabelEncoder()\n        le.fit(pd.concat([train_df[column], val_df[column]]))\n        train_df[column] = le.transform(train_df[column])\n        val_df[column] = le.transform(val_df[column])\n\ntrain_ds = Dataset(train_df, label='income')\nvalidation_ds = Dataset(val_df, label='income')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classification model\n====================\n\nWe use the AdaBoost boosting algorithm with a decision tree as weak\nlearner.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n\nclf = AdaBoostClassifier(random_state=0, n_estimators=100)\nclf.fit(train_ds.data[train_ds.features], train_ds.data[train_ds.label_name])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.checks import BoostingOverfit\n\nresult = BoostingOverfit().run(train_ds, validation_ds, clf)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nNow, we define a condition that will validate if the percent of decline\nbetween the maximal score achieved in any boosting iteration and the\nscore achieved in the last iteration is above 0.02%.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = BoostingOverfit()\ncheck.add_condition_test_score_percent_decline_not_greater_than(0.0002)\nresult = check.run(train_ds, validation_ds, clf)\nresult.show(show_additional_outputs=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}