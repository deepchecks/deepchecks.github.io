{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Object Detection Tutorial {#vision_detection_tutorial}\n=========================\n\nIn this tutorial, you will learn how to validate your **object detection\nmodel** using deepchecks test suites. You can read more about the\ndifferent checks and suites for computer vision use cases at the\n`examples section  </checks_gallery/vision/index>`{.interpreted-text\nrole=\"doc\"}\n\nIf you just want to see the output of this tutorial, jump to the\n`observing the results <vision_segmentation_tutorial__observing_the_result>`{.interpreted-text\nrole=\"ref\"} section.\n\nAn object detection tasks usually consist of two parts:\n\n-   Object Localization, where the model predicts the location of an\n    object in the image,\n-   Object Classification, where the model predicts the class of the\n    detected object.\n\nThe common output of an object detection model is a list of bounding\nboxes around the objects, and their classes.\n\n``` {.sourceCode .bash}\n# Before we start, if you don't have deepchecks vision package installed yet, run:\nimport sys\n!{sys.executable} -m pip install \"deepchecks[vision]\" --quiet --upgrade # --user\n\n# or install using pip from your python environment\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the data and model\n===========================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Importing the required packages\nimport os\nimport urllib.request\nimport xml.etree.ElementTree as ET\nimport zipfile\nfrom functools import partial\n\nimport albumentations as A\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torchvision\nfrom albumentations.pytorch import ToTensorV2\nfrom PIL import Image\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.models.detection import _utils as det_utils\nfrom torchvision.models.detection.ssdlite import SSDLiteClassificationHead\n\nfrom deepchecks.vision.detection_data import DetectionData"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Data\n=========\n\nThe model in this tutorial is used to detect tomatoes in images. The\nmodel is trained on a dataset consisted of 895 images of tomatoes, with\nbounding box annotations provided in PASCAL VOC format. All annotations\nbelong to a single class: tomato.\n\n::: {.note}\n::: {.admonition-title}\nNote\n:::\n\nThe dataset is available at the following link:\n<https://www.kaggle.com/andrewmvd/tomato-detection>\n\nWe thank the authors of the dataset for providing the dataset.\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "url = 'https://figshare.com/ndownloader/files/34488599'\nurllib.request.urlretrieve(url, 'tomato-detection.zip')\n\nwith zipfile.ZipFile('tomato-detection.zip', 'r') as zip_ref:\n    zip_ref.extractall('.')\n\nclass TomatoDataset(Dataset):\n    def __init__(self, root, transforms):\n        self.root = root\n        self.transforms = transforms\n\n        self.images = list(sorted(os.listdir(os.path.join(root, 'images'))))\n        self.annotations = list(sorted(os.listdir(os.path.join(root, 'annotations'))))\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.root, \"images\", self.images[idx])\n        ann_path = os.path.join(self.root, \"annotations\", self.annotations[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        bboxes = []\n        labels = []\n        with open(ann_path, 'r') as f:\n            tree = ET.parse(f)\n            root = tree.getroot()\n            size = root.find('size')\n            w = int(size.find('width').text)\n            h = int(size.find('height').text)\n\n            for obj in root.iter('object'):\n                difficult = obj.find('difficult').text\n                if int(difficult) == 1:\n                    continue\n                cls_id = 1\n                xmlbox = obj.find('bndbox')\n                b = [float(xmlbox.find('xmin').text), float(xmlbox.find('ymin').text), float(xmlbox.find('xmax').text),\n                        float(xmlbox.find('ymax').text)]\n                bboxes.append(b)\n                labels.append(cls_id)\n\n        bboxes = torch.as_tensor(np.array(bboxes), dtype=torch.float32)\n        labels = torch.as_tensor(np.array(labels), dtype=torch.int64)\n\n        if self.transforms is not None:\n            res = self.transforms(image=np.array(img), bboxes=bboxes, class_labels=labels)\n\n        target = {\n            'boxes': [torch.Tensor(x) for x in res['bboxes']],\n            'labels': res['class_labels']\n        }\n\n        img = res['image']\n\n        return img, target\n\n    def __len__(self):\n        return len(self.images)\n\ndata_transforms = A.Compose([\n    A.Resize(height=256, width=256),\n    A.CenterCrop(height=224, width=224),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n\ndataset = TomatoDataset(root=os.path.join(os.path.curdir, 'tomato-detection/data'),\n                        transforms=data_transforms)\ntrain_set, test_set = torch.utils.data.random_split(dataset,\n                                                    [int(len(dataset)*0.9), len(dataset)-int(len(dataset)*0.9)],\n                                                    generator=torch.Generator().manual_seed(42))\ntest_set.transforms = A.Compose([ToTensorV2()])\ntrain_loader = DataLoader(train_set, batch_size=64, collate_fn=(lambda batch: tuple(zip(*batch))))\ntest_loader = DataLoader(test_set, batch_size=64, collate_fn=(lambda batch: tuple(zip(*batch))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize a Few Images\n======================\n\nLet\\'s visualize a few training images so as to understand the data\naugmentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def prepare(inp):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1) * 255\n    inp = inp.transpose((2,0,1))\n    return torch.tensor(inp, dtype=torch.uint8)\n\nimport torchvision.transforms.functional as F\n\n\ndef show(imgs):\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=(20,20))\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\nfrom torchvision.utils import draw_bounding_boxes\n\ndata = next(iter(train_loader))\ninp, targets = data[0][:4], data[1][:4]\n\n\nresult = [draw_bounding_boxes(prepare(inp[i]), torch.stack(targets[i]['boxes']),\n                              colors=['yellow'] * torch.stack(targets[i]['boxes']).shape[0], width=5)\n          for i in range(len(targets))]\nshow(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloading a Pre-trained Model\n===============================\n\nIn this tutorial, we will download a pre-trained SSDlite model and a\nMobileNetV3 Large backbone from the official PyTorch repository. For\nmore details, please refer to the [official\ndocumentation](https://pytorch.org/vision/stable/generated/torchvision.models.detection.ssdlite320_mobilenet_v3_large.html#torchvision.models.detection.ssdlite320_mobilenet_v3_large).\n\nAfter downloading the model, we will fine-tune it for our particular\nclasses. We will do it by replacing the pre-trained head with a new one\nthat matches our needs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nmodel = torchvision.models.detection.ssdlite320_mobilenet_v3_large(pretrained=True)\n\nin_channels = det_utils.retrieve_out_channels(model.backbone, (320, 320))\nnum_anchors = model.anchor_generator.num_anchors_per_location()\nnorm_layer = partial(nn.BatchNorm2d, eps=0.001, momentum=0.03)\n\nmodel.head.classification_head = SSDLiteClassificationHead(in_channels, num_anchors, 2, norm_layer)\nmodel.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading Pre-trained Weights\n===========================\n\nFor this tutorial we will not include the training code itself, but will\ndownload and load pre-trained weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('tomato-detection/ssd_model.pth'))\n_ = model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validating the Model With Deepchecks\n====================================\n\nNow, after we have the training data, test data and the model, we can\nvalidate the model with deepchecks test suites.\n\nVisualize the Data Loader and the Model Outputs\n-----------------------------------------------\n\nFirst we\\'ll make sure we are familiar with the data loader and the\nmodel outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_loader))\n\nprint(\"Batch type is: \", type(batch))\nprint(\"First element is: \", type(batch[0]), \"with len of \", len(batch[0]))\nprint(\"Example output of an image shape from the dataloader \", batch[0][0].shape)\nprint(\"Image values\", batch[0][0])\nprint(\"-\"*80)\n\nprint(\"Second element is: \", type(batch[1]), \"with len of \", len(batch[1]))\nprint(\"Example output of a label from the dataloader \", batch[1][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementing the DetectionData class\n====================================\n\nThe checks in the package validate the model & data by calculating\nvarious quantities over the data, labels and predictions. In order to do\nthat, those must be in a pre-defined format, according to the task type.\nThe first step is to implement a class that enables deepchecks to\ninteract with your model and data and transform them to this pre-defined\nformat, which is set for each task type. In this tutorial, we will\nimplement the object detection task type by implementing a class that\ninherits from the\n`deepchecks.vision.detection_data.DetectionData`{.interpreted-text\nrole=\"class\"} class.\n\nThe DetectionData class contains additional data and general methods\nintended for easy access to relevant metadata for object detection ML\nmodels validation. To learn more about the expected format please visit\nthe API reference for the\n`deepchecks.vision.detection_data.DetectionData`{.interpreted-text\nrole=\"class\"} class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.detection_data import DetectionData\n\n\nclass TomatoData(DetectionData):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def batch_to_images(self, batch):\n        \"\"\"\n        Convert a batch of data to images in the expected format. The expected format is an iterable of cv2 images,\n        where each image is a numpy array of shape (height, width, channels). The numbers in the array should be in the\n        range [0, 255] in a uint8 format.\n        \"\"\"\n        inp = torch.stack(list(batch[0])).cpu().detach().numpy().transpose((0, 2, 3, 1))\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n        # Un-normalize the images\n        inp = std * inp + mean\n        inp = np.clip(inp, 0, 1)\n        return inp * 255\n\n    def batch_to_labels(self, batch):\n        \"\"\"\n        Convert a batch of data to labels in the expected format. The expected format is a list of tensors of length N,\n        where N is the number of samples. Each tensor element is in a shape of [B, 5], where B is the number of bboxes\n        in the image, and each bounding box is in the structure of [class_id, x, y, w, h].\n        \"\"\"\n        tensor_annotations = batch[1]\n        label = []\n        for annotation in tensor_annotations:\n            if len(annotation[\"boxes\"]):\n                bbox = torch.stack(annotation[\"boxes\"])\n                # Convert the Pascal VOC xyxy format to xywh format\n                bbox[:, 2:] = bbox[:, 2:] - bbox[:, :2]\n                # The label shape is [class_id, x, y, w, h]\n                label.append(\n                    torch.concat([torch.stack(annotation[\"labels\"]).reshape((-1, 1)), bbox], dim=1)\n                )\n            else:\n                # If it's an empty image, we need to add an empty label\n                label.append(torch.tensor([]))\n        return label\n\n    def infer_on_batch(self, batch, model, device):\n        \"\"\"\n        Returns the predictions for a batch of data. The expected format is a list of tensors of shape length N, where N\n        is the number of samples. Each tensor element is in a shape of [B, 6], where B is the number of bboxes in the\n        predictions, and each bounding box is in the structure of [x, y, w, h, score, class_id].\n        \"\"\"\n        nm_thrs = 0.2\n        score_thrs = 0.7\n        imgs = list(img.to(device) for img in batch[0])\n        # Getting the predictions of the model on the batch\n        with torch.no_grad():\n            preds = model(imgs)\n        processed_pred = []\n        for pred in preds:\n            # Performoing non-maximum suppression on the detections\n            keep_boxes = torchvision.ops.nms(pred['boxes'], pred['scores'], nm_thrs)\n            score_filter = pred['scores'][keep_boxes] > score_thrs\n\n            # get the filtered result\n            test_boxes = pred['boxes'][keep_boxes][score_filter].reshape((-1, 4))\n            test_boxes[:, 2:] = test_boxes[:, 2:] - test_boxes[:, :2]  # xyxy to xywh\n            test_labels = pred['labels'][keep_boxes][score_filter]\n            test_scores = pred['scores'][keep_boxes][score_filter]\n\n            processed_pred.append(\n                torch.concat([test_boxes, test_scores.reshape((-1, 1)), test_labels.reshape((-1, 1))], dim=1))\n        return processed_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After defining the task class, we can validate it by running the\nfollowing code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We have a single label here, which is the tomato class\n# The label_map is a dictionary that maps the class id to the class name, for display purposes.\nLABEL_MAP = {\n    1: 'Tomato'\n}\ntraining_data = TomatoData(data_loader=train_loader, label_map=LABEL_MAP)\ntest_data = TomatoData(data_loader=test_loader, label_map=LABEL_MAP)\n\ntraining_data.validate_format(model, device=device)\ntest_data.validate_format(model, device=device)\n\n# And observe the output:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running Deepchecks\\' suite on our data and model!\n=================================================\n\nNow that we have defined the task class, we can validate the model with\nthe deepchecks\\' model evaluation suite. This can be done with this\nsimple few lines of code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.suites import model_evaluation\n\nsuite = model_evaluation()\nresult = suite.run(training_data, test_data, model, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also have suites for:\n`data integrity <deepchecks.vision.suites.default_suites.data_integrity>`{.interpreted-text\nrole=\"func\"} - validating a single dataset and\n`train test validation <deepchecks.vision.suites.default_suites.train_test_validation>`{.interpreted-text\nrole=\"func\"} -validating the dataset split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observing the results: {#observing_the_result}\n======================\n\nThe results can be saved as a html file with the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.save_as_html('output.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or, if working inside a notebook, the output can be displayed directly\nby simply printing the result object:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}