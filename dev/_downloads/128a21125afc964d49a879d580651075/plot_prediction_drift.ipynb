{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediction Drift {#vision__prediction_drift}\n================\n\nThis notebooks provides an overview for using and understanding the\nvision prediction drift check.\n\n**Structure:**\n\n-   [What Is Prediction Drift?](#what-is-prediction-drift)\n-   [Which Prediction Properties Are\n    Used?](#which-prediction-properties-are-used)\n-   [Run Check on a Classification\n    Task](#run-the-check-on-a-classification-task-mnist)\n-   [Run Check on an Object Detection\n    Task](#run-the-check-on-an-object-detection-task-coco)\n\nWhat Is Prediction Drift?\n-------------------------\n\nDrift is simply a change in the distribution of data over time, and it\nis also one of the top reasons why machine learning model\\'s performance\ndegrades over time.\n\nPrediction drift is when drift occurs in the prediction itself.\nCalculating prediction drift is especially useful in cases in which\nlabels are not available for the test dataset, and so a drift in the\npredictions is a direct indication that a change that happened in the\ndata has affected the model\\'s predictions. If labels are available,\nit\\'s also recommended to run the\n`Label Drift check <vision__label_drift>`{.interpreted-text role=\"ref\"}.\n\nFor more information on drift, please visit our\n`Drift Guide <drift_user_guide>`{.interpreted-text role=\"ref\"}\n\n### How Deepchecks Detects Prediction Drift\n\nThis check detects prediction drift by using\n`univariate measures <drift_detection_by_univariate_measure>`{.interpreted-text\nrole=\"ref\"} on the prediction properties.\n\n### Using Prediction Properties to Detect Prediction Drift\n\nIn computer vision specifically, our predictions may be complex, and\nmeasuring their drift is not a straightforward task. Therefore, we\ncalculate drift on different\n`properties of the prediction <vision__properties_guide>`{.interpreted-text\nrole=\"ref\"}, on which we can directly measure drift.\n\nWhich Prediction Properties Are Used?\n-------------------------------------\n\n  Task Type          Property name                        What is it\n  ------------------ ------------------------------------ ----------------------------------------------\n  Classification     Samples Per Class                    Number of images per class\n  Object Detection   Samples Per Class                    Number of bounding boxes per class\n  Object Detection   Bounding Box Area                    Area of bounding box (height \\* width)\n  Object Detection   Number of Bounding Boxes Per Image   Number of bounding box objects in each image\n\nRun the Check on a Classification Task (MNIST)\n----------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports\n=======\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nIn this example, we use the pytorch version of the mnist dataset and\nmodel. In order to run this example using tensorflow, please change the\nimport statements to:\n\nfrom deepchecks.vision.datasets.classification.mnist\\_tensorflow import\nload\\_dataset\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.checks import PredictionDrift\nfrom deepchecks.vision.datasets.classification.mnist_torch import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Dataset\n============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_ds = load_dataset(train=True, batch_size=64, object_type='VisionData')\ntest_ds = load_dataset(train=False, batch_size=64, object_type='VisionData')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running PredictionDrift on classification\n=========================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = PredictionDrift()\nresult = check.run(train_ds, test_ds)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To display the results in an IDE like PyCharm, you can use the following\ncode:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#  result.show_in_window()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result will be displayed in a new window.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding the results\n=========================\n\nWe can see there is almost no drift between the train & test\npredictions. This means the split to train and test was good (as it is\nbalanced and random). Let\\'s check the performance of a simple model\ntrained on MNIST.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.checks import ClassPerformance\n\nClassPerformance().run(train_ds, test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MNIST with prediction drift\n===========================\n\nNow, let\\'s try to separate the MNIST dataset in a different manner that\nwill result in a prediction drift, and see how it affects the\nperformance. We are going to create a custom [collate\\_fn]{.title-ref}\\`\nin the test dataset, that will select a few of the samples with class 0\nand change their most of their predicted classes to 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inserting drift to the test set\n===============================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport torch\n\nnp.random.seed(42)\n\ndef generate_collate_fn_with_label_drift(collate_fn):\n    def collate_fn_with_label_drift(batch):\n        batch_dict = collate_fn(batch)\n        images = batch_dict['images']\n        labels = batch_dict['labels']\n        for i in range(len(images)):\n            image, label = images[i], labels[i]\n            if label == 0:\n                if np.random.randint(5) != 0:\n                    batch_dict['labels'][i] = 1\n                    # In 9/10 cases, the prediction vector will change to match the label\n                    if np.random.randint(10) != 0:\n                        batch_dict['predictions'][i] = torch.tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n\n        return batch_dict\n    return collate_fn_with_label_drift\n\n\nmod_test_ds = load_dataset(train=False, batch_size=1000, object_type='VisionData')\nmod_test_ds._batch_loader.collate_fn = generate_collate_fn_with_label_drift(mod_test_ds._batch_loader.collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = PredictionDrift()\nresult = check.run(train_ds, mod_test_ds)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add a condition\n===============\n\nWe could also add a condition to the check to alert us about changes in\nthe prediction distribution, such as the one that occurred here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = PredictionDrift().add_condition_drift_score_less_than()\nresult = check.run(train_ds, mod_test_ds)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the condition alerts us to the presence of drift in the\npredictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results\n=======\n\nWe can see the check successfully detects the (expected) drift in class\n0 distribution between the train and test sets. It means the the model\ncorrectly predicted 0 for those samples and so we\\'re seeing drift in\nthe predictions as well as the labels. We note that this check enabled\nus to detect the presence of label drift (in this case) without needing\nactual labels for the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But how does this affect the performance of the model?\n======================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result = ClassPerformance().run(train_ds, mod_test_ds)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inferring the results\n=====================\n\nWe can see the drop in the precision of class 0, which was caused by the\nclass imbalance indicated earlier by the label drift check.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the Check on an Object Detection Task (COCO)\n================================================\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nIn this example, we use the pytorch version of the coco dataset and\nmodel. In order to run this example using tensorflow, please change the\nimport statements to:\n\nfrom deepchecks.vision.datasets.detection.coco\\_tensorflow import\nload\\_dataset\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.datasets.detection.coco_torch import load_dataset\n\ntrain_ds = load_dataset(train=True, object_type='VisionData')\ntest_ds = load_dataset(train=False, object_type='VisionData')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = PredictionDrift()\nresult = check.run(train_ds, test_ds)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediction drift is detected!\n=============================\n\nWe can see that the COCO128 contains a drift in the out of the box\ndataset. In addition to the prediction count per class, the prediction\ndrift check for object detection tasks include drift calculation on\ncertain measurements, like the bounding box area and the number of\nbboxes per image.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}