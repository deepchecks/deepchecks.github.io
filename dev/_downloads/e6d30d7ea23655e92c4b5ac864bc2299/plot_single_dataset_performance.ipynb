{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Single Dataset Performance {#nlp__single_dataset_performance}\n==========================\n\nThis notebook provides an overview for using and understanding single\ndataset performance check for NLP tasks.\n\n**Structure:**\n\n-   [What is the purpose of the\n    check?](#what-is-the-purpose-of-the-check)\n-   [Generate data & model](#generate-data-model)\n-   [Run the check](#run-the-check)\n-   [Define a condition](#define-a-condition)\n\nWhat is the purpose of the check?\n---------------------------------\n\nThis check is designed for evaluating a model\\'s performance on a\nlabeled dataset based on a scorer or multiple scorers.\n\nScorers are a convention of sklearn to evaluate a model, it is a\nfunction which accepts (model, X, y\\_true) and returns a float result\nwhich is the score. A sklearn convention is that higher scores are\nbetter than lower scores. For additional details [see scorers\ndocumentation](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring).\n\nThe default scorers that are used are F1, Precision, and Recall.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate data & model\n=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.datasets.classification.tweet_emotion import load_data, load_precalculated_predictions\n\n_, test_dataset = load_data(data_format='TextData')\n_, test_probas = load_precalculated_predictions(pred_format='probabilities')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n\nYou can select which scorers to use by passing either a list or a dict\nof scorers to the check, see\n`Metrics Guide <metrics_user_guide>`{.interpreted-text role=\"ref\"} for\nadditional details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.checks import SingleDatasetPerformance\n\ncheck = SingleDatasetPerformance(scorers=['recall_per_class', 'precision_per_class', 'f1_macro', 'f1_micro'])\nresult = check.run(dataset=test_dataset, probabilities=test_probas)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nWe can define on our check a condition to validate that the different\nmetric scores are above a certain threshold. Using the `class_mode`\nargument we can define select a sub set of the classes to use for the\ncondition.\n\nLet\\'s add a condition to the check and see what happens when it fails:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check.add_condition_greater_than(threshold=0.85, class_mode='all')\nresult = check.run(dataset=test_dataset, probabilities=test_probas)\nresult.show(show_additional_outputs=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We detected that the Recall score is below specified threshold in at\nleast one of the classes.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}