{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Image Classification Tutorial {#vision__classification_tutorial}\n=============================\n\nIn this tutorial, you will learn how to validate your **classification\nmodel** using deepchecks test suites. You can read more about the\ndifferent checks and suites for computer vision use cases at the\n`examples section  <vision__checks_gallery>`{.interpreted-text\nrole=\"ref\"}.\n\nA classification model is usually used to classify an image into one of\na number of classes. Although there are multi label use-cases, in which\nthe model is used to classify an image into multiple classes, most\nuse-cases require the model to classify images into a single class.\nCurrently, deepchecks supports only single label classification (either\nbinary or multi-class).\n\n``` {.bash}\n# Before we start, if you don't have deepchecks vision package installed yet, run:\nimport sys\n!{sys.executable} -m pip install \"deepchecks[vision]\" --quiet --upgrade # --user\n\n# or install using pip from your python environment\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the data and model\n===========================\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nIn this tutorial, we use the pytorch to create the dataset and model. To\nsee how this can be done using tensorflow or other frameworks, please\nvisit the\n`creating VisionData guide <vision__vision_data_class>`{.interpreted-text\nrole=\"ref\"}.\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloading the dataset\n=======================\n\nThe data is available from the torch library. We will download and\nextract it to the current directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport urllib.request\nimport zipfile\n\nurl = 'https://download.pytorch.org/tutorial/hymenoptera_data.zip'\nurllib.request.urlretrieve(url, './hymenoptera_data.zip')\n\nwith zipfile.ZipFile('./hymenoptera_data.zip', 'r') as zip_ref:\n    zip_ref.extractall('.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Data\n=========\n\nWe will use torchvision and torch.utils.data packages for loading the\ndata. The model we are building will learn to classify **ants** and\n**bees**. We have about 120 training images each for ants and bees.\nThere are 75 validation images for each class. This dataset is a very\nsmall subset of imagenet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import albumentations as A\nimport numpy as np\nimport PIL.Image\nimport torch\nimport torchvision\nfrom albumentations.pytorch import ToTensorV2\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\nclass AntsBeesDataset(torchvision.datasets.ImageFolder):\n\n    def __getitem__(self, index: int):\n        \"\"\"overrides __getitem__ to be compatible to albumentations\"\"\"\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        sample = self.get_cv2_image(sample)\n        if self.transforms is not None:\n            transformed = self.transforms(image=sample, target=target)\n            sample, target = transformed[\"image\"], transformed[\"target\"]\n        else:\n            if self.transform is not None:\n                sample = self.transform(image=sample)['image']\n            if self.target_transform is not None:\n                target = self.target_transform(target)\n\n        return sample, target\n\n    def get_cv2_image(self, image):\n        if isinstance(image, PIL.Image.Image):\n            return np.array(image).astype('uint8')\n        elif isinstance(image, np.ndarray):\n            return image\n        else:\n            raise RuntimeError(\"Only PIL.Image and CV2 loaders currently supported!\")\n\ndata_dir = './hymenoptera_data'\n# Just normalization for validation\ndata_transforms = A.Compose([\n    A.Resize(height=256, width=256),\n    A.CenterCrop(height=224, width=224),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])\ntrain_dataset = AntsBeesDataset(root=os.path.join(data_dir,'train'))\ntrain_dataset.transforms = data_transforms\n\ntest_dataset = AntsBeesDataset(root=os.path.join(data_dir, 'val'))\ntest_dataset.transforms = data_transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the dataset\n=====================\n\nLet\\'s see how our data looks like.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f'Number of training images: {len(train_dataset)}')\nprint(f'Number of validation images: {len(test_dataset)}')\nprint(f'Example output of an image shape: {train_dataset[0][0].shape}')\nprint(f'Example output of a label: {train_dataset[0][1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloading a pre-trained model\n===============================\n\nNow, we will download a pre-trained model from torchvision, that was\ntrained on the ImageNet dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = torchvision.models.resnet18(pretrained=True)\nnum_ftrs = model.fc.in_features\n# We have only 2 classes\nmodel.fc = nn.Linear(num_ftrs, 2)\nmodel = model.to(device)\n_ = model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validating the Model with Deepchecks\n====================================\n\nNow, after we have the training data, validation data and the model, we\ncan validate the model with deepchecks test suites.\n\nImplementing the VisionData class\n---------------------------------\n\nThe checks in the package validate the model & data by calculating\nvarious quantities over the data, labels and predictions. In order to do\nthat, those must be in a pre-defined format, according to the task type.\nIn the following example we\\'re using pytorch. To see an implementation\nof this in tensorflow, please refer to the\n`vision__vision_data_class`{.interpreted-text role=\"ref\"} guide. For\npytorch, we will use our DataLoader, but we\\'ll create a new collate\nfunction for it, that transforms the batch to the correct format. Then,\nwe\\'ll create a\n`deepchecks.vision.vision_data.vision_data.VisionData`{.interpreted-text\nrole=\"class\"} object, that will hold the data loader.\n\nTo learn more about the expected formats, please visit the\n`vision__supported_tasks`{.interpreted-text role=\"ref\"}.\n\nFirst, we\\'ll create the collate function that will be used by the\nDataLoader. In pytorch, the collate function is used to transform the\noutput batch to any custom format, and we\\'ll use that in order to\ntransform the batch to the correct format for the checks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.vision_data import BatchOutputFormat\n\ndef deepchecks_collate_fn(batch) -> BatchOutputFormat:\n    \"\"\"Return a batch of images, labels and predictions for a batch of data. The expected format is a dictionary with\n    the following keys: 'images', 'labels' and 'predictions', each value is in the deepchecks format for the task.\n    You can also use the BatchOutputFormat class to create the output.\n    \"\"\"\n    # batch received as iterable of tuples of (image, label) and transformed to tuple of iterables of images and labels:\n    batch = tuple(zip(*batch))\n\n    # images:\n    inp = torch.stack(batch[0]).detach().numpy().transpose((0, 2, 3, 1))\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    inp = std * inp + mean\n    images = np.clip(inp, 0, 1) * 255\n\n    #labels:\n    labels = batch[1]\n\n    #predictions:\n    logits = model.to(device)(torch.stack(batch[0]).to(device))\n    predictions = nn.Softmax(dim=1)(logits)\n    return BatchOutputFormat(images=images, labels=labels, predictions=predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have a single label here, which is the tomato class The label\\_map is\na dictionary that maps the class id to the class name, for display\npurposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "LABEL_MAP = {\n    0: 'ants',\n    1: 'bees'\n  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our updated collate function, we can recreate the\ndataloader in the deepchecks format, and use it to create a VisionData\nobject:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision import VisionData\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=deepchecks_collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, collate_fn=deepchecks_collate_fn)\n\ntraining_data = VisionData(batch_loader=train_loader, task_type='classification', label_map=LABEL_MAP)\ntest_data = VisionData(batch_loader=test_loader, task_type='classification', label_map=LABEL_MAP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Making sure our data is in the correct format:\n==============================================\n\nThe VisionData object automatically validates your data format and will\nalert you if there is a problem. However, you can also manually view\nyour images and labels to make sure they are in the correct format by\nusing the `head` function to conveniently visualize your data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "training_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And observe the output:\n\nRunning Deepchecks\\' suite on our data and model!\n=================================================\n\nNow that we have defined the task class, we can validate the train and\ntest data with deepchecks\\' train test validation suite. This can be\ndone with this simple few lines of code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.suites import train_test_validation\n\nsuite = train_test_validation()\nresult = suite.run(training_data, test_data,  max_samples = 5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also have suites for:\n`data integrity <deepchecks.vision.suites.data_integrity>`{.interpreted-text\nrole=\"func\"} - validating a single dataset and\n`model evaluation <deepchecks.vision.suites.model_evaluation>`{.interpreted-text\nrole=\"func\"} -evaluating the model\\'s performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observing the results:\n======================\n\nThe results can be saved as a html file with the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.save_as_html('output.html')\n\n# Or displayed in a new window in an IDE like Pycharm:\n# result.show_in_window()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or, if working inside a notebook, the output can be displayed directly\nby simply printing the result object:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that we do not have any meaningful issues with our data, and\nalthough there\\'s some drift between the train and test datasets (under\nthe \\\"Passed\\\" section), this is not significant enough to cause any\nissues (and therefor is not displayed in the \\\"Didn\\'t Pass\\\" section).\nHowever, under the \\\"Other\\\" section, that details checks without a\nspecific pass/fail condition, we can see that the heatmap of brightness\nin the images is not uniformly distributed, which means that in most\nimages, there are brighter objects in the center of the image. This\nmakes sense as these images of bees and ants tend to have the insects in\nthe center of the image, but it is something to be aware of and maybe\nuse data augmentation to fix.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}