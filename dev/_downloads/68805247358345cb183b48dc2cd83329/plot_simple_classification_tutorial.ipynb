{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Image Data Validation in 5 Minutes {#vision_simple_classification_tutorial}\n==================================\n\nDeepchecks Vision is built to validate your data and model, however\ncomplex your model and data may be. That being said, sometime there is\nno need to write a full-blown\n`ClassificationData </user-guide/vision/auto_tutorials/plot_classification_tutorial>`{.interpreted-text\nrole=\"doc\"} or\n`DetectionData </user-guide/vision/auto_tutorials/plot_detection_tutorial>`{.interpreted-text\nrole=\"doc\"}. In the case of a simple classification task, there is quite\na few checks that can be run writing only a few lines of code. In this\ntutorial, we will show you how to run all checks that do not require a\nmodel on a simple classification task.\n\nThis is ideal, for example, when receiving a new dataset for a\nclassification task. Running these checks on the dataset before even\nstarting with training will give you a quick idea of how the dataset\nlooks like and what potential issues it contains.\n\n``` {.bash}\n# Before we start, if you don't have deepchecks vision package installed yet, run:\nimport sys\n!{sys.executable} -m pip install \"deepchecks[vision]\" --quiet --upgrade # --user\n\n# or install using pip from your python environment\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloading the Data\n====================\n\nFor this example we\\'ll use a small sample of the RGB [EuroSAT\ndataset](https://github.com/phelber/eurosat#). EuroSAT dataset is based\non Sentinel-2 satellite images covering 13 spectral bands and consisting\nof 10 classes with 27000 labeled and geo-referenced samples.\n\nCitations:\n\n\\[1\\] Eurosat: A novel dataset and deep learning benchmark for land use\nand land cover classification. Patrick Helber, Benjamin Bischke, Andreas\nDengel, Damian Borth. IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing, 2019.\n\n\\[2\\] Introducing EuroSAT: A Novel Dataset and Deep Learning Benchmark\nfor Land Use and Land Cover Classification. Patrick Helber, Benjamin\nBischke, Andreas Dengel. 2018 IEEE International Geoscience and Remote\nSensing Symposium, 2018.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import urllib.request\nimport zipfile\n\nimport numpy as np\n\nurl = 'https://figshare.com/ndownloader/files/34912884'\nurllib.request.urlretrieve(url, 'EuroSAT_data.zip')\n\nwith zipfile.ZipFile('EuroSAT_data.zip', 'r') as zip_ref:\n    zip_ref.extractall('EuroSAT')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading a Simple Classification Dataset\n=======================================\n\nA simple classification dataset is an image dataset structured in the\nfollowing way:\n\n> -   \n>\n>     root/\n>\n>     :   -   \n>\n>             train/\n>\n>             :   -   \n>\n>                     class1/\n>\n>                     :   image1.jpeg\n>\n>         -   \n>\n>             test/\n>\n>             :   -   \n>\n>                     class1/\n>\n>                     :   image1.jpeg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision import classification_dataset_from_directory\n\ntrain_ds, test_ds = classification_dataset_from_directory(\n    root='./EuroSAT/euroSAT/', object_type='VisionData', image_extension='jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running Deepchecks\\' `train_test_validation` suite\n==================================================\n\nThat\\'s it, we have just defined the classification data object and are\nready can run the different deepchecks suites and checks. Here we will\ndemonstrate how to run train\\_test\\_validation suite:\n\nfor additional information on the different suites and checks available\nsee our `Vision Checks </checks_gallery/vision>`{.interpreted-text\nrole=\"doc\"} gallery.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.suites import train_test_validation\n\nsuite = train_test_validation()\nresult = suite.run(train_ds, test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observing the Results\n=====================\n\nThe results can be saved as a html file with the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.save_as_html('output.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or, if working inside a notebook, the output can be displayed directly\nby simply printing the result object:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding the Results\n=========================\n\nLooking at the results we see two checks whose conditions have failed:\n\n1.  Similar Image Leakage\n2.  Feature Label Correlation\n\nThe first has clearly failed due to the naturally occurring similarity\nbetween different ocean / lake image, and the prevailing green of some\nforest images. We may wish to remove some of these duplicate images but\nfor this dataset they make sense.\n\nThe second failure is more interesting. The\n`Property Label Correlation Change\n</checks_gallery/vision/train_test_validation/plot_property_label_correlation_change>`{.interpreted-text\nrole=\"doc\"} check computes various\n`image properties </user-guide/vision/vision_properties>`{.interpreted-text\nrole=\"doc\"} and checks if the image label can be inferred using a simple\nmodel (for example, a Classification Tree) using the property values.\nThe ability to predict the label using these properties is measured by\nthe Predictive Power Score (PPS) and this measure is compared between\nthe training and test dataset. In this case, the condition alerts us to\nthe fact that this PPS for the \\\"RMS Contrast\\\" property was\nsignificantly higher in the training dataset than in the test dataset.\n\nWe\\'ll show the relevant plot again for ease of discussion:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check_idx = np.where([result.results[i].check.name() == 'Property Label Correlation Change'\n                      for i in range(len(result.results))])[0][0]\nresult.results[check_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can see the plot dedicated to the PPS of the property RMS\nContrast, which measures the contrast in the image by calculating the\ngrayscale standard deviation of the image. This plot shows us that\nspecifically for the classes \\\"Forest\\\" and \\\"SeaLake\\\" (the same\nculprits from the Similar Image Leakage condition), the contrast is a\ngreat predictor, but only in the training data! This means we have a\ncritical problem - or model may learn to classify these classes using\nonly the contrast, without actually learning anything about the image\ncontent. We now can go on and fix this issue (perhaps by adding train\naugmentations, or enriching our training set) even before we start\nthinking about what model to train for the task.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}