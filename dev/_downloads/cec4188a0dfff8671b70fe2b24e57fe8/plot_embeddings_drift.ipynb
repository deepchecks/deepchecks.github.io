{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embeddings Drift {#nlp__embeddings_drift}\n================\n\nThis notebooks provides an overview for using and understanding the\nembeddings drift check.\n\n**Structure:**\n\n-   [What Is Embeddings Drift?](#what-is-embeddings-drift)\n-   [Loading the Data](#load-data)\n-   [Run the Check](#run-check)\n\nWhat Is Embeddings Drift?\n-------------------------\n\nDrift is simply a change in the distribution of data over time, and it\nis also one of the top reasons why machine learning model\\'s performance\ndegrades over time.\n\nIn unstructured data such as text, we cannot measure the drift of the\ndata directly, as there\\'s no \\\"distribution\\\" to measure. In order to\nmeasure the drift of the data, we can use the model\\'s embeddings as a\nproxy for the data distribution.\n\nFor more on embeddings, see the\n`Text Embeddings Guide <nlp__embeddings_guide>`{.interpreted-text\nrole=\"ref\"}.\n\nThis detects embeddings drift by using\n`a domain classifier <drift_detection_by_domain_classifier>`{.interpreted-text\nrole=\"ref\"}. For more information on drift, see the\n`Drift Guide <drift_user_guide>`{.interpreted-text role=\"ref\"}.\n\nHow Does This Check Work?\n-------------------------\n\nThis check detects the embeddings drift by using\n`a domain classifier <drift_detection_by_domain_classifier>`{.interpreted-text\nrole=\"ref\"}, and uses the AUC score of the classifier as the basis for\nthe measure of drift. For efficiency, the check first reduces the\ndimensionality of the embeddings, and then trains the classifier on the\nreduced embeddings. By default, the check uses UMAP for dimensionality\nreduction, but you can also use PCA by setting the\n[dimension\\_reduction\\_method]{.title-ref} parameter to\n[pca]{.title-ref}.\n\nThe check also provides a scatter plot of the embeddings, which is a 2D\nrepresentation of the embeddings space. This is achieved by further\nreducing the dimensionality, using UMAP.\n\nHow To Use Embeddings in Deepchecks?\n------------------------------------\n\nSee how to calculate default embeddings or setting your own embeddings\nin the\n`Embeddings Guide <using_nlp_embeddings_in_checks>`{.interpreted-text\nrole=\"ref\"}.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.datasets.classification import tweet_emotion\nfrom deepchecks.nlp.checks import TextEmbeddingsDrift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Data\n=========\n\nFor this example, we\\'ll use the tweet emotion dataset, which is a\ndataset of tweets labeled by one of four emotions: happiness, anger,\nsadness and optimism.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_ds, test_ds = tweet_emotion.load_data()\ntrain_embeddings, test_embeddings = tweet_emotion.load_embeddings(as_train_test=True)\n\n# Set the embeddings in the datasets:\ntrain_ds.set_embeddings(train_embeddings)\ntest_ds.set_embeddings(test_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s see how our data looks like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_ds.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run Check\n=========\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As there\\'s natural drift in this dataset, we can expect to see some\ndrift in the data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = TextEmbeddingsDrift()\nresult = check.run(train_dataset=train_ds, test_dataset=test_ds)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observing the results\n=====================\n\nWe can see that the check found drift in the data. Moreover, we can\ninvestigate the drift by looking at the scatter plot, which is a 2D\nrepresentation of the embeddings space. We can see that there are a few\nclusters in the graph where there are more tweets from the test dataset\nthan the train dataset. This is a sign of drift in the data. By hovering\nover the points, we can see the actual tweets that are in the dataset,\nand see for example that there are clusters of tweets about motivational\nquotes, which are more common in the test dataset than the train\ndataset.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}