{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unknown Tokens {#nlp__unknown_tokens}\n==============\n\nThis notebook provides an overview for using and understanding the\nUnknown Tokens check.\n\n**Structure:**\n\n-   [What is the purpose of the\n    check?](#what-is-the-purpose-of-the-check)\n-   [Generate data & model](#generate-data-model)\n-   [Run the check](#run-the-check)\n-   [Using the Check Value](#using-the-check-value)\n-   [Define a condition](#define-a-condition)\n\nWhat is the purpose of the check?\n---------------------------------\n\nThe Unknown Tokens check is designed to help you identify samples that\ncontain tokens not supported by your tokenizer. These not supported\ntokens can lead to poor model performance, as the model may not be able\nto understand the meaning of such tokens. By identifying these unknown\ntokens, you can take appropriate action, such as updating your tokenizer\nor preprocessing your data to handle them.\n\nGenerate data & model\n---------------------\n\nIn this example, we\\'ll use the twitter dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.datasets.classification import tweet_emotion\n\ndataset, _ = tweet_emotion.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n\nThe check has several key parameters that affect its behavior and\noutput:\n\n-   \\`tokenizer\\`: Tokenizer from the HuggingFace transformers library\n    to use for tokenization. If None,\n    BertTokenizer.from\\_pretrained(\\'bert-base-uncased\\') will be used.\n-   \\`group\\_singleton\\_words\\`: If True, group all words that appear\n    only once in the data into the \\\"Other\\\" category in the display.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.checks import UnknownTokens\n\ncheck = UnknownTokens()\nresult = check.run(dataset)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observe the check\\'s output\n===========================\n\nWe see in the results that the check found many emojis and some foreign\nwords (Korean, can be seen by hovering over the \\\"Other Unknown Words\\\"\nslice of the pie chart) that are not supported by the tokenizer. We can\nalso see that the check grouped all words that appear only once in the\ndata into the \\\"Other\\\"\n\nUse a Different Tokenizer\n=========================\n\nWe can also use a different tokenizer, such as the GPT2 tokenizer, to\nsee how the results change.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n\nUnknownTokens(tokenizer=tokenizer).run(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the Check Value\n=====================\n\nOn top of observing the check\\'s display, we can use the check\\'s\nreturned value to get more information about the words containing\nunknown tokens in our dataset. The check\\'s value is a nested dictionary\nwith the following keys:\n\n1.  `unknown_word_ratio`: The ratio of unknown words out of all words in\n    the dataset.\n2.  `unknown_word_details`: This is in turn also a dict, containing a\n    key for each unknown word. The value for each key is a dict\n    containing \\'ratio\\' (the ratio of the unknown word out of all words\n    in the dataset) and \\'indexes\\' (the indexes of the samples\n    containing the unknown word).\n\nWe\\'ll show here how you can use this value to get the individual\nsamples containing unknown tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n\nunknown_word_details = result.value['unknown_word_details']\nfirst_unknown_word = list(unknown_word_details.keys())[0]\nprint(f\"Unknown word: {first_unknown_word}\")\n\nword_indexes = unknown_word_details[first_unknown_word]['indexes']\npprint(dataset.text[word_indexes].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the GPT2 tokenizer supports emojis, so the check did not\nfind any unknown tokens.\n\nDefine a condition\n==================\n\nWe can add a condition that validates the ratio of unknown words in the\ndataset is below a certain threshold. This can be useful to ensure that\nyour dataset does not have a high percentage of unknown tokens, which\nmight negatively impact the performance of your model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check.add_condition_ratio_of_unknown_words_less_or_equal(0.005)\nresult = check.run(dataset)\nresult.show(show_additional_outputs=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, the condition checks if the ratio of unknown words is\nless than or equal to 0.005 (0.5%). If the ratio is higher than the\nthreshold, the condition will fail, indicating a potential issue with\nthe dataset.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}