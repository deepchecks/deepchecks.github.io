{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Error Analysis\n====================\n\nThis notebooks provides an overview for using and understanding the\nmodel error analysis check.\n\n**Structure:**\n\n-   [What is Model Error Analysis?](#what-is-model-error-analysis)\n-   [Run the check](#run-the-check)\n-   [Define a condition](#define-a-condition)\n\nWhat is Model Error Analysis?\n-----------------------------\n\nEvaluating the model\\'s overall performance metrics gives a good\nhigh-level overview and can be useful for tracking model progress during\ntraining of for comparing models. However, when it\\'s time to fully\nevaluate if a model is fit for production, or when you\\'re interested in\na deeper understanding of your model\\'s performance in order to improve\nit or to be aware of its weaknesses, it\\'s recommended to look deeper at\nhow the model performs on various segments of the data. The model error\nanalysis check searches for data segments in which the model error is\nsignificantly lower from the model error of the dataset as a whole.\n\n### Algorithm:\n\n1.  Computes the per-sample loss (for log-loss for classification, mse\n    for regression).\n2.  Trains a regression model to predict the error of the user\\'s model,\n    based on the input features.\n3.  Repeat stage 2 several times with various tree parameters and random\n    states to ensure that the most relevant partitions for model error\n    are selected.\n4.  The features scoring the highest feature importance for the error\n    regression model are selected and the distribution of the error vs\n    the feature values is plotted.\n\nThe check results are shown only if the error regression model manages\nto predict the error well enough (above a given r squared performance\nthreshold, defined by the min\\_error\\_model\\_score parameter and set by\ndefault to 0.5). The resulting plots show the distribution of the error\nfor the features that are most effective at segmenting the error to high\nand low values, without need for manual selection of segmentation\nfeatures.\n\n### Related Checks:\n\nWhen the important segments of the data are known in advance (when we\nknow that some population segments have different behaviours and\nbusiness importance, for example income levels or state of residence) it\nis possible to just have a look at the performance at various\npre-defined segments. In deepchecks, this can be done using the\n`Segment Performance </checks_gallery/tabular/performance/plot_segment_performance>`{.interpreted-text\nrole=\"doc\"} check, which shows the performance for segments defined by\ncombination of values from two pre-defined columns.\n\nRun the check\n-------------\n\nWe will run the check on the adult dataset which can be downloaded from\nthe [UCI machine learning repository](http://archive.ics.uci.edu/ml) and\nis also available in [deepchecks.tabular.datasets]{.title-ref}.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.datasets.classification import adult\nfrom deepchecks.tabular.checks import ModelErrorAnalysis\n\ntrain_ds, test_ds = adult.load_data(data_format='Dataset', as_train_test=True)\nmodel = adult.load_fitted_model()\n\n# We create the check with a slightly lower r squared threshold to ensure that the check can run on the example dataset.\ncheck = ModelErrorAnalysis(min_error_model_score=0.3)\nresult = check.run(train_ds, test_ds, model)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The check has found that the features \\'hours-per-week\\', \\'age\\' and\n\\'relationship\\' are the most predictive of differences in the model\nerror. We can further investigate the model performance by passing two\nof these columns to the\n`Segment Performance </checks_gallery/tabular/performance/plot_segment_performance>`{.interpreted-text\nrole=\"doc\"} check:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.checks import SegmentPerformance\n\nSegmentPerformance(feature_1='age', feature_2='relationship').run(test_ds, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From which we learn that the model error is exceptionally higher for\npeople in the \\\"Husband\\\" or \\\"Other\\\" status, except for the lower age\ngroups for which the error is lower.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nWe can define a condition that enforces that the relative difference\nbetween the weak and strong segments is not greater than a certain\nratio, for example ratio of 0.05\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = check.add_condition_segments_performance_relative_difference_not_greater_than(0.05)\nresult = check.run(train_ds, test_ds, model)\nresult.show(show_additional_outputs=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}