{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using Pre-computed Predictions\n==============================\n\nThere are several cases in which it\\'s useful to compute the model\npredictions before using deepchecks.\n\nIn some cases the model evaluation can take a long time, i.e. for a very\nlarge dataset. This feature can also be helpful if the Model inference\nhappens on a production node and you can get the predictions using an\napi. You can also use this feature to run deepchecks on models that do\nnot support the sklearn api.\n\n::: {.note}\n::: {.admonition-title}\nNote\n:::\n\nIf the train dataset shares indices with the test dataset we will add\ntrain/test prefixes. This will cause the IndexTrainTestLeakage condition\nto pass even when leakage is present in cases where the DataFrame index\nis also defined as the deepchecks Dataset index.\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate data & model\n=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.datasets.classification.iris import (\n    load_data, load_fitted_model)\n\ntrain_dataset, test_dataset = load_data()\nmodel = load_fitted_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set Up The Required Data\n========================\n\nWe will calculate the feature importance which is optional but will\naffect some displays and checks. (feature importance can also be\nprovided from other sources (e.g. using a custom model that has FI as a\nproperty, shap, etc))\n\nWe are also calculating all the model predict\\_proba results.\n\nFor regression we would provide the predict result, we can also provide\nthe predict result explicitly for classification (on default argmax will\nbe used to calculate them). In order to pass the decisions, we can\nprovide them using the [y\\_pred\\_train]{.title-ref} and\n[y\\_pred\\_test]{.title-ref} arguments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.utils.features import _calculate_feature_importance\n\nfeature_importance, _ = _calculate_feature_importance(model, test_dataset)\n\ntrain_proba = model.predict_proba(train_dataset.features_columns)\ntest_proba = model.predict_proba(test_dataset.features_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run a Deepchecks Suite Using The Static Predictions\n===================================================\n\nRun the model\\_evaluation suite\n-------------------------------\n\nWe will now pass the feature importance and the predictions we\ncalculated beforehand.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.suites import model_evaluation\n\nsuite = model_evaluation()\nresult = suite.run(train_dataset=train_dataset, test_dataset=test_dataset,\n                   features_importance=feature_importance,\n                   y_proba_train=train_proba, y_proba_test=test_proba)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observing the results:\n======================\n\nThe results can be saved as a html file with the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.save_as_html('output.html')\n\nresult = suite.run(train_dataset=train_dataset, test_dataset=test_dataset,\n                   features_importance=feature_importance,\n                   y_proba_train=train_proba, y_proba_test=test_proba)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Export the results to HTML report\n=================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.save_as_html('report.html')\n\nresult"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}