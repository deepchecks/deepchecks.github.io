{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mean Average Precision Report {#vision__mean_average_precision_report}\n=============================\n\nThis notebooks provides an overview for using and understanding the mean\naverage precision report check.\n\n**Structure:**\n\n-   [What is the purpose of the\n    check?](#what-is-the-purpose-of-the-check)\n-   [Generate Dataset](#generate-dataset)\n-   [Run the check](#run-the-check)\n-   [Define a condition](#define-a-condition)\n\nWhat Is the Purpose of the Check?\n---------------------------------\n\nThe Mean Average Precision Report evaluates the [mAP\nmetric](https://manalelaidouni.github.io/Evaluating-Object-Detection-Models-Guide-to-Performance-Metrics.html)\non the given model and data, plots the AP on graph, and returns the mAP\nvalues per bounding box size category (small, medium, large). This check\nonly works on the Object Detection task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate Dataset\n================\n\nWe generate a sample dataset of 128 images from the [COCO\ndataset](https://cocodataset.org/#home), and using the [YOLOv5\nmodel](https://github.com/ultralytics/yolov5).\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nIn this example, we use the pytorch version of the coco dataset and\nmodel. In order to run this example using tensorflow, please change the\nimport statements to:\n\nfrom deepchecks.vision.datasets.detection import coco\\_tensorflow as\ncoco\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.checks import MeanAveragePrecisionReport\nfrom deepchecks.vision.datasets.detection import coco_torch as coco\n\ntest_ds = coco.load_dataset(train=False, object_type='VisionData')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = MeanAveragePrecisionReport()\nresult = check.run(test_ds)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To display the results in an IDE like PyCharm, you can use the following\ncode:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#  result.show_in_window()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result will be displayed in a new window.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observe the check's output\n==========================\n\nThe result value is a dataframe that has the Mean Average Precision\nscore for different bounding box area sizes. We report the mAP for\ndifferent IoU thresholds: 0.5, 0.75 and an average of mAP values for IoU\nthresholds between 0.5 and 0.9 (with a jump size of 0.05).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nWe can define a condition that checks whether our model\\'s mean average\nprecision score is not less than a given threshold for all bounding box\nsizes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = MeanAveragePrecisionReport().add_condition_average_mean_average_precision_greater_than(0.4)\nresult = check.run(test_ds)\nresult.show(show_additional_outputs=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}