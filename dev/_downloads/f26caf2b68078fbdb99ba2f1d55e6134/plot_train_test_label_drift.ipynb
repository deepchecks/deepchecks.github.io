{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Test Label Drift\n======================\n\nThis notebooks provides an overview for using and understanding label\ndrift check.\n\n**Structure:**\n\n-   [What Is Label Drift?](#what-is-label-drift)\n-   [Run Check on a Classification\n    Label](#run-check-on-a-classification-label)\n-   [Run Check on a Regression Label](#run-check-on-a-regression-label)\n-   [Add a Condition](#run-check)\n\nWhat Is Label Drift?\n--------------------\n\nDrift is simply a change in the distribution of data over time, and it\nis also one of the top reasons why machine learning model\\'s performance\ndegrades over time.\n\nLabel drift is when drift occurs in the label itself.\n\nFor more information on drift, please visit our\n`drift guide </user-guide/general/drift_guide>`{.interpreted-text\nrole=\"doc\"}.\n\n### How Deepchecks Detects Label Drift\n\nThis check detects label drift by using\n`univariate measures <drift_detection_by_univariate_measure>`{.interpreted-text\nrole=\"ref\"} on the label column.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pprint\n\nimport numpy as np\nimport pandas as pd\n\nfrom deepchecks.tabular import Dataset\nfrom deepchecks.tabular.checks import TrainTestLabelDrift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run Check on a Classification Label\n===================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate data:\n# --------------\n\nnp.random.seed(42)\n\ntrain_data = np.concatenate([np.random.randn(1000,2), np.random.choice(a=[1,0], p=[0.5, 0.5], size=(1000, 1))], axis=1)\n#Create test_data with drift in label:\ntest_data = np.concatenate([np.random.randn(1000,2), np.random.choice(a=[1,0], p=[0.35, 0.65], size=(1000, 1))], axis=1)\n\ndf_train = pd.DataFrame(train_data, columns=['col1', 'col2', 'target'])\ndf_test = pd.DataFrame(test_data, columns=['col1', 'col2', 'target'])\n\ntrain_dataset = Dataset(df_train, label='target')\ntest_dataset = Dataset(df_test, label='target')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run Check\n=========\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = TrainTestLabelDrift()\nresult = check.run(train_dataset=train_dataset, test_dataset=test_dataset)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run Check on a Regression Label\n===============================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate data:\n# --------------\n\ntrain_data = np.concatenate([np.random.randn(1000,2), np.random.randn(1000, 1)], axis=1)\ntest_data = np.concatenate([np.random.randn(1000,2), np.random.randn(1000, 1)], axis=1)\n\ndf_train = pd.DataFrame(train_data, columns=['col1', 'col2', 'target'])\ndf_test = pd.DataFrame(test_data, columns=['col1', 'col2', 'target'])\n#Create drift in test:\ndf_test['target'] = df_test['target'].astype('float') + abs(np.random.randn(1000)) + np.arange(0, 1, 0.001) * 4\n\ntrain_dataset = Dataset(df_train, label='target')\ntest_dataset = Dataset(df_test, label='target')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run check\n=========\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = TrainTestLabelDrift()\nresult = check.run(train_dataset=train_dataset, test_dataset=test_dataset)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add a Condition\n===============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check_cond = TrainTestLabelDrift().add_condition_drift_score_less_than()\ncheck_cond.run(train_dataset=train_dataset, test_dataset=test_dataset)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}