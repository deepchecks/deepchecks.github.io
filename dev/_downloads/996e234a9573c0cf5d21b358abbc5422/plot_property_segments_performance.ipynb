{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Property Segments Performance {#nlp__property_segments_performance}\n=============================\n\nThis notebook provides an overview for using and understanding the\nproperty segment performance check.\n\n**Structure:**\n\n-   [What is the purpose of the\n    check?](#what-is-the-purpose-of-the-check)\n-   [Automatically detecting weak\n    segments](#automatically-detecting-weak-segments)\n-   [Generate data & model](#generate-data-model)\n-   [Run the check](#run-the-check)\n-   [Define a condition](#define-a-condition)\n\nWhat is the purpose of the check?\n---------------------------------\n\nThe check is designed to help you easily identify the model\\'s weakest\nsegments based on the provided\n`properties <nlp__properties_guide>`{.interpreted-text role=\"ref\"}. In\naddition, it enables to provide a sublist of the metadata columns, thus\nlimiting the check to search in interesting subspaces.\n\nAutomatically detecting weak segments\n-------------------------------------\n\nThe check contains several steps:\n\n1.  We calculate loss for each sample in the dataset using the provided\n    model via either log-loss or MSE according to the task type.\n2.  We train multiple simple tree based models, each one is trained\n    using exactly two properties (out of the ones selected above) to\n    predict the per sample error calculated before.\n3.  We extract the corresponding data samples for each of the leaves in\n    each of the trees (data segments) and calculate the model\n    performance on them. For the weakest data segments detected we also\n    calculate the model\\'s performance on data segments surrounding\n    them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate data & model\n=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.datasets.classification.tweet_emotion import load_data, load_precalculated_predictions\n\n_, test_dataset = load_data(data_format='TextData')\n_, test_probas = load_precalculated_predictions(pred_format='probabilities')\n\ntest_dataset.properties.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n\nThe check has several key parameters (that are all optional) that affect\nthe behavior of the check and especially its output.\n\n`properties / ignore_properties`: Controls which properties should be\nsearched for weak segments. By default, uses all properties data\nprovided.\n\n`alternative_scorer`: Determines the metric to be used as the\nperformance measurement of the model on different segments. It is\nimportant to select a metric that is relevant to the data domain and\ntask you are performing. For additional information on scorers and how\nto use them see `Metrics Guide <metrics_user_guide>`{.interpreted-text\nrole=\"ref\"}.\n\n`segment_minimum_size_ratio`: Determines the minimum size of segments\nthat are of interest. The check will return data segments that contain\nat least this fraction of the total data samples. It is recommended to\ntry different configurations of this parameter as larger segments can be\nof interest even the model performance on them is superior.\n\n`categorical_aggregation_threshold`: By default the check will combine\nrare categories into a single category called \\\"Other\\\". This parameter\ndetermines the frequency threshold for categories to be mapped into to\nthe \\\"other\\\" category.\n\nsee\n`API reference <deepchecks.tabular.checks.model_evaluation.WeakSegmentsPerformance>`{.interpreted-text\nrole=\"class\"} for more details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.checks import PropertySegmentsPerformance\nfrom sklearn.metrics import make_scorer, f1_score\n\nscorer = {'f1': make_scorer(f1_score, average='micro')}\ncheck = PropertySegmentsPerformance(alternative_scorer=scorer,\n                                    segment_minimum_size_ratio=0.03)\nresult = check.run(test_dataset, probabilities=test_probas)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observe the check\\'s output\n===========================\n\nWe see in the results that the check indeed found several segments on\nwhich the model performance is below average. In the heatmap display we\ncan see model performance on the weakest segments and their environment\nwith respect to the two features that are relevant to the segment. In\norder to get the full list of weak segments found we will inspect the\n`result.value` attribute. Shown below are the 3 segments with the worst\nperformance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.value['weak_segments_list'].head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nWe can add a condition that will validate the model\\'s performance on\nthe weakest segment detected is above a certain threshold. A scenario\nwhere this can be useful is when we want to make sure that the model is\nnot under performing on a subset of the data that is of interest to us.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Let's add a condition and re-run the check:\n\ncheck = PropertySegmentsPerformance(alternative_scorer=scorer, segment_minimum_size_ratio=0.03)\ncheck.add_condition_segments_relative_performance_greater_than(0.1)\nresult = check.run(test_dataset, probabilities=test_probas)\nresult.show(show_additional_outputs=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}