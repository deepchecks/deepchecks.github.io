{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quickstart - Model Evaluation Suite {#quick_model_evaluation}\n===================================\n\nThe deepchecks model evaluation suite is relevant any time you wish to\nevaluate your model. For example:\n\n-   Thorough analysis of the model\\'s performance before deploying it.\n-   Evaluation of a proposed model during the model selection and\n    optimization stage.\n-   Checking the model\\'s performance on a new batch of data (with or\n    without comparison to previous data batches).\n\nHere we\\'ll build a regression model using the wine quality dataset\n(`deepchecks.tabular.datasets.regression.wine_quality`{.interpreted-text\nrole=\"mod\"}), to demonstrate how you can run the suite with only a few\nsimple lines of code, and see which kind of insights it can find.\n\n``` {.sourceCode .bash}\n# Before we start, if you don't have deepchecks installed yet, run:\nimport sys\n!{sys.executable} -m pip install deepchecks -U --quiet\n\n# or install using pip from your python environment\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare Data and Model\n======================\n\nLoad Data\n---------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.datasets.regression import wine_quality\n\ndata = wine_quality.load_data(data_format='Dataframe', as_train_test=False)\ndata.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split Data and Train a Simple Model\n===================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nX_train, X_test, y_train, y_test = train_test_split(data.iloc[:, :-1], data['quality'], test_size=0.2)\ngbr = GradientBoostingRegressor()\ngbr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run Deepchecks for Model Evaluation\n===================================\n\nCreate a Dataset Object\n-----------------------\n\nCreate a deepchecks Dataset, including the relevant metadata (label,\ndate, index, etc.). Check out\n`deepchecks.tabular.Dataset`{.interpreted-text role=\"class\"} to see all\nthe column types and attributes that can be declared.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular import Dataset\n\n# Categorical features can be heuristically inferred, however we\n# recommend to state them explicitly to avoid misclassification.\n\n# Metadata attributes are optional. Some checks will run only if specific attributes are declared.\n\ntrain_ds = Dataset(X_train, label=y_train, cat_features=[])\ntest_ds = Dataset(X_test, label=y_test, cat_features=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the Deepchecks Suite\n========================\n\nValidate your data with the\n`deepchecks.tabular.suites.model_evaluation`{.interpreted-text\nrole=\"class\"} suite. It runs on two datasets and a model, so you can use\nit to compare the performance of the model between any two batches of\ndata (e.g. train data, test data, a new batch of data that recently\narrived)\n\nCheck out the\n`\"when should you use deepchecks guide\" </getting-started/when_should_you_use>`{.interpreted-text\nrole=\"doc\"} for some more info about the existing suites and when to use\nthem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.suites import model_evaluation\n\nevaluation_suite = model_evaluation()\nsuite_result = evaluation_suite.run(train_ds, test_ds, gbr)\n# Note: the result can be saved as html using suite_result.save_as_html()\n# or exported to json using suite_result.to_json()\nsuite_result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analyzing the results\n=====================\n\nThe result showcase a number of interesting insights, first let\\'s\ninspect the \\\"Didn\\'t Pass\\\" section.\n\n-   `/checks_gallery/tabular/model_evaluation/plot_performance_report`{.interpreted-text\n    role=\"doc\"} check result implies that the model overfitted the\n    training data.\n-   `/checks_gallery/tabular/model_evaluation/plot_regression_systematic_error`{.interpreted-text\n    role=\"doc\"} (test set) check result demonstrate the model small\n    positive bias.\n-   `/checks_gallery/tabular/model_evaluation/plot_weak_segments_performance`{.interpreted-text\n    role=\"doc\"} (test set) check result visualize some specific\n    sub-spaces on which the model performs poorly. Examples for those\n    sub-spaces are wines with low total sulfur dioxide and wines with\n    high alcohol percentage.\n\nNext, let\\'s examine the \\\"Passed\\\" section.\n\n-   `/checks_gallery/tabular/model_evaluation/plot_simple_model_comparison`{.interpreted-text\n    role=\"doc\"} check result states that the model performs better than\n    naive baseline models, an opposite result could indicate a problem\n    with the model or the data it was trained on.\n-   `/checks_gallery/tabular/model_evaluation/plot_boosting_overfit`{.interpreted-text\n    role=\"doc\"} check and the\n    `/checks_gallery/tabular/model_evaluation/plot_unused_features`{.interpreted-text\n    role=\"doc\"} check results implies that the model has a well\n    calibrating boosting stopping rule and that it make good use on the\n    different data features.\n\nLet\\'s try and fix the overfitting issue found in the model.\n\nFix the Model and Re-run a Single Check\n\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\\^\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.checks import TrainTestPerformance\n\ngbr = GradientBoostingRegressor(n_estimators=20)\ngbr.fit(X_train, y_train)\n# Initialize the check and add an optional condition\ncheck = TrainTestPerformance().add_condition_train_test_relative_degradation_less_than(0.3)\nresult = check.run(train_ds, test_ds, gbr)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We mitigated the overfitting to some extent. Additional model tuning is\nrequired to overcome other issues discussed above. For now, we will\nupdate and remove the relevant conditions from the suite.\n\nUpdating an Existing Suite\n==========================\n\nTo create our own suite, we can start with an empty suite and add checks\nand condition to it (see\n`/user-guide/general/customizations/examples/plot_create_a_custom_suite`{.interpreted-text\nrole=\"doc\"}), or we can start with one of the default suites and update\nit as demonstrated in this section.\n\nlet\\'s inspect our model evaluation suite\\'s structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evaluation_suite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will update the Performance Report condition and remove the\nRegression Systematic Error check:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evaluation_suite[0].clean_conditions()\nevaluation_suite[0].add_condition_train_test_relative_degradation_less_than(0.3)\nevaluation_suite = evaluation_suite.remove(7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Re-run the suite using:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result = evaluation_suite.run(train_ds, test_ds, gbr)\nresult.passed(fail_if_warning=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For more info about working with conditions, see the detailed\n`/user-guide/general/customizations/examples/plot_configure_check_conditions`{.interpreted-text\nrole=\"doc\"} guide.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}