{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simple Model Comparison {#plot_vision_simple_model_comparison}\n=======================\n\nThis notebooks provides an overview for using and understanding simple\nmodel comparison check.\n\n**Structure:**\n\n-   [What Is the Purpose of the\n    Check?](#what-is-the-purpose-of-the-check)\n-   [Generate data an model](#generate-data-and-model)\n-   [Run the check](#run-the-check)\n\nWhat Is the Purpose of the Check?\n---------------------------------\n\nThis check compares your current model to a \\\"simple model\\\", which is a\nmodel designed to produce the best performance achievable using very\nsimple rules, such as \\\"always predict the most common class\\\". The\nsimple model is used as a **baseline** model; If your model achieves\nless or similar score to the simple model, this is an indicator of a\npossible problem with the model (e.g. it wasn\\'t trained properly).\n\nUsing the parameter `strategy`, you can select the simple model used in\nthe check:\n\n  Strategy          Description\n  ----------------- ----------------------------------------------------------------------------------------------------------------------------------------------------\n  prior (default)   The probability vector always contains the empirical class prior distribution (i.e. the class distribution observed in the training set).\n  most\\_frequent    The most frequent prediction is predicted. The probability vector is 1 for the most frequent prediction and 0 for the other predictions.\n  stratified        The predictions are generated by sampling one-hot vectors from a multinomial distribution parametrized by the empirical class prior probabilities.\n  uniform           Generates predictions uniformly at random from the list of unique classes observed in y, i.e. each class has equal probability.\n\nSimiliar to the `tabular simple model comparison check\n</checks_gallery/tabular/model_evaluation/plot_simple_model_comparison>`{.interpreted-text\nrole=\"doc\"}, there is no simple model which is more \\\"correct\\\" to use,\neach gives a different baseline to compare to, and you may experiment\nwith the different types and see how it performs on your data.\n\nThis checks applies only to classification datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate data and model\n=======================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.checks import SimpleModelComparison\nfrom deepchecks.vision.datasets.classification import mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mnist_model = mnist.load_model()\ntrain_ds = mnist.load_dataset(train=True, object_type='VisionData')\ntest_ds = mnist.load_dataset(train=False, object_type='VisionData')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n\nWe will run the check with the prior model type. The check will use the\ndefault classification metrics - precision and recall. This can be\noverridden by providing an alternative scorer using the\n`alternative_metrics`\\` parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = SimpleModelComparison(strategy='stratified')\nresult = check.run(train_ds, test_ds, mnist_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you have a GPU, you can speed up this check by passing it as an\nargument to .run() as device=\\<your GPU\\>\n\nTo display the results in an IDE like PyCharm, you can use the following\ncode:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#  result.show_in_window()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result will be displayed in a new window.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observe the check\\'s output\n===========================\n\nWe can see in the results that the check calculates the score for each\nclass in the dataset, and compares the scores between our model and the\nsimple model.\n\nIn addition to the graphic output, the check also returns a value which\nincludes all of the information that is needed for defining the\nconditions for validation.\n\nThe value is a dataframe that contains the metrics\\' values for each\nclass and dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.value.sort_values(by=['Class', 'Metric']).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nWe can define on our check a condition that will validate our model is\nbetter than the simple model by a given margin called gain. For\nclassification we check the gain for each class separately and if there\nis a class that doesn\\'t pass the defined gain the condition will fail.\n\nThe performance gain is the percent of the improved performance out of\nthe \\\"remaining\\\" unattained performance. Its purpose is to reflect the\nsignificance of the said improvement. Take for example for a metric\nbetween 0 and 1. A change of only 0.03 that takes us from 0.95 to 0.98\nis highly significant (especially in an imbalance scenario), but\nimproving from 0.1 to 0.13 is not a great achievement.\n\nThe gain is calculated as:\n$gain = \\frac{\\text{model score} - \\text{simple score}}\n{\\text{perfect score} - \\text{simple score}}$\n\nLet\\'s add a condition to the check and see what happens when it fails:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = SimpleModelComparison(strategy='stratified')\ncheck.add_condition_gain_greater_than(min_allowed_gain=0.99)\nresult = check.run(train_ds, test_ds, mnist_model)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We detected that for several classes our gain did not passed the target\ngain we defined, therefore it failed.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}