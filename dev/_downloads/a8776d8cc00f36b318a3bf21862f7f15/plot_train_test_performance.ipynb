{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Test Performance for NLP Models {#nlp__train_test_performance}\n=====================================\n\nThis notebook provides an overview for using and understanding the train\ntest performance check.\n\n**Structure:**\n\n-   [What is the purpose of the\n    check?](#what-is-the-purpose-of-the-check)\n-   [Generate data & predictions](#generate-data-predictions)\n-   [Run the check](#run-the-check)\n-   [Define a condition](#define-a-condition)\n-   [Using a custom scorer](#using-a-custom-scorer)\n\nWhat is the purpose of the check?\n---------------------------------\n\nThis check helps you compare your NLP model\\'s performance between the\ntrain and test datasets based on multiple metrics.\n\nFor Text Classification tasks the supported metrics are sklearn scorers.\nYou may use any of the existing sklearn scorers or create your own. For\nmore information about the supported sklearn scorers, defining your own\nmetrics and to learn how to use metrics for other supported task types,\nsee the `metrics_user_guide`{.interpreted-text role=\"ref\"}.\n\nThe default scorers are F1, Precision, and Recall for Classification,\nand F1 Macro, Recall Macro and Precision Macro for Token Classification.\nSee more about the supported task types at the\n`nlp__supported_tasks`{.interpreted-text role=\"ref\"} guide.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load data & predictions\n=======================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.datasets.classification.tweet_emotion import load_data, load_precalculated_predictions\n\ntrain_dataset, test_dataset = load_data()\ntrain_preds, test_preds = load_precalculated_predictions('predictions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n\nYou can select which scorers to use by passing either a list or a dict\nof scorers to the check, the full list of possible scorers can be seen\nat the `metrics_user_guide`{.interpreted-text role=\"ref\"}.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.checks import TrainTestPerformance\n\ncheck = TrainTestPerformance(scorers=['recall_per_class', 'precision_per_class', 'f1_macro', 'f1_micro'])\nresult = check.run(train_dataset, test_dataset, train_predictions=train_preds, test_predictions=test_preds)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nWe can define on our check a condition that will validate that our model\ndoesn\\'t degrade on new data.\n\nLet\\'s add a condition to the check and see what happens when it fails:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check.add_condition_train_test_relative_degradation_less_than(0.15)\nresult = check.run(train_dataset, test_dataset, train_predictions=train_preds, test_predictions=test_preds)\nresult.show(show_additional_outputs=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We detected that for class \\\"optimism\\\" the Recall has degraded by more\nthan 70%!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using a custom scorer\n=====================\n\nIn addition to the built-in scorers, we can define our own scorer based\non sklearn api and run it using the check alongside other scorers:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import fbeta_score, make_scorer\n\nfbeta_scorer = make_scorer(fbeta_score, labels=np.arange(len(set(test_dataset.label))), average=None, beta=0.2)\n\ncheck = TrainTestPerformance(scorers={'my scorer': fbeta_scorer, 'recall': 'recall_per_class'})\nresult = check.run(train_dataset, test_dataset, train_predictions=train_preds, test_predictions=test_preds)\nresult.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}