{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confusion Matrix Report {#tabular__confusion_matrix_report}\n=======================\n\nThis notebook provides an overview for using and understanding the\nConfusion Matrix Report check.\n\n**Structure:**\n\n-   [What is the Confusion Matrix\n    Report?](#what-is-the-confusion-matrix-report)\n-   [Generate data & model](#generate-data-model)\n-   [Run the check](#run-the-check)\n\nWhat is the Confusion Matrix Report?\n------------------------------------\n\nThe `ConfusionMatrixReport` produces a confusion matrix visualization\nwhich summarizes the performance of the model. The confusion matrix\ncontains the TP (true positive), FP (false positive), TN (true negative)\nand FN (false negative), from which we can derive the relevant metrics,\nsuch as accuracy, precision, recall etc. ([confusion\nmatrix](https://en.wikipedia.org/wiki/Confusion_matrix)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate data & model\n=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import train_test_split\n\nfrom deepchecks.tabular import Dataset\nfrom deepchecks.tabular.checks import ConfusionMatrixReport\n\niris = load_iris(as_frame=True)\nclf = AdaBoostClassifier()\nframe = iris.frame\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\nclf.fit(X_train, y_train)\nds = Dataset(pd.concat([X_test, y_test], axis=1), \n            features=iris.feature_names,\n            label='target')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = ConfusionMatrixReport()\nresult = check.run(ds, clf)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nWe can define our check a condition that will validate if all the\nmisclassified cells/samples in the confusion matrix is below a certain\nthreshold. Using the `misclassified_samples_threshold` argument, we can\nspecify what percentage of the total samples our condition should use to\nvalidate the misclassified cells.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Let's add a condition and re-run the check:\n\ncheck = ConfusionMatrixReport()\ncheck.add_condition_misclassified_samples_lower_than_condition(misclassified_samples_threshold=0.2)\nresult = check.run(ds, clf)\nresult.show()\n\n#%%"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}