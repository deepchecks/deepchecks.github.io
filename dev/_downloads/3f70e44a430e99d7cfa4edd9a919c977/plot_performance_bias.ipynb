{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performance Bias {#tabular__performance_bias}\n================\n\nThis notebook provides an overview for using and understanding the\nPerformance Bias check.\n\n**Structure:**\n\n-   [What is the purpose of the\n    check?](#what-is-the-purpose-of-the-check)\n-   [Generate data & model](#generate-data-model)\n-   [Run the check](#run-the-check)\n-   [Define a condition](#define-a-condition)\n\nWhat is the purpose of the check?\n---------------------------------\n\nThe check is designed to help you identify subgroups for which the model\nhas a much lower performance score than its baseline score (its overall\nperformance). The subgroups are defined by a chosen *protected* feature\n(e.g., \\\"sex\\\", \\\"race\\\") and you can specify a *control* feature (e.g.,\n\\\"education\\\") by which to group the data before computing performance\ndifferences. This is primarily useful for fairness analyses, but can\nalso be used to identify other types of performance disparities.\n\nLarge performance disparities can indicate a problem with the model. The\ntraining data may not be sufficient for certain subgroups or may contain\nbiases, or the model may need to be re-calibrated when applied to\ncertain subgroups. When using appropriate scoring functions, looking at\nperformance disparities can help uncover issues of these kinds.\n\nRemember that this check relies on labeled data provided in the dataset.\nAs such, it can only assess performance disparities to the extent that\nthe labeled data is accurate and representative of the population of\ninterest. Using scoring functions that are robust to class imbalance or\nthat are computed for each model class can help mitigate this issue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate data & model\n=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.datasets.classification.adult import (\n    load_data, load_fitted_model)\n\ntrain_dataset, test_dataset = load_data()\nmodel = load_fitted_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n\nThe check requires the argument `protected_feature` identifying a column\nthat defines the subgroups for which performance disparities are\nassessed. In addition, the check has several optional parameters that\naffect its behavior and output.\n\n-   `control_feature`: Column to use to split the data by groups prior\n    to computing performance disparities.\n-   `scorer`: Scoring function to measure performance. Default to\n    \\\"accuracy\\\" for classification tasks and \\\"r2\\\" for regression\n    tasks.\n-   `max_subgroups_per_control_cat_to_display`: Maximum number of\n    subgroups (per `control_feature` category) to display.\n-   `max_control_cat_to_display`: Maximum number of `control_feature`\n    categories to display.\n\nsee\n`API reference <deepchecks.tabular.checks.model_evaluation.PerformanceBias>`{.interpreted-text\nrole=\"class\"} for more details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.checks.model_evaluation import PerformanceBias\n\ncheck = PerformanceBias(\n   protected_feature=\"race\",\n   control_feature=\"education\",\n   scorer=\"accuracy\",\n   max_segments=3)\nresult = check.run(test_dataset, model)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observe the check\\'s output\n===========================\n\nWe see in the results that the check identified the largest performance\ndisparity for the subgroup \\\"Others\\\" within the category of \\\"HS-grad\\\"\nfor the control feature \\\"education\\\". The model performance on this\nsubgroup is 0.095 versus 0.258 for this entire education category.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.value['scores_df'].head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nWe can define on our check a condition that will validate all\nperformance disparities fall within a certain threshold. If the\ncondition is not met, the check will fail.\n\nLet\\'s add a condition and re-run the check:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check.add_condition_bounded_performance_difference(lower_bound=-0.1)\nresult = check.run(test_dataset, model)\nresult.show(show_additional_outputs=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}