{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Property Label Correlation {#plot_vision_feature_label_correlation}\n==========================\n\nThis notebook provides an overview for using and understanding the\n\\\"Property Label Correlation\\\" check.\n\n**Structure:**\n\n-   [What is the purpose of the\n    check?](#what-is-the-purpose-of-the-check)\n-   [Run the check](#run-the-check)\n-   [Define a condition](#define-a-condition)\n\nWhat is the purpose of the check?\n---------------------------------\n\nThe check estimates for every\n`image property </user-guide/vision/vision_properties>`{.interpreted-text\nrole=\"doc\"} (such as brightness, contrast etc.) its ability to predict\nthe label by itself.\n\nThis check can help find a potential bias in the dataset - the labels\nbeing strongly correlated with simple image properties such as color,\nbrightness, aspect ratio and more.\n\nThis is a critical problem, sometimes referred to as shortcut learning,\nwhere the model is likely to learn this property instead of the actual\nvisual characteristics of each class, as it\\'s easier to do so. In this\ncase, the model will show high performance on images taken in similar\nconditions, but will fail in the wild, where the simple properties\ndon\\'t hold true. This kind of correlation will likely stay hidden\nwithout this check until tested in the wild.\n\nA famous example is the case of wolves vs. dogs classification, where a\nmodel needs to classify whether an image contains a wolf or a dog, and\ncan learn to do it by the background instead of the actual animal - in\nthe dataset most of the wolves were photographed in the snow and\ntherefore had a white background while all the dogs were photographed in\nthe grass and therefore had a green background.\n\nThe check is based on calculating the predictive power score (PPS) of\neach image property. For more details you can read here [how the PPS is\ncalculated](#how-is-the-predictive-power-score-pps-calculated).\n\n### What is a problematic result?\n\nImage properties with a high predictive score can indicate that there is\na bias in the dataset, as a single property can be used to predict the\nlabel successfully (e.g. using simple classic ML algorithms).\n\nThis means that a deep learning algorithm may accidentally learn these\nproperties instead of more accurate complex abstractions. For example,\nin the dataset of wolves and dogs photographs, the brightness of the\nimage may be used to predict the label \\\"wolf\\\" easily.\n\n### How do we calculate the predictive power for different vision tasks?\n\n-   For classification tasks, this check uses PPS to predict the class\n    by image properties.\n-   For object detection tasks, this check uses PPS to predict the class\n    of each bounding box, by the image properties of that specific\n    bounding box. This means that for each image, this check crops all\n    the sub-images defined by bounding boxes, and uses them as inputs as\n    though they were regular classification dataset images.\n\n### How is the Predictive Power Score (PPS) calculated?\n\nThe properties\\' predictive score results in a numeric score between 0\n(feature has no predictive power) and 1 (feature can fully predict the\nlabel alone).\n\nThe process of calculating the PPS is the following:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  Extract from the data only the label and the feature being tested\n2.  Drop samples with missing values\n3.  Keep 5000 (this is configurable parameter) samples from the data\n4.  Preprocess categorical columns. For the label using\n    [sklearn.LabelEncoder]{.title-ref} and for the feature using\n    [sklearn.OneHotEncoder]{.title-ref}\n5.  Partition the data with 4-fold cross-validation\n6.  Train decision tree\n7.  Compare the trained model\\'s performance with naive model\\'s\n    performance as follows:\n\nRegression: The naive model always predicts the median of the label\ncolumn, the metric being used is MAE and the PPS calculation is:\n$1 - \\frac{\\text{MAE model}}{\\text{MAE naive}}$\n\nClassification: The naive model always predicts the most common class of\nthe label column, The metric being used is F1 and the PPS calculation\nis: $\\frac{\\text{F1 model} - \\text{F1 naive}}{1 - \\text{F1 naive}}$\n\n::: {.note}\n::: {.admonition-title}\nNote\n:::\n\nAll the PPS parameters can be changed by passing to the check the\nparameter `ppscore_params`\n:::\n\nFor further information about PPS you can visit the [ppscore\ngithub](https://github.com/8080labs/ppscore) or the following blog post:\n[RIP correlation. Introducing the Predictive Power\nScore](https://towardsdatascience.com/rip-correlation-introducing-the-predictive-power-score-3d90808b9598)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the Check\n=============\n\nIn this example we will run the check on the dataset of wolves vs. dogs.\nFor example purposes we picked 10 images of dogs and 10 images of wolves\nout of the full dataset. The original data was downloaded from\n<https://www.kaggle.com/datasets/harishvutukuri/dogs-vs-wolves>, which\nis licensed under [DbCL\nv1.0](https://opendatacommons.org/licenses/dbcl/1-0/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.checks import PropertyLabelCorrelation\nfrom deepchecks.vision.simple_classification_data import classification_dataset_from_directory\nfrom deepchecks.vision.utils import visualize_vision_data\nimport albumentations as A\nimport urllib.request\nimport zipfile\n\nurl = 'https://figshare.com/ndownloader/files/36671001'\nurllib.request.urlretrieve(url, 'wolves_vs_dogs_mini.zip')\n\nwith zipfile.ZipFile('wolves_vs_dogs_mini.zip', 'r') as zip_ref:\n    zip_ref.extractall('.')\n\ndataset = classification_dataset_from_directory(\n    'wolves_vs_dogs_mini', object_type='VisionData', transforms=A.Resize(128, 128))\ndataset._label_map = {0: 'dog', 1: 'wolf'}  # Replacing the built-in label map \"dogs\" and \"wolves\" with \"dog\" and \"wolf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see an example of the dataset images and their labels below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "visualize_vision_data(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets run the check:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check_result = PropertyLabelCorrelation().run(dataset)\ncheck_result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that both the \\\"Brightness\\\" property and the \\\"Mean Green\nRelative Intensity\\\" property have a significant ability to predict the\nlabel.\n\nThis is as expected - pictures of wolves have higher brightness because\nthey appear with a white background, while dogs appear with a green\nbackground, making \\\"Green-ness\\\" a strong predictor for an image\ncontaining a dog. Using this check we can be made aware of these\nartifacts, and can solve them (for example by collecting images with\ndifferent backgrounds) before training any kind of model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nWe can define a condition to verify that the results are less than a\ncertain threshold.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check_result = PropertyLabelCorrelation().add_condition_property_pps_less_than(0.5).run(dataset)\ncheck_result.show(show_additional_outputs=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now see that the condition failed because the results here are\nabove the threshold.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}