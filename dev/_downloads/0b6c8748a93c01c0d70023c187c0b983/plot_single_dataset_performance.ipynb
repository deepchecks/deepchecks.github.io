{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Single Dataset Performance {#tabular__single_dataset_performance}\n==========================\n\nThis notebook provides an overview for using and understanding the\nsingle dataset performance check.\n\n**Structure:**\n\n-   [What is the purpose of the\n    check?](#what-is-the-purpose-of-the-check)\n-   [Generate data & model](#generate-data-model)\n-   [Run the check](#run-the-check)\n-   [Define a condition](#define-a-condition)\n-   [Using a custom scorer](#using-a-custom-scorer)\n\nWhat is the purpose of the check?\n---------------------------------\n\nThis check is designed for evaluating a model\\'s performance on a\nlabeled dataset based on a scorer or multiple scorers.\n\nScorers are a convention of sklearn to evaluate a model, it is a\nfunction which accepts (model, X, y\\_true) and returns a float result\nwhich is the score. A sklearn convention is that higher scores are\nbetter than lower scores. For additional details [see scorers\ndocumentation](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring).\n\nThe default scorers that are used are F1, Precision, and Recall for\nClassification and Negative Root Mean Square Error, Negative Mean\nAbsolute Error, and R2 for Regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate data & model\n=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.datasets.classification.iris import load_data, load_fitted_model\n\n_, test_dataset = load_data()\nmodel = load_fitted_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n\nYou can select which scorers to use by passing either a list or a dict\nof scorers to the check, see `metrics_user_guide`{.interpreted-text\nrole=\"ref\"} for additional details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.checks import SingleDatasetPerformance\n\ncheck = SingleDatasetPerformance(scorers=['recall_per_class', 'precision_per_class', 'f1_macro', 'f1_micro'])\nresult = check.run(test_dataset, model)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nWe can define on our check a condition to validate that the different\nmetric scores are above a certain threshold. Using the `class_mode`\nargument we can define select a sub set of the classes to use for the\ncondition.\n\nLet\\'s add a condition to the check and see what happens when it fails:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check.add_condition_greater_than(threshold=0.85, class_mode='all')\nresult = check.run(test_dataset, model)\nresult.show(show_additional_outputs=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We detected that the Recall score is below specified threshold in at\nleast one of the classes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using a custom scorer\n=====================\n\nIn addition to the built-in scorers, we can define our own scorer based\non sklearn api and run it using the check alongside other scorers:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import fbeta_score, make_scorer\n\nfbeta_scorer = make_scorer(fbeta_score, labels=[0, 1, 2], average=None, beta=0.2)\n\ncheck = SingleDatasetPerformance(scorers={'my scorer': fbeta_scorer, 'recall': 'recall_per_class'})\nresult = check.run(test_dataset, model)\nresult.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}