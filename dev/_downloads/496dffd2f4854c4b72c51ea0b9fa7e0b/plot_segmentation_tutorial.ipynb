{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Semantic Segmentation Tutorial {#vision__segmentation_tutorial}\n==============================\n\nIn this tutorial, you will learn how to validate your **semantic\nsegmentation model** using deepchecks test suites. You can read more\nabout the different checks and suites for computer vision use cases at\nthe `examples section <vision__checks_gallery>`{.interpreted-text\nrole=\"ref\"}.\n\nIf you just want to see the output of this tutorial, jump to\n`observing_the_result`{.interpreted-text role=\"ref\"} section.\n\nA semantic segmentation task is a task where every pixel of the image is\nlabeled with a single class. Therefore, a common output of these tasks\nis an image of identical size to the input, with a vector for each pixel\nof the probability for each class.\n\n``` {.bash}\n# Before we start, if you don't have deepchecks vision package installed yet, run:\nimport sys\n!{sys.executable} -m pip install \"deepchecks[vision]\" --quiet --upgrade # --user\n\n# or install using pip from your python environment\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the data and model\n===========================\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nIn this tutorial, we use the pytorch to create the dataset and model. To\nsee how this can be done using tensorflow or other frameworks, please\nvisit the `vision__vision_data_class`{.interpreted-text role=\"ref\"}\nguide.\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Data\n=========\n\nThe model in this tutorial is used to detect different object segments\nin images (labels based on the Pascal VOC dataset). The model is trained\nto identify 20 different objects (person, bicycle etc.) and background.\nThe dataset itself is the COCO128 dataset with semantic segmentation\nlabels, mapped to the Pascal VOC labels (Originally, the COCO dataset\nincludes more labels, but those have been filtered out). The dataset can\nbe loaded as a pytorch Dataset object from\ndeepchecks.vision.datasets.segmentation, as is done in this tutorial,\nbut can also be loaded as a VisionData object using the\n\\\"load\\_dataset\\\" function from that directory,\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The full pascal VOC data and information can be found here: http://host.robots.ox.ac.uk/pascal/VOC/voc2012/\n# And the COCO128 dataset can be found here: https://www.kaggle.com/datasets/ultralytics/coco128\n\nfrom deepchecks.vision.datasets.segmentation.segmentation_coco import CocoSegmentationDataset, load_model\n\ntrain_dataset = CocoSegmentationDataset.load_or_download(train=True)\ntest_dataset = CocoSegmentationDataset.load_or_download(train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the dataset\n=====================\n\nLet\\'s see how our data looks like.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f'Number of training images: {len(train_dataset)}')\nprint(f'Number of test images: {len(test_dataset)}')\nprint(f'Example output of an image shape: {train_dataset[0][0].shape}')\nprint(f'Example output of a label shape: {train_dataset[0][1].shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloading a Pre-trained Model\n===============================\n\nIn this tutorial, we will download a pre-trained LRSAPP model and a\nMobileNetV3 Large backbone from the official PyTorch repository. For\nmore details, please refer to the [official\ndocumentation](https://pytorch.org/vision/main/models/generated/torchvision.models.segmentation.lraspp_mobilenet_v3_large.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = load_model(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementing the VisionData class\n=================================\n\nThe checks in the package validate the model & data by calculating\nvarious quantities over the data, labels and predictions. In order to do\nthat, those must be in a pre-defined format, according to the task type.\nIn the following example we\\'re using pytorch. To see an implementation\nof this in tensorflow, please refer to\n`vision__vision_data_class`{.interpreted-text role=\"ref\"} guide. For\npytorch, we will use our DataLoader, but we\\'ll create a new collate\nfunction for it, that transforms the batch to the correct format. Then,\nwe\\'ll create a\n`deepchecks.vision.vision_data.vision_data.VisionData`{.interpreted-text\nrole=\"class\"} object, that will hold the data loader.\n\nTo learn more about the expected formats, please visit the\n`vision__supported_tasks`{.interpreted-text role=\"ref\"}.\n\nFirst, we\\'ll create the collate function that will be used by the\nDataLoader. In pytorch, the collate function is used to transform the\noutput batch to any custom format, and we\\'ll use that in order to\ntransform the batch to the correct format for the checks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchvision.transforms.functional as F\nfrom deepchecks.vision.vision_data import BatchOutputFormat\n\ndef deepchecks_collate_fn(batch) -> BatchOutputFormat:\n    \"\"\"Return a batch of images, labels and predictions for a batch of data. The expected format is a dictionary with\n    the following keys: 'images', 'labels' and 'predictions', each value is in the deepchecks format for the task.\n    You can also use the BatchOutputFormat class to create the output.\n    \"\"\"\n    # batch received as iterable of tuples of (image, label) and transformed to tuple of iterables of images and labels:\n    batch = tuple(zip(*batch))\n\n    # images:\n    images = [tensor.numpy().transpose((1, 2, 0)) for tensor in batch[0]]\n\n    #labels:\n    labels = batch[1]\n\n    #predictions:\n    normalized_batch = [F.normalize(img.unsqueeze(0).float() / 255,\n                                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) for img in batch[0]]\n    predictions = [model(img)[\"out\"].squeeze(0).detach() for img in normalized_batch]\n    predictions = [torch.nn.functional.softmax(pred, dim=0) for pred in predictions]\n\n    return BatchOutputFormat(images=images, labels=labels, predictions=predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The label\\_map is a dictionary that maps the class id to the class name,\nfor display purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "LABEL_MAP = {0: 'background', 1: 'airplane', 2: 'bicycle', 3: 'bird', 4: 'boat', 5: 'bottle', 6: 'bus', 7: 'car',\n             8: 'cat', 9: 'chair', 10: 'cow', 11: 'dining table', 12: 'dog', 13: 'horse', 14: 'motorcycle',\n             15: 'person', 16: 'potted plant', 17: 'sheep', 18: 'couch', 19: 'train', 20: 'tv'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our updated collate function, we can create the\ndataloader in the deepchecks format, and use it to create a VisionData\nobject:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\nfrom deepchecks.vision import VisionData\n\ntrain_loader = DataLoader(dataset=train_dataset, shuffle=True, collate_fn=deepchecks_collate_fn)\ntest_loader = DataLoader(dataset=test_dataset, shuffle=True, collate_fn=deepchecks_collate_fn)\n\ntraining_data = VisionData(batch_loader=train_loader, task_type='semantic_segmentation', label_map=LABEL_MAP)\ntest_data = VisionData(batch_loader=test_loader, task_type='semantic_segmentation', label_map=LABEL_MAP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Making sure our data is in the correct format:\n==============================================\n\nThe VisionData object automatically validates your data format and will\nalert you if there is a problem. However, you can also manually view\nyour images and labels to make sure they are in the correct format by\nusing the `head` function to conveniently visualize your data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "training_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running Deepchecks\\' model evaluation suite on our data and model!\n==================================================================\n\nNow that we have defined the task class, we can validate the model with\nthe model evaluation suite of deepchecks. This can be done with this\nsimple few lines of code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.suites import model_evaluation\n\nsuite = model_evaluation()\nresult = suite.run(training_data, test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observing the results: {#vision_segmentation_tutorial__observing_the_result}\n======================\n\nThe results can be saved as a html file with the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.save_as_html('output.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or, if working inside a notebook, the output can be displayed directly\nby simply printing the result object:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From these results, we can see that mostly our model performs well.\nHowever, the model had an issue with identifying a specific class\n(\\\"bicycle\\\") in the test set, which casued a major degradation in the\n[Dice\nmetric](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient)\nfor that class, as can be seen in the check \\\"Class Performance\\\" under\nthe \\\"Didn\\'t Pass\\\" section. However, as this dataset has very few\nsamples, this would require further investigation.\n\nWe can also see that there are significant changes between the train and\ntest set, regarding the model\\'s predictions on them. in the\n\\\"Prediction Drift\\\" check, which checks drift in 3 properties of the\npredictions, we can see there\\'s a change in the distribution of the\npredicted classes. This can tell us that the train set is not\nrepresenting the test set well, even without knowing the actual test set\nlabels.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}