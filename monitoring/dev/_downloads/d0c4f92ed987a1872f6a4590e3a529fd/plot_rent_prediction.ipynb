{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Predicting Rent Prices (Regression)\n\nIn this Demo we are using an adaptation of the [Airbnb rent regression dataset](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data)_ to show how you can use Deepchecks to\nmonitor and identify issues in a Rent Prediction task.\n\nYou are a Data Scientist whose company has collected historical data on the apartments within New York\ncity, and your goal is to accurately predict the actual rent for the property given its attributes and the community\nfeedback. This estimation is then sold as an estimated price to the landlords, helping them correctly asses the value\nof their property.\n\nRecently, you have received word that some landlords in northern Manhattan are getting estimations from your platform\nthat are way off the market price, and you are sent to investigate.\n\nIn this notebook, we'll show how you can use deepchecks to pinpoint where these problematic predictions are coming\nfrom, and what is their root cause. Before that, we'll start by quickly setting up deepchecks monitoring for your\ndata and model.\n\n1. `Setting You Up on Deepchecks`_\n2. `Creating a Model & Model Version`_\n3. `Uploading Production Data`_\n4. `Analyzing Using Deepchecks`_\n\n..  tip::\n    To see how Deepchecks Monitoring is used, you can skip right to `Analyzing Using Deepchecks`_\n\n## Setting You Up on Deepchecks\n\n### Installation & API key\n\nIn order to work with Deepchecks Monitoring, you need to:\n\n1. Install with ``pip`` the deepchecks-client SDK\n2. Log in to the Deepchecks Monitoring app and create an organization\n3. Obtain an API key from the app\n\nFor more details, please refer to the :doc:`Quickstart </user-guide/tabular/auto_quickstarts/plot_quickstart>`.\n\n### Creating a Client\n\nTo work with Deepchecks Monitoring we first instantiate a client object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom deepchecks_client import DeepchecksClient\n# Note:  add an environment variable DEEPCHECKS_API_TOKEN and set it to your API token's value. Alternatively (not\n# recommended for security reasons) copy-paste your token string here, instead of retrieving it from the environment\n# variable.\ntoken = os.getenv('DEEPCHECKS_API_TOKEN')\n# Point the host to deepchecks host url (e.g. https://app.deepchecks.com. Save it to an environment variable,\n# or alternatively copy-paste it here directly)\nhost = os.getenv('DEEPCHECKS_API_HOST')\n# Create a DeepchecksClient with relevant credentials\ndc_client = DeepchecksClient(host=host, token=token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use this object during the remainder of this example.\n\n## Creating a Model & Model Version\n\nIn this section we'll create a model and a model version, using the training data as the reference. Reference data is\na dataset to which we wish to compare our production data stream. To learn more about models and model versions, and\nother important terms in Deepchecks please refer to the :doc:`Concepts guide </user-guide/general/concepts>`.\n\n### Getting the Data\n\nWe'll start by downloading the training data from the deepchecks testing package. This training data will be used\nto set the reference for the model version. We'll also download the pre-calculated predictions for this data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.datasets.regression.airbnb import load_data_large\n\ntrain_df, ref_predictions = load_data_large(data_format='DataFrame')\ntrain_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So what do we have? Let's note the special columns in our data:\n\n1. datestamp - The timestamp of the sample (seconds since epoch)\n2. price - Our label\n\nAll the other columns are features that can be used by our model to predict the price. We note that there are some\ncategorical features, and some numeric ones.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Schema\n\nA Schema file contains the description of the data (features and additional data) associated with a model version.\nTo learn more about creating a schema, please refer to the\n:doc:`Tabular Setup guide </user-guide/tabular/tabular_setup>`.\n\n**It is highly recommended to review the created schema file before moving forward to creating the model version.**\n\nIn order to create a schema file, the easiest way is to first define a deepchecks\n:doc:`Dataset <deepchecks:user-guide/tabular/dataset_object>` object, which contains the actual data (DataFrame)\ntogether with metadata about the role of each column.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular import Dataset\ntimestamp, label_col = 'datestamp', 'price'\ntrain_dataset = Dataset(\n    train_df, label=label_col,\n    features=['room_type', 'neighbourhood', 'neighbourhood_group', 'has_availability', 'minimum_nights',\n              'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365'],\n    cat_features=['neighbourhood_group', 'neighbourhood', 'room_type', 'has_availability'],\n    datetime_name=timestamp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll create the schema file, and print it to show (and validate) the schema that was created.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks_client import create_schema, read_schema\n\nschema_file_path = 'schema_file.yaml'\ncreate_schema(dataset=train_dataset, schema_output_file=schema_file_path)\nread_schema(schema_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>For conveniently changing the auto-inferred schema it's recommended to edit the textual file with an app of your\n  choice.\n\n  After editing, you can use the :meth:`read_schema <deepchecks_client.tabular.utils.read_schema>`\n  function to verify the validity of the syntax in your updated schema.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Importance\n\nIn order to provide the best analysis and alerts, we should let Deepchecks know about the relative importance of\nthe features to the model's prediction. In this example we'll load pre-calculated features importnaces,\nbut these can be easily calculated using :doc:`deepchecks <deepchecks:user-guide/tabular/feature_importance>`,\nor other methods (such as SHAP). Note that the feature importance values should be normalized to sum to 1.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.datasets.regression.airbnb import load_pre_calculated_feature_importance\nfeature_importance = load_pre_calculated_feature_importance()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating a Model Version\n\nWe'll use the :meth:`create_tabular_model_version\n<deepchecks_client.DeepchecksClient.create_tabular_model_version>` method. Calling it, we can create both our\nmodel, our model version and define the reference data with one call. We'll also let Deepchecks know this is a\nregression task, so we will set the ``task_type`` argument. Lastly, in order to fully define the reference,\nwe must also pass model predictions for the reference data. For classification tasks, not that it's highly recommended\nto also send the predicted probabilities.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_name = 'Rent Prediction - Example'\n\nmodel_version = dc_client.create_tabular_model_version(model_name=model_name, version_name='ver_1',\n                                                       schema=schema_file_path,\n                                                       feature_importance=feature_importance,\n                                                       reference_dataset=train_dataset,\n                                                       reference_predictions=ref_predictions,\n                                                       task_type='regression')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we know our model, model version and reference data where set, and we're ready to start uploading production data.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Uploading Production Data\n\nOnce in production, uploading data can be done either sample by sample, or by batch. To read more, please referTo\nread more, refer to the :doc:`Production Data Guide </user-guide/tabular/tabular-production>`. Here we'll\nshow how to use the batch upload method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "prod_data, prod_predictions = load_data_large(data_format='DataFrame', load_train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll change the original timestamps so the samples are recent\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import datetime\nyesterdays_timestamp = int(datetime.datetime.now().timestamp()) - 3600*24\nprod_data[timestamp] = prod_data[timestamp] + (yesterdays_timestamp - prod_data[timestamp].max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Uploading a Batch of Data\n\n\n#### Uploading the First Batch\n\nLet's start by uploading the first part of the dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "timestamps = prod_data[timestamp].unique()\nend_of_first_half = timestamps[3 * int(len(timestamps) // 4)]  # This is the first 3 weeks of the production data\n\nfirst_half_df = prod_data[prod_data.datestamp < end_of_first_half]\nsecond_half_df = prod_data[prod_data.datestamp >= end_of_first_half]\n\nmodel_version.log_batch(sample_ids=first_half_df.index,\n                        data=first_half_df.drop([timestamp, label_col], axis=1),\n                        timestamps=first_half_df[timestamp], predictions=prod_predictions[:len(first_half_df)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Uploading the Second Batch\n\nNow let's upload the second half of the dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_version.log_batch(sample_ids=second_half_df.index,\n                        data=second_half_df.drop([timestamp, label_col], axis=1),\n                        timestamps=second_half_df[timestamp], predictions=prod_predictions[len(first_half_df):])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Uploading The Labels\nThe labels are global to the model, and not specific to a version. Therefore, to upload them we need the model client.\nYou can do this directly after uploading the predictions, or at any other time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_client = dc_client.get_or_create_model(model_name)\nmodel_client.log_batch_labels(sample_ids=prod_data.index, labels=prod_data[label_col])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Making Sure Your Data Has Arrived\n\nIf you're not sure if your data has arrived, please refer to the relevant section in the\n`Production Data guide <tabular_production__validating_your_data_has_arrived>`.\n\n## Analyzing Using Deepchecks\n\nNow that you have our data in Deepchecks, you can start monitoring and analyzing it. Let's remember that you've\nreceived some complaints about the model performance from some of your customers, can you find the source of the\nissue?\n\n### Dashboard & Alerts - Finding the issue\n\nWhen you log in to Deepchecks, you'll be greeted by the Dashboard, in which you can view all the monitors defined for\nyour models. Selecting our ``Loan Default - Example`` model, we see the default monitors and their corresponding\ndefault alert rules.\n\n<img src=\"file://_static/images/examples/rent/dashboard.jpg\" alt=\"Dashboard\" align=\"center\">\n\n|\nWe can see that there are a couple of alerts of medium importance (yellow) that raised. Let's click on the alert in the Models\ntable to see the alert details.\n\nOnce in the alerts screen, you can see the list of all current alerts on your model. Clicking on that alert will\nopen an alert analysis screen. This screen has three main components - the monitor history graph, the segment explorer\nand the check details.\n\n1. The monitor history graph shows the monitor's value over time and the point at which the alert was triggered.\n2. The segment explorer allows you to see what was the value of the check at the time of the alert, in different\n   segments of the data.\n3. The check details section shows the details of the check that triggered the alert.\n\nIn this case, you can see in the check details section that there is a new category in the ``room_type`` feature,\nThis is the case for the \"All Data\" segment, but does this issue originate from a specific segment? Let's find out.\n\n<img src=\"file://_static/images/examples/rent/alert.gif\" alt=\"Alert\" align=\"center\">\n\n|\nSegmenting on the ``neighborhood`` feature, we can easily see that the new category is only present in the Harlem\nneighborhood. In it, ``room_type`` has a new category ``None``, appearing in 3.82% of the samples in the selected\ndate.\n\n### Analysis - Performance Check\n\nNow that we've found the source of the issue, we can go ahead and check the performance of the model on the\nproblematic samples. To do so, we'll now go to the analysis screen.\nThe analysis screen is where you can freely explore your data. There, you can run any of the checks you defined for\nyour model over different time periods and segments and easily change the parameters of the checks.\n\nIn our case, we'll head over to the Performance check. We'll select one of the time windows in\nwhich we saw the drift and select room_type as the segmentation feature. Right away we see that the samples with\n\"None\" room type have a higher RMSE than the rest of the samples. This is indeed the source of our issue and the\ncomplaints we've received from our customers! We can now go fix this integrity issue that caused these \"None\"\nroom types to appear in the data.\n\n<img src=\"file://_static/images/examples/rent/analysis_rmse.gif\" alt=\"Analysis RMSE\" align=\"center\">\n\n### Alerts - Defining a new alert\n\nNow that we know for sure that the complaints have been coming from landlords in the Harlem neighborhood, we can\ndefine an alert to notify us specifically of any future changes in this neighborhood. To do that we head to the Alert\nRules screen, nested under the Configuration menu. Here we can define our alert, and make sure it runs only on data\nthat comes from the Harlem neighborhood.\n\n<img src=\"file://_static/images/examples/rent/alert_rule.gif\" alt=\"Alert Rule\" align=\"center\">\n\n## Cleaning up\n\nIf you wish to delete this model from your environment, you can do that using the ``delete_model`` function.\nCAUTION: This will delete the model, all model versions, and all associated datasets.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dc_client.delete_model(model_name)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}