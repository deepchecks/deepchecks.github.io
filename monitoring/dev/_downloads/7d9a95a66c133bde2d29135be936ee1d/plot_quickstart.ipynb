{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Quickstart - Get Deepchecks Monitoring Up and Running for Computer Vision Data\n\nThis quickstart is the perfect starting point for monitoring your vision model using Deepchecks Monitoring. We'll\nquickly walk you through setting up a model to represent your task in the system, uploading data, setting the\ncomputed checks and alerts in the system and seeing some results for your effort. We'll be using the\n[Mask Detection Dataset](https://www.kaggle.com/datasets/andrewmvd/face-mask-detection)_, in which\nthe goal of the model is to detect faces, and classify whether the face is wearing a mask, partially wearing a mask or not wearing a mask.\n\n```bash\n# Before we start, if you don't have deepchecks-client installed yet, run:\nimport sys\n!{sys.executable} -m pip install -U deepchecks-client\n\n# or install using pip from your python environment\n```\n## Creating a New Model Version\n\nOur first step is to create a new model version in the system. A model in Deepchecks Monitoring\nrepresents an ML pipeline performing a single task in production through time,\nwhere the model's versions and the structure of the data may change over time.\nOur terminology to refer to a specific version within a model is \"model version\".\n\nThe easiest way to create a model version, which is demonstrated\nhere, requires a :doc:`Vision Data <deepchecks:user-guide/vision/data-classes>` object\ncontaining the reference data for the version. Reference data is a dataset to which we wish to compare\nour production data stream. Typically, this will be the dataset on which the model was trained.\nProviding reference data is optional yet many important :doc:`checks <deepchecks:user-guide/general/deepchecks_hierarchy>`\nsuch as :doc:`Train Test Prediction Drift (Vision Version) <deepchecks:checks_gallery/vision/model_evaluation/plot_train_test_prediction_drift>`\ncannot run without it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing the Reference Data\n\nIn this example we're loading a pre-made VisionData object containing the data at the first time stamp, with which\nthe model was trained. In order to create your own VisionData object from your own pytorch dataloader\nplease read the :doc:`Vision Data <deepchecks:user-guide/vision/data-classes/index>` documentation or follow the\nappropriate deepchecks computer vision :doc:`quickstart <deepchecks:user-guide/vision/auto_quickstarts/index>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.datasets.detection.mask import load_dataset, load_model, get_data_timestamps\nref_dataset = load_dataset(day_index=0, object_type='VisionData', shuffle=False)\nmodel = load_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predictions must be given in one of the following formats:\n\n1. A list predictions for each image in the dataset, according to the order they are loaded from the dataloader.\n2. A dictionary where the values are the predictions and the keys are the image indices in the pytorch Dataset object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ref_predictions = []\nfor batch in ref_dataset:\n    ref_predictions.extend(list(model(batch[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating a model version\nIn order to create a model version we must first create an organization in the\n[deepchecks system](https://app.deepchecks.com/) and generate a personal\nAPI token using the application's dashboard.\n\n<img src=\"file://_static/images/quickstart/get_api_token.png\" width=\"600\">\n\nUsing the API token we can now create a new model version and upload the reference data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport typing as t\nimport numpy as np\nimport torch\nfrom deepchecks_client import DeepchecksClient\n\nhost = os.environ.get('DEEPCHECKS_API_HOST')  # Replace this with https://app.deepchecks.com\n# note to put the API token in your environment variables. Or alternatively (less recommended):\n# os.environ['DEEPCHECKS_API_TOKEN'] = 'uncomment-this-line-and-insert-your-api-token-here'\nmodel_name = 'Mask Data'\ndc_client = DeepchecksClient(host=host, token=os.getenv('DEEPCHECKS_API_TOKEN'))\nmodel_version = dc_client.create_vision_model_version(model_name=model_name, version_name='v1',\n                                                      reference_dataset=ref_dataset,\n                                                      reference_predictions=ref_predictions,\n                                                      task_type='vision_detection')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Uploading Production Data\n\nNo matter what else you'll be doing with Deepchecks Monitoring, it will start by uploading some production data that\nyou want monitored. In this the mask data collected for dates ranging from the start of July 2022 to the end of\nAugust 2022. For simplicity and quicker runtime, we'll upload only the last few days in this tutorial.\nThen, we'll update the labels for some of the samples we uploaded.\n\n### Uploading Data and Predictions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "number_of_batches_to_upload = 5  # Limited to save time running this tutorial\nbatch_size = 32\n\n# Only upload the last few days.\ndaily_timestamps = get_data_timestamps()[55:]\n# To upload all production data, use:\n# daily_timestamps = get_data_timestamps()[1:]\n# (Disregard the first day, which is the reference data)\n\n\n# Defining a helper function that will convert the label format to the one supported by Deepchecks Monitoring.\ndef extract_label_dict(in_dict: t.Dict[str, torch.Tensor]) -> torch.Tensor:\n    return torch.concat([in_dict['labels'].reshape((-1, 1)), in_dict['boxes']], axis=1)\n\n\nfor day_idx, timestamp in enumerate(daily_timestamps):\n    # Load the DataLoader for the current day\n    data_loader = load_dataset(day_index=day_idx, object_type='DataLoader', batch_size=batch_size)\n\n    for batch_id, batch in enumerate(data_loader):\n        # We also upload only a small number of batches in this example to save time.\n        # Remove this for loop to upload all data from chosen timestamps.\n        if batch_id >= number_of_batches_to_upload:\n            break\n\n        indices = [f'{timestamp}_{batch_id}_{i}' for i in range(batch_size)]\n        timestamps = [timestamp] * batch_size\n\n        model_version.log_batch(sample_id=indices,\n                                timestamps=timestamps,\n                                images=[np.array(x.permute(1, 2, 0)) * 255 for x in batch[0]],\n                                labels=[extract_label_dict(tensor) for tensor in batch[1]],\n                                predictions=model(batch[0])\n                                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Images, labels and prediction must be provided in specific required formats.\nThe required format for the can be found at :doc:`here <deepchecks:user-guide/vision/data-classes/index>`.\nPlease look at the following entries:\n\n- Image format - can be found at :doc:`here <deepchecks:user-guide/vision/data-classes/VisionData>`.\n- Label & prediction format - look at documentation of the respective VisionData subclass according to your task type\n\nIn this example, the changes needed in the data format are pretty trivial, but in more complex cases\nyou may either implement them in dedicated functions (such as the ``extract_label_dict`` function here), or use\nthe ``batch_to_image`` and other formatting methods you already implemented as part of building your\nDeepchecks VisionData object.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Updating the Labels\nIn many real world scenarios, the labels of the data are only available at a later time. We can update them\nin hindsight using the global sample ids. Here we update the last sample that was uploaded.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_version.update_sample(sample_id=indices[-1],\n                            label=[extract_label_dict(tensor) for tensor in batch[1]][-1])\nmodel_version.send()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can update multiple samples. Once done updating all desired samples, call the `send` method to upload the samples\nand make the updates appear in the system.\n\nWhen updating multiple samples we can verify that status of the process that is running in the background by checking\nthe amount of samples that have been processed and uploaded by the system, using:\nmodel_version.time_window_statistics(min(prod_data[timestamp]), max(prod_data[timestamp]))\nupon completion, the statistics should equal the total number of samples sent\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Dashboard Screen\nAfter creating the model version and uploading the data, we can now see the monitors within the\n[application dashboard](https://app.deepchecks.com/).\nThe monitors below are generated by default when a new model is created, all versions of the same model are tracked\nwithin the same monitor.\n\n<img src=\"file://_static/images/quickstart/vision_dashboard_w_defaults.png\" width=\"600\">\n\nNote: The displayed dashboard was created using all the production data, not only the last few days\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we wish to remove the model to free up space for new models we can do it in the following way:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# CAUTION: This will delete the model, all model versions, and all associated datasets.\ndc_client.delete_model(model_name)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}