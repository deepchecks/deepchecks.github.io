
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "user-guide/vision/auto_quickstarts/plot_quickstart.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_user-guide_vision_auto_quickstarts_plot_quickstart.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_user-guide_vision_auto_quickstarts_plot_quickstart.py:


.. _quick_vision:

Quickstart - Get Deepchecks Monitoring Up and Running for Computer Vision Data
******************************************************************************

This quickstart is the perfect starting point for monitoring your vision model using Deepchecks Monitoring. We'll
quickly walk you through setting up a model to represent your task in the system, uploading data, setting the
computed checks and alerts in the system and seeing some results for your effort. We'll be using the
`Mask Detection Dataset <https://www.kaggle.com/datasets/andrewmvd/face-mask-detection>`__, in which
the goal of the model is to detect faces, and classify whether the face is wearing a mask, partially wearing a mask or not wearing a mask.

.. code-block:: bash

    # Before we start, if you don't have deepchecks-client installed yet, run:
    import sys
    !{sys.executable} -m pip install -U deepchecks-client

    # or install using pip from your python environment

Creating a New Model Version
============================

Our first step is to create a new model version in the system. A model in Deepchecks Monitoring
represents an ML pipeline performing a single task in production through time,
where the model's versions and the structure of the data may change over time.
Our terminology to refer to a specific version within a model is "model version".

The easiest way to create a model version, which is demonstrated
here, requires a :doc:`Vision Data <deepchecks:user-guide/vision/data-classes>` object
containing the reference data for the version. Reference data is a dataset to which we wish to compare
our production data stream. Typically, this will be the dataset on which the model was trained.
Providing reference data is optional yet many important :doc:`checks <deepchecks:user-guide/general/deepchecks_hierarchy>`
such as :doc:`Train Test Prediction Drift (Vision Version) <deepchecks:checks_gallery/vision/model_evaluation/plot_train_test_prediction_drift>`
cannot run without it.

.. GENERATED FROM PYTHON SOURCE LINES 40-47

Preparing the Reference Data
-------------------------------

In this example we're loading a pre-made VisionData object containing the data at the first time stamp, with which
the model was trained. In order to create your own VisionData object from your own pytorch dataloader
please read the :doc:`Vision Data <deepchecks:user-guide/vision/data-classes/index>` documentation or follow the
appropriate deepchecks computer vision :doc:`quickstart <deepchecks:user-guide/vision/auto_quickstarts/index>`.

.. GENERATED FROM PYTHON SOURCE LINES 47-52

.. code-block:: default


    from deepchecks.vision.datasets.detection.mask import load_dataset, load_model, get_data_timestamps
    ref_dataset = load_dataset(day_index=0, object_type='VisionData', shuffle=False)
    model = load_model()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/416968083 [00:00<?, ?it/s]      0%|          | 52224/416968083 [00:00<17:45, 391438.89it/s]      0%|          | 261120/416968083 [00:00<06:26, 1076940.18it/s]      0%|          | 1166336/416968083 [00:00<01:53, 3647630.68it/s]      1%|1         | 4271104/416968083 [00:00<00:36, 11272630.89it/s]      2%|2         | 8973312/416968083 [00:00<00:20, 19723949.59it/s]      3%|3         | 13544448/416968083 [00:00<00:16, 24473956.23it/s]      4%|4         | 18230272/416968083 [00:00<00:14, 27748161.54it/s]      6%|5         | 23063552/416968083 [00:01<00:13, 30233148.32it/s]      7%|6         | 27618304/416968083 [00:01<00:12, 31265550.19it/s]      8%|7         | 32435200/416968083 [00:01<00:11, 32570785.88it/s]      9%|8         | 37350400/416968083 [00:01<00:11, 33672693.97it/s]     10%|#         | 42019840/416968083 [00:01<00:11, 33881861.15it/s]     11%|#1        | 46869504/416968083 [00:01<00:10, 34405223.77it/s]     12%|#2        | 51637248/416968083 [00:01<00:10, 34599444.74it/s]     13%|#3        | 56241152/416968083 [00:02<00:10, 34421884.95it/s]     15%|#4        | 61467648/416968083 [00:02<00:09, 35634074.22it/s]     16%|#6        | 66874368/416968083 [00:02<00:09, 36885131.24it/s]     17%|#7        | 71871488/416968083 [00:02<00:09, 36828613.75it/s]     19%|#8        | 77229056/416968083 [00:02<00:09, 37623080.01it/s]     20%|#9        | 82160640/416968083 [00:02<00:08, 37228204.18it/s]     21%|##1       | 87649280/416968083 [00:02<00:08, 38181130.15it/s]     22%|##2       | 92924928/416968083 [00:02<00:08, 38348613.64it/s]     23%|##3       | 97807360/416968083 [00:03<00:08, 37645154.87it/s]     25%|##4       | 102951936/416968083 [00:03<00:08, 37679367.50it/s]     26%|##5       | 107998208/416968083 [00:03<00:08, 37479593.30it/s]     27%|##7       | 112634880/416968083 [00:03<00:08, 36490726.30it/s]     28%|##8       | 117615616/416968083 [00:03<00:08, 36546728.79it/s]     29%|##9       | 122252288/416968083 [00:03<00:08, 35790140.61it/s]     31%|###       | 127626240/416968083 [00:03<00:07, 36937910.03it/s]     32%|###1      | 132787200/416968083 [00:04<00:07, 37276019.22it/s]     33%|###3      | 137882624/416968083 [00:04<00:07, 37341068.95it/s]     34%|###4      | 142650368/416968083 [00:04<00:07, 38269641.96it/s]     35%|###5      | 147696640/416968083 [00:04<00:07, 38254773.49it/s]     37%|###6      | 153027584/416968083 [00:04<00:06, 41971937.54it/s]     38%|###7      | 157309952/416968083 [00:04<00:06, 40832750.04it/s]     39%|###8      | 162393088/416968083 [00:04<00:06, 40690139.94it/s]     40%|####      | 167471104/416968083 [00:04<00:05, 43327379.34it/s]     41%|####1     | 171869184/416968083 [00:05<00:05, 42285945.64it/s]     42%|####2     | 176958464/416968083 [00:05<00:05, 41357318.92it/s]     44%|####3     | 181878784/416968083 [00:05<00:05, 43445800.07it/s]     45%|####4     | 186276864/416968083 [00:05<00:05, 42427142.65it/s]     46%|####5     | 191294464/416968083 [00:05<00:05, 41070710.63it/s]     47%|####7     | 196668416/416968083 [00:05<00:04, 44449107.55it/s]     48%|####8     | 201182208/416968083 [00:05<00:05, 41659673.37it/s]     49%|####9     | 205810688/416968083 [00:05<00:05, 41290884.52it/s]     51%|#####     | 211044352/416968083 [00:05<00:04, 44273847.91it/s]     52%|#####1    | 215538688/416968083 [00:06<00:04, 43037355.53it/s]     53%|#####2    | 220359680/416968083 [00:06<00:04, 41667207.19it/s]     54%|#####4    | 225766400/416968083 [00:06<00:04, 44485212.67it/s]     55%|#####5    | 230265856/416968083 [00:06<00:04, 43454415.64it/s]     56%|#####6    | 235057152/416968083 [00:06<00:04, 44693493.60it/s]     57%|#####7    | 239560704/416968083 [00:06<00:04, 43501475.26it/s]     59%|#####8    | 244263936/416968083 [00:06<00:04, 42413067.41it/s]     60%|#####9    | 249261056/416968083 [00:06<00:03, 43867962.88it/s]     61%|######    | 253670400/416968083 [00:06<00:03, 43880606.35it/s]     62%|######1   | 258272256/416968083 [00:07<00:03, 44170296.26it/s]     63%|######3   | 262701056/416968083 [00:07<00:03, 43921474.49it/s]     64%|######4   | 267101184/416968083 [00:07<00:03, 43659793.14it/s]     65%|######5   | 271612928/416968083 [00:07<00:03, 44084860.24it/s]     66%|######6   | 276026368/416968083 [00:07<00:03, 43590238.14it/s]     67%|######7   | 280505344/416968083 [00:07<00:03, 43361020.92it/s]     68%|######8   | 284903424/416968083 [00:07<00:03, 43542221.60it/s]     69%|######9   | 289319936/416968083 [00:07<00:02, 43524808.66it/s]     70%|#######   | 293675008/416968083 [00:07<00:02, 43472057.43it/s]     71%|#######1  | 298023936/416968083 [00:07<00:02, 43243339.00it/s]     73%|#######2  | 302443520/416968083 [00:08<00:02, 43345845.62it/s]     74%|#######3  | 306779136/416968083 [00:08<00:02, 43247222.52it/s]     75%|#######4  | 311356416/416968083 [00:08<00:02, 43629070.08it/s]     76%|#######5  | 315719680/416968083 [00:08<00:02, 43277469.70it/s]     77%|#######6  | 320302080/416968083 [00:08<00:02, 43894928.90it/s]     78%|#######7  | 324692992/416968083 [00:08<00:02, 43305816.88it/s]     79%|#######8  | 329280512/416968083 [00:08<00:02, 43794671.93it/s]     80%|########  | 333662208/416968083 [00:08<00:01, 43595821.47it/s]     81%|########1 | 338340864/416968083 [00:08<00:01, 44333272.30it/s]     82%|########2 | 342775808/416968083 [00:08<00:01, 43773933.41it/s]     83%|########3 | 347468800/416968083 [00:09<00:01, 44702695.31it/s]     84%|########4 | 351942656/416968083 [00:09<00:01, 43840702.72it/s]     85%|########5 | 356423680/416968083 [00:09<00:01, 44124390.00it/s]     87%|########6 | 360840192/416968083 [00:09<00:01, 43794280.74it/s]     88%|########7 | 365222912/416968083 [00:09<00:01, 43716198.21it/s]     89%|########8 | 369596416/416968083 [00:09<00:01, 42234763.79it/s]     90%|########9 | 374828032/416968083 [00:09<00:00, 42505043.14it/s]     91%|#########1| 379481088/416968083 [00:09<00:00, 43591849.64it/s]     92%|#########2| 383851520/416968083 [00:09<00:00, 42973276.72it/s]     93%|#########3| 388482048/416968083 [00:09<00:00, 43929122.54it/s]     94%|#########4| 392884224/416968083 [00:10<00:00, 43368172.51it/s]     95%|#########5| 397314048/416968083 [00:10<00:00, 43638249.98it/s]     96%|#########6| 401683456/416968083 [00:10<00:00, 43289771.30it/s]     97%|#########7| 406203392/416968083 [00:10<00:00, 43848702.96it/s]     98%|#########8| 410592256/416968083 [00:10<00:00, 43530506.76it/s]    100%|#########9| 415050752/416968083 [00:10<00:00, 43840029.22it/s]    416968704it [00:10, 39145337.49it/s]                               




.. GENERATED FROM PYTHON SOURCE LINES 53-57

Predictions must be given in one of the following formats:

1. A list predictions for each image in the dataset, according to the order they are loaded from the dataloader.
2. A dictionary where the values are the predictions and the keys are the image indices in the pytorch Dataset object.

.. GENERATED FROM PYTHON SOURCE LINES 58-64

.. code-block:: default


    ref_predictions = []
    for batch in ref_dataset:
        ref_predictions.extend(list(model(batch[0])))









.. GENERATED FROM PYTHON SOURCE LINES 65-75

Creating a model version
------------------------
In order to create a model version we must first create an organization in the
`deepchecks system <https://app.deepchecks.com/>`_ and generate a personal
API token using the application's dashboard.

.. image:: /_static/images/quickstart/get_api_token.png
   :width: 600

Using the API token we can now create a new model version and upload the reference data.

.. GENERATED FROM PYTHON SOURCE LINES 75-93

.. code-block:: default


    import os
    import typing as t
    import numpy as np
    import torch
    from deepchecks_client import DeepchecksClient

    host = os.environ.get('DEEPCHECKS_API_HOST')  # Replace this with https://app.deepchecks.com
    # note to put the API token in your environment variables. Or alternatively (less recommended):
    # os.environ['DEEPCHECKS_API_TOKEN'] = 'uncomment-this-line-and-insert-your-api-token-here'
    model_name = 'Mask Data'
    dc_client = DeepchecksClient(host=host, token=os.getenv('DEEPCHECKS_API_TOKEN'))
    model_version = dc_client.create_vision_model_version(model_name=model_name, version_name='v1',
                                                          reference_dataset=ref_dataset,
                                                          reference_predictions=ref_predictions,
                                                          task_type='vision_detection',
                                                          send_images=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Model Mask Data was successfully created!. Default checks, monitors and alerts added.
    Reference data uploaded.




.. GENERATED FROM PYTHON SOURCE LINES 94-104

Uploading Production Data
=========================

No matter what else you'll be doing with Deepchecks Monitoring, it will start by uploading some production data that
you want monitored. In this the mask data collected for dates ranging from the start of July 2022 to the end of
August 2022. For simplicity and quicker runtime, we'll upload only the last few days in this tutorial.
Then, we'll update the labels for some of the samples we uploaded.

Uploading Data and Predictions
------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 104-140

.. code-block:: default


    number_of_batches_to_upload = 5  # Limited to save time running this tutorial
    batch_size = 32

    # Only upload the last few days.
    daily_timestamps = get_data_timestamps()[55:]
    # To upload all production data, use:
    # daily_timestamps = get_data_timestamps()[1:]
    # (Disregard the first day, which is the reference data)


    # Defining a helper function that will convert the label format to the one supported by Deepchecks Monitoring.
    def extract_label_dict(in_dict: t.Dict[str, torch.Tensor]) -> torch.Tensor:
        return torch.concat([in_dict['labels'].reshape((-1, 1)), in_dict['boxes']], axis=1)


    for day_idx, timestamp in enumerate(daily_timestamps):
        # Load the DataLoader for the current day
        data_loader = load_dataset(day_index=day_idx, object_type='DataLoader', batch_size=batch_size)

        for batch_id, batch in enumerate(data_loader):
            # We also upload only a small number of batches in this example to save time.
            # Remove this for loop to upload all data from chosen timestamps.
            if batch_id >= number_of_batches_to_upload:
                break

            indices = [f'{timestamp}_{batch_id}_{i}' for i in range(batch_size)]
            timestamps = [timestamp] * batch_size

            model_version.log_batch(sample_id=indices,
                                    timestamps=timestamps,
                                    images=[np.array(x.permute(1, 2, 0)) * 255 for x in batch[0]],
                                    labels=[extract_label_dict(tensor) for tensor in batch[1]],
                                    predictions=model(batch[0])
                                    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.
    32 new samples were successfully logged.




.. GENERATED FROM PYTHON SOURCE LINES 141-152

Images, labels and prediction must be provided in specific required formats.
The required format for the can be found at :doc:`here <deepchecks:user-guide/vision/data-classes/index>`.
Please look at the following entries:

- Image format - can be found at :doc:`here <deepchecks:user-guide/vision/data-classes/VisionData>`.
- Label & prediction format - look at documentation of the respective VisionData subclass according to your task type

In this example, the changes needed in the data format are pretty trivial, but in more complex cases
you may either implement them in dedicated functions (such as the ``extract_label_dict`` function here), or use
the ``batch_to_image`` and other formatting methods you already implemented as part of building your
Deepchecks VisionData object.

.. GENERATED FROM PYTHON SOURCE LINES 155-159

Updating the Labels
-------------------
In many real world scenarios, the labels of the data are only available at a later time. We can update them
in hindsight using the global sample ids. Here we update the last sample that was uploaded.

.. GENERATED FROM PYTHON SOURCE LINES 159-164

.. code-block:: default


    model_version.update_sample(sample_id=indices[-1],
                                label=[extract_label_dict(tensor) for tensor in batch[1]][-1])
    model_version.send()








.. GENERATED FROM PYTHON SOURCE LINES 165-172

You can update multiple samples. Once done updating all desired samples, call the `send` method to upload the samples
and make the updates appear in the system.

When updating multiple samples we can verify that status of the process that is running in the background by checking
the amount of samples that have been processed and uploaded by the system, using:
model_version.time_window_statistics(min(prod_data[timestamp]), max(prod_data[timestamp]))
upon completion, the statistics should equal the total number of samples sent

.. GENERATED FROM PYTHON SOURCE LINES 174-186

The Dashboard Screen
====================
After creating the model version and uploading the data, we can now see the monitors within the
`application dashboard <https://app.deepchecks.com/>`_.
The monitors below are generated by default when a new model is created, all versions of the same model are tracked
within the same monitor.

.. image:: /_static/images/quickstart/vision_dashboard_w_defaults.png
   :width: 600

Note: The displayed dashboard was created using all the production data, not only the last few days


.. GENERATED FROM PYTHON SOURCE LINES 188-189

If we wish to remove the model to free up space for new models we can do it in the following way:

.. GENERATED FROM PYTHON SOURCE LINES 189-192

.. code-block:: default


    # CAUTION: This will delete the model, all model versions, and all associated datasets.
    dc_client.delete_model(model_name)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The following model was successfully deleted: Mask Data





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 1 minutes  48.234 seconds)


.. _sphx_glr_download_user-guide_vision_auto_quickstarts_plot_quickstart.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_quickstart.py <plot_quickstart.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_quickstart.ipynb <plot_quickstart.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
