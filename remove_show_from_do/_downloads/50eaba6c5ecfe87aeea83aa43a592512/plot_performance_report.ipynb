{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performance Report\n==================\n\nThis notebooks provides an overview for using and understanding\nperformance report check.\n\n**Structure:**\n\n-   [What is the purpose of the\n    check?](#what-is-the-purpose-of-the-check)\n-   [Generate data & model](#generate-data-model)\n-   [Run the check](#run-the-check)\n-   [Define a condition](#define-a-condition)\n-   [Using alternative scorers](#using-alternative-scorers)\n\nWhat is the purpose of the check?\n---------------------------------\n\nThis check helps you compare your model\\'s performance between two\ndatasets. The default metric that are used are F1, Percision, and Recall\nfor Classification and Negative Root Mean Square Error, Negative Mean\nAbsolute Error, and R2 for Regression. RMSE and MAE Scorers are negative\nbecause we subscribe to the sklearn convention of defining scoring\nfunctions. [See scorers\ndocumentation](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate data & model\n=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.datasets.classification.phishing import load_data, load_fitted_model\n\ntrain_dataset, test_dataset = load_data()\nmodel = load_fitted_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.checks.performance import PerformanceReport\n\ncheck = PerformanceReport()\ncheck.run(train_dataset, test_dataset, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nWe can define on our check a condition that will validate that our model\ndoesn\\'t degrade on new data.\n\nLet\\'s add a condition to the check and see what happens when it fails:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = PerformanceReport()\ncheck.add_condition_train_test_relative_degradation_not_greater_than(0.05)\nresult = check.run(train_dataset, test_dataset, model)\nresult.show(show_additional_outputs=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We detected that for class \\\"0\\\" our the Precision result is degraded by\nmore than 5%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using alternative scorers\n=========================\n\nWe can define alternative scorers that are not run by default:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer, fbeta_score\n\nfbeta_scorer = make_scorer(fbeta_score, labels=[0, 1], average=None, beta=0.2)\n\ncheck = PerformanceReport(alternative_scorers={'my scorer': fbeta_scorer})\ncheck.run(train_dataset, test_dataset, model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}