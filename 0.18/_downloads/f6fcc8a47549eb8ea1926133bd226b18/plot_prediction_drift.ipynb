{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prediction Drift {#nlp__prediction_drift}\n================\n\nThis notebook provides an overview for using and understanding the NLP\nprediction drift check.\n\n**Structure:**\n\n-   [What Is Prediction Drift?](#what-is-prediction-drift)\n-   [Get Data and Predictions](#get-data-and-predictions)\n-   [Run Check](#run-check)\n\nWhat Is Prediction Drift?\n-------------------------\n\nDrift is simply a change in the distribution of data over time, and it\nis also one of the top reasons why machine learning model\\'s performance\ndegrades over time.\n\nPrediction drift is when drift occurs in the prediction itself.\nCalculating prediction drift is especially useful in cases in which\nlabels are not available for the test dataset, and so a drift in the\npredictions is our only indication that a changed has happened in the\ndata that actually affects model predictions. If labels are available,\nit\\'s also recommended to run the\n`Label Drift check <nlp__label_drift>`{.interpreted-text role=\"ref\"}.\n\nFor more information on drift, please visit our\n`drift guide <drift_user_guide>`{.interpreted-text role=\"ref\"}.\n\n### How Deepchecks Detects Prediction Drift\n\nThis check detects prediction drift by using\n`univariate measures <drift_detection_by_univariate_measure>`{.interpreted-text\nrole=\"ref\"} on the prediction output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get Data and Predictions\n========================\n\nFor this example, we\\'ll use the tweet emotion dataset, which is a\ndataset of tweets labeled by one of four emotions: happiness, anger,\nsadness and optimism.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom deepchecks.nlp.checks import PredictionDrift\nfrom deepchecks.nlp.datasets.classification import tweet_emotion\nnp.random.seed(42)\n\ntrain_ds, test_ds = tweet_emotion.load_data()\n\n# Load precalculated model predictions:\ntrain_preds, test_preds = tweet_emotion.load_precalculated_predictions(as_train_test=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s see how our data looks like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_ds.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s introduce drift into the data by dropping 50% of the \\\"anger\\\"\ntweets from the train dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "angry_tweets = np.argwhere(train_ds.label == 'anger').flatten()  # Get all angry tweets\n# Select 50% of those to keep with the other tweets:\nangry_tweets_to_ignore = np.random.choice(angry_tweets, size=len(angry_tweets) // 2, replace=False)\nindices_to_keep = [x for x in range(len(train_ds)) if x not in angry_tweets_to_ignore]  # All indices to keep\n\n# Recreate the dataset and predictions without the dropped samples:\ntrain_ds = train_ds.copy(rows_to_use=indices_to_keep)\ntrain_preds = train_preds[indices_to_keep]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run Check\n=========\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = PredictionDrift()\nresult = check.run(train_dataset=train_ds, test_dataset=test_ds,\n                   train_predictions=train_preds, test_predictions=test_preds)\n\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that we found drift in the distribution of the predictions,\nand that the drift is mainly in the \\\"anger\\\" class. This makes sense,\nas we dropped 50% of the \\\"anger\\\" tweets from the train dataset, and so\nthe model is now predicting less \\\"anger\\\" tweets in the test dataset.\n\nThe prediction drift check can also calculate drift on the probability\nof each class separately rather than the final predicted class. To force\nthis behavior, set the `drift_mode` parameter to `proba`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# First let's get the probabilities for our data, instead of the predictions:\ntrain_probas, test_probas = tweet_emotion.load_precalculated_predictions(pred_format='probabilities')\ntrain_probas = train_probas[indices_to_keep]  # Filter the probabilities again\n\ncheck = PredictionDrift(drift_mode='proba')\nresult = check.run(train_dataset=train_ds, test_dataset=test_ds,\n                   train_probabilities=train_probas, test_probabilities=test_probas)\n\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This time, we can see there\\'s small drift in each class. The \\\"anger\\\"\nclass drift is actually probably caused by low sample size, and not by\ndrift in the data itself, as we did not change the data within the\nclass, but only changed the prevalence of the class in the data.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}