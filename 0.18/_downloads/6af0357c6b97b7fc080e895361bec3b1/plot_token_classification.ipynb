{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Token Classification Quickstart {#nlp__token_classification_quickstart}\n===============================\n\nDeepchecks NLP tests your models during model development/research and\nbefore deploying to production. Using our testing package reduces model\nfailures and saves tests development time. In this quickstart guide, you\nwill learn how to use the deepchecks NLP package to analyze and evaluate\ntoken classification tasks. A token classification task is a case in\nwhich we wish to give a specific label for each token (usually a word or\na part of a word), rather than assigning a class or classes for the text\nas a whole. For a more complete example showcasing the range of checks\nand capabilities of the NLP package, refer to our\n`Multiclass Quickstart <nlp__multiclass_quickstart>`{.interpreted-text\nrole=\"ref\"}. We will cover the following steps:\n\n1.  [Creating a TextData object and auto calculating\n    properties](#setting-up)\n2.  [Running checks](#running-checks)\n\nTo run deepchecks for token classification, you need the following for\nboth your train and test data:\n\n1.  Your tokenized text dataset - a list containing lists of strings,\n    each string is a single token within the sample, where a sample can\n    be a sentence, paragraph, document and so on.\n2.  Your labels - a\n    `Token Classification <nlp__supported_token_classification>`{.interpreted-text\n    role=\"ref\"} label. These are not needed for checks that don\\'t\n    require labels (such as the Embeddings Drift check or most data\n    integrity checks), but are needed for many other checks.\n3.  Your model\\'s predictions (see\n    `nlp__supported_tasks`{.interpreted-text role=\"ref\"} for info on\n    supported formats). These are needed only for the model related\n    checks, shown in the [Model Evaluation](#running-checks) check in\n    this guide.\n\nIf you don\\'t have deepchecks installed yet:\n\n``` {.python}\nimport sys\n!{sys.executable} -m pip install 'deepchecks[nlp]' -U --quiet #--user\n```\n\nSome properties calculated by `deepchecks.nlp` require additional\npackages to be installed. You can also install them by running:\n\n``` {.python}\nimport sys\n!{sys.executable} -m pip install 'deepchecks[nlp-properties]' -U --quiet #--user\n```\n\nSetting Up\n----------\n\n### Load Data\n\nFor the purpose of this guide, we\\'ll use a small subset of the\n[SCIERC](http://nlp.cs.washington.edu/sciIE/) dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\nfrom deepchecks.nlp import TextData\nfrom deepchecks.nlp.datasets.token_classification import scierc_ner\n\ntrain, test = scierc_ner.load_data(data_format='Dict')\npprint(train['text'][0][:10])\npprint(train['label'][0][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The SCIERC dataset is a dataset of scientific articles with annotations\nfor named entities, relations and coreferences. In this example we\\'ll\nonly use the named entity annotations, which are the labels we\\'ll use\nfor our token classification task. We can see that we have the article\ntext itself, and the labels for each token in the text in the\n`IOB format <nlp__supported_token_classification>`{.interpreted-text\nrole=\"ref\"}.\n\nCreate a TextData Object\n========================\n\nWe can now create a `TextData <nlp__textdata_object>`{.interpreted-text\nrole=\"ref\"} object for the train and test dataframes. This object is\nused to pass your data to the deepchecks checks.\n\nTo create a TextData object, the only required argument is the tokenized\ntext itself. In most cases we\\'ll want to pass labels as well, as they\nare needed in order to calculate many checks. In this example we\\'ll\npass the label and define the task type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train = TextData(tokenized_text=train['text'], label=train['label'], task_type='token_classification')\ntest = TextData(tokenized_text=test['text'], label=test['label'], task_type='token_classification')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculating Properties\n======================\n\nSome of deepchecks\\' checks use properties of the text samples for\nvarious calculations. Deepcheck has a wide variety of such properties,\nsome simple and some that rely on external models and are more heavy to\nrun. In order for deepchecks\\' checks to be able to use the properties,\nthey must be added to the\n`TextData <nlp__textdata_object>`{.interpreted-text role=\"ref\"} object,\nusually by calculating them. You can read more about properties in the\n`Property Guide <nlp__properties_guide>`{.interpreted-text role=\"ref\"}.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# properties can be either calculated directly by Deepchecks\n# or imported from other sources in appropriate format\n\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# train.calculate_builtin_properties(\n#   include_long_calculation_properties=True, device=device\n# )\n# test.calculate_builtin_properties(\n#   include_long_calculation_properties=True, device=device\n# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example though we\\'ll use pre-calculated properties:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_properties, test_properties = scierc_ner.load_properties()\n\ntrain.set_properties(train_properties, categorical_properties=['Language'])\ntest.set_properties(test_properties, categorical_properties=['Language'])\n\ntrain.properties.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running Checks\n==============\n\nTrain Test Performance\n----------------------\n\nOnce the `TextData <nlp__textdata_object>`{.interpreted-text role=\"ref\"}\nobject is ready, we can run the checks. We\\'ll start by running the\n`TrainTestPerformance <nlp__train_test_performance>`{.interpreted-text\nrole=\"ref\"} check, which compares the performance of the model on the\ntrain and test sets. For this check, we\\'ll need to pass the model\\'s\npredictions on the train and test sets, also provided in the format of\nan IOB annotation per token in the tokenized text.\n\nWe\\'ll also define a condition for the check with the default threshold\nvalue. You can learn more about customizing checks and conditions, as\nwell as defining suites of checks in our\n`Customizations Guide <general__customizations>`{.interpreted-text\nrole=\"ref\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_preds, test_preds = scierc_ner.load_precalculated_predictions()\n\nfrom deepchecks.nlp.checks import TrainTestPerformance\ncheck = TrainTestPerformance().add_condition_train_test_relative_degradation_less_than()\nresult = check.run(train, test, train_predictions=train_preds, test_predictions=test_preds)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the model performs better on the train set than on the\ntest set, which is expected. We can also note specifically that the\nrecall for class \\\"OtherScientificTerm\\\" has declined significantly on\nthe test set, which is something we might want to investigate further.\n\nEmbeddings Drift\n================\n\nThe `EmbeddingsDrift <nlp__embeddings_drift>`{.interpreted-text\nrole=\"ref\"} check compares the embeddings of the train and test sets. In\norder to run this check you must have text embeddings loaded to both\ndatasets. You can read more about using embeddings in deepchecks NLP in\nour `Embeddings Guide <nlp__embeddings_guide>`{.interpreted-text\nrole=\"ref\"}. In this example, we have the embeddings already\npre-calculated:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_embeddings, test_embeddings = scierc_ner.load_embeddings()\n\n\ntrain.set_embeddings(train_embeddings)\ntest.set_embeddings(test_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also calculate the embeddings using deepchecks, either using an\nopen-source sentence-transformer or using Open AI's embedding API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# train.calculate_builtin_embeddings()\n# test.calculate_builtin_embeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.checks import TextEmbeddingsDrift\n\ncheck = TextEmbeddingsDrift()\nres = check.run(train, test)\nres.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The check shows the samples from the train and test datasets as points\nin the 2-dimensional reduced embedding space. We can see some distinct\nsegments - in the upper left corner we can notice (by hovering on the\nsamples and reading the abstracts) that these are papers about computer\nvision, while the bottom right corner is mostly about Natural Language\nProcessing. We can also see that although there isn\\'t significant drift\nbetween the train and test, the training dataset has a bit more samples\nfrom the NLP domain, while the test set has more samples from the\ncomputer vision domain.\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nYou can find the full list of available NLP checks in the\n`nlp.checks api documentation \u05bf\n<deepchecks.nlp.checks>`{.interpreted-text role=\"mod\"}.\n:::\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}