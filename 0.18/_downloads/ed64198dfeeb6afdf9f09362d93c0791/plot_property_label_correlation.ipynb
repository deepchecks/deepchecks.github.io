{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Property Label Correlation {#nlp__property_label_correlation}\n==========================\n\nThis notebook provides an overview for using and understanding the\n\\\"Property Label Correlation\\\" check.\n\n**Structure:**\n\n-   [What Is The Purpose of the\n    Check?](#what-is-the-purpose-of-the-check)\n-   [Run the Check](#run-the-check)\n\nWhat Is The Purpose of the Check?\n---------------------------------\n\nThe check estimates for every\n`text property <nlp__properties_guide>`{.interpreted-text role=\"ref\"}\n(such as text length, language etc.) its ability to predict the label by\nitself.\n\nThis check can help find a potential bias in the dataset - the labels\nbeing strongly correlated with simple text properties such as percentage\nof special characters, sentiment, toxicity and more.\n\nThis is a critical problem that can result in a phenomenon called\n\\\"shortcut learning\\\", where the model is likely to learn this property\ninstead of the actual textual characteristics of each class, as it\\'s\neasier to do so. In this case, the model will show high performance on\ntext collected under similar conditions (e.g. same source), but will\nfail to generalize on other data (for example, when production receives\nnew data from another source). This kind of correlation will likely stay\nhidden without this check until tested on the actual problem data.\n\nFor example, in a classification dataset of true and false statements,\nif only true facts are written in detail, and false facts are written in\na short and vague manner, the model might learn to predict the label by\nthe length of the statement, and not by the actual content. In this\ncase, the model will perform well on the training data, and may even\nperform well on the test data, but will fail to generalize to new data.\n\nThe check is based on calculating the predictive power score (PPS) of\neach text property. In simple terms, the PPS is a metric that measures\nhow well can one feature predict another (in our case, how well can one\nproperty predict the label). For further information about PPS you can\nvisit the [ppscore github](https://github.com/8080labs/ppscore) or the\nfollowing blog post: [RIP correlation. Introducing the Predictive Power\nScore](https://towardsdatascience.com/rip-correlation-introducing-the-predictive-power-score-3d90808b9598)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the Check\n=============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.checks import PropertyLabelCorrelation\nfrom deepchecks.nlp.datasets.classification import tweet_emotion\n\n# For this example, we'll use the tweet emotion dataset, which is a dataset of tweets labeled by one of four emotions:\n# happiness, anger, sadness and optimism.\n\n# Load Data:\ndataset = tweet_emotion.load_data(as_train_test=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s see how our data looks like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets run the check:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result = PropertyLabelCorrelation().run(dataset)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that in our example of tweet emotion dataset, the label is\ncorrelated with the \\\"sentiment\\\" property, which makes sense, as the\nlabel is the emotion of the tweet, and the sentiment expresses whether\nthe tweet is positive or negative. Also, there\\'s some correlation with\nthe \\\"toxciity\\\" property, which is a measure of how toxic the tweet is.\nThis is also reasonable, as some emotions are more likely to be\nexpressed in a toxic way. However, these correlation may indicate that a\nmodel may learn to predict the label by curse words, for instance,\ninstead of the actual content of the tweet, which could lead it to fail\non new tweets that don\\'t contain curse words.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}