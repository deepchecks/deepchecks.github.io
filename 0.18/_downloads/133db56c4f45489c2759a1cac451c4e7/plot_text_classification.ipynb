{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Text Classification Quickstart {#nlp__multiclass_quickstart}\n==============================\n\nDeepchecks NLP tests your models during model development/research and\nbefore deploying to production. Using our testing package reduces model\nfailures and saves tests development time. In this quickstart guide, you\nwill learn how to use the deepchecks NLP package to analyze and evaluate\ntext classification tasks. If you are interested in a multilabel\nclassification task, you can refer to our\n`Multilabel Quickstart <nlp__multilabel_quickstart>`{.interpreted-text\nrole=\"ref\"}. We will cover the following steps:\n\n1.  [Creating a TextData object and auto calculating\n    properties](#setting-up)\n2.  [Running the built-in suites and inspecting the\n    results](#running-the-deepchecks-default-suites)\n3.  [We\\'ll spotlight two interesting checks - Embeddings drift and\n    Under-Annotated Segments](#running-individual-checks)\n\nTo run deepchecks for NLP, you need the following for both your train\nand test data:\n\n1.  Your `text data <nlp__textdata_object>`{.interpreted-text\n    role=\"ref\"} - a list of strings, each string is a single sample (can\n    be a sentence, paragraph, document, etc.).\n2.  Your labels - either a\n    `Text Classification <nlp_supported_text_classification>`{.interpreted-text\n    role=\"ref\"} label or a\n    `Token Classification <nlp_supported_token_classification>`{.interpreted-text\n    role=\"ref\"} label. These are not needed for checks that don\\'t\n    require labels (such as the Embeddings Drift check or most data\n    integrity checks), but are needed for many other checks.\n3.  Your model\\'s predictions (see\n    `nlp__supported_tasks`{.interpreted-text role=\"ref\"} for info on\n    supported formats). These are needed only for the model related\n    checks, shown in the [Model Evaluation](#model-evaluation) section\n    of this guide.\n\nIf you don\\'t have deepchecks installed yet:\n\n``` {.python}\nimport sys\n!{sys.executable} -m pip install 'deepchecks[nlp]' -U --quiet #--user\n```\n\nSome properties calculated by `deepchecks.nlp` require additional\npackages to be installed. You can install them by running:\n\n``` {.python}\nimport sys\n!{sys.executable} -m pip install 'deepchecks[nlp-properties]' -U --quiet #--user\n```\n\nSetting Up\n----------\n\n### Load Data\n\nFor the purpose of this guide, we\\'ll use a small subset of the [tweet\nemotion](https://github.com/cardiffnlp/tweeteval) dataset. This dataset\ncontains tweets and their corresponding emotion - Anger, Happiness,\nOptimism, and Sadness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp import TextData\nfrom deepchecks.nlp.datasets.classification import tweet_emotion\n\ntrain, test = tweet_emotion.load_data(data_format='DataFrame')\ntrain.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that we have the tweet text itself, the label (the emotion)\nand then some additional metadata columns.\n\nCreate a TextData Objects\n=========================\n\nWe can now create a `TextData <nlp__textdata_object>`{.interpreted-text\nrole=\"ref\"} object for the train and test dataframes. This object is\nused to pass your data to the deepchecks checks.\n\nTo create a TextData object, the only required argument is the text\nitself, but passing only the text will prevent multiple checks from\nrunning. In this example we\\'ll pass the label and define the task type\nand finally define the\n`metadata columns <nlp__metadata_guide>`{.interpreted-text role=\"ref\"}\n(the other columns in the dataframe) which we\\'ll use later on in the\nguide.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train = TextData(train.text, label=train['label'], task_type='text_classification',\n                 metadata=train.drop(columns=['label', 'text']))\ntest = TextData(test.text, label=test['label'], task_type='text_classification',\n                metadata=test.drop(columns=['label', 'text']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculating Properties\n======================\n\nSome of deepchecks\\' checks use properties of the text samples for\nvarious calculations. Deepcheck has a wide variety of such properties,\nsome simple and some that rely on external models and are more heavy to\nrun. In order for deepchecks\\' checks to be able to access the\nproperties, they must be stored within the\n`TextData <nlp__textdata_object>`{.interpreted-text role=\"ref\"} object.\nYou can read more about properties in the\n`Property Guide <nlp__properties_guide>`{.interpreted-text role=\"ref\"}.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# properties can be either calculated directly by Deepchecks\n# or imported from other sources in appropriate format\n\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# train.calculate_builtin_properties(\n#   include_long_calculation_properties=True, device=device\n# )\n# test.calculate_builtin_properties(\n#   include_long_calculation_properties=True,  device=device\n# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example though we\\'ll use pre-calculated properties:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_properties, test_properties = tweet_emotion.load_properties()\n\ntrain.set_properties(train_properties, categorical_properties=['Language'])\ntest.set_properties(test_properties, categorical_properties=['Language'])\n\ntrain.properties.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the Deepchecks Default Suites\n=====================================\n\nDeepchecks comes with a set of pre-built suites that can be used to run\na set of checks on your data, alongside with their default conditions\nand thresholds. You can read more about customizing and creating your\nown suites in the\n`Customizations Guide <general__customizations>`{.interpreted-text\nrole=\"ref\"}. In this guide we\\'ll be using 3 suites - the data integrity\nsuite, the train test validation suite and the model evaluation suite.\nYou can also run all the checks at once using the\n`full_suite <deepchecks.nlp.suites>`{.interpreted-text role=\"mod\"}.\n\nData Integrity\n--------------\n\nWe will start by doing preliminary integrity check to validate the text\nformatting. It is recommended to do this step before model training as\nit may imply additional data engineering is required.\n\nWe\\'ll do that using the\n`data_integrity <deepchecks.nlp.suites>`{.interpreted-text role=\"mod\"}\npre-built suite.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.suites import data_integrity\n\ndata_integrity_suite = data_integrity()\ndata_integrity_suite.run(train, test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Integrity \\#1: Unknown Tokens\n=============================\n\nFirst up (in the \"Didn\\'t Pass\" tab) we see that the Unknown Tokens\ncheck has returned a problem.\n\nLooking at the result, we can see that it assumed (by default) that\nwe're going to use the bert-base-uncased tokenizer for our NLP model,\nand that if that's the case there are many words in the dataset that\ncontain characters (such as emojis, or Korean characters) that are\nunrecognized by the tokenizer. This is an important insight, as bert\ntokenizers are very common. You can configure the tokenizer used by this\ncheck by passing the tokenizer to the check's constructor, and can also\nconfigure the threshold for the percent of unknown tokens allowed by\nmodifying the checks condition.\n\nIntegrity \\#2: Text Outliers\n============================\n\nIn the \"Didn\\'t Pass\" tab, by looking at the Text Outlier check result\nwe can derive several insights by hovering over the different values and\ninspecting the outlier texts:\n\n1.  hashtags ('\\#...') are usually several words written together\n    without spaces - we might consider splitting them before feeding the\n    tweet to a model\n2.  In some instances users deliberately misspell words, for example '!'\n    instead of the letter 'l' or 'okayyyyyyyyyy'.\n3.  The majority of the data is in English but not all. If we want a\n    classifier that is multilingual we should collect more data,\n    otherwise we may consider dropping tweets in other languages from\n    our dataset before training our model.\n\nIntegrity \\#3: Property-Label Correlation (Shortcut Learning)\n=============================================================\n\nIn the \\\"Passed\\\" tab we can see tha Property-Label Correlation check,\nthat verifies the data does not contain any shortcuts the model can\nfixate on during the learning process. In our case we can see no\nindication that this problem exists in our dataset. For more information\nabout shortcut learning see:\n<https://towardsdatascience.com/shortcut-learning-how-and-why-models-cheat-1b37575a159>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Test Validation\n=====================\n\nThe next suite, the\n`train_test_validation <deepchecks.nlp.suites>`{.interpreted-text\nrole=\"mod\"} suite serves to validate our split and compare the two\ndataset. These splits can be either you training and val / test sets, in\nwhich case you\\'d want to run this suite after the split was made but\nbefore training, or for example your training and inference data, in\nwhich case the suite is useful for validating that the inference data is\nsimilar enough to the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.suites import train_test_validation\n\ntrain_test_validation().run(train, test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Label Drift\n===========\n\nThis check, appearing in the \\\"Didn\\'t Pass\\\" tab, lets us see that we\nhave some significant change in the distribution of the label - the\nlabel \"optimism\" is suddenly way more common in the test dataset, while\nother labels declined. This happened because we split on time, so the\ntopics covered by the tweets in the test dataset may correspond to\nspecific trends or events that happened later in time. Let's\ninvestigate!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Evaluation\n================\n\nThe suite below, the\n`model_evaluation <deepchecks.nlp.suites>`{.interpreted-text role=\"mod\"}\nsuite, is designed to be run after a model has been trained and requires\nmodel predictions which can be supplied via the relevant arguments in\nthe `run` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_preds, test_preds = tweet_emotion.load_precalculated_predictions(\n    pred_format='predictions', as_train_test=True)\ntrain_probas, test_probas = tweet_emotion.load_precalculated_predictions(\n    pred_format='probabilities', as_train_test=True)\n\nfrom deepchecks.nlp.suites import model_evaluation\n\nresult = model_evaluation().run(train, test,\n                                train_predictions=train_preds,\n                                test_predictions=test_preds,\n                                train_probabilities=train_probas,\n                                test_probabilities=test_probas)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OK! We have many important issues being surfaced by this suite. Let's\ndive into the individual checks:\n\nModel Eval \\#1: Train Test Performance\n======================================\n\nWe can immediately see in the \\\"Didn\\'t Pass\\\" tab that there has been\nsignificant degradation in the Recall on class \"optimism\". This is very\nlikely a result of the severe label drift we saw after running the\nprevious suite.\n\nModel Eval \\#2: Segment Performance\n===================================\n\nAlso in the \\\"Didn\\'t Pass\\\" tab we can see the two segment performance\nchecks - Property Segment Performance and Metadata Segment Performance.\nThese use the `metadata columns <nlp__metadata_guide>`{.interpreted-text\nrole=\"ref\"} of user related information OR our\n`calculated properties <nlp__properties_guide>`{.interpreted-text\nrole=\"ref\"} to try and **automatically** detect significant data\nsegments on which our model performs badly.\n\nIn this case we can see that both checks have found issues in the test\ndataset:\n\n1.  The Property Segment Performance check has found that we're getting\n    very poor results on low toxicity samples. That probably means that\n    our model is using the toxicity of the text to infer the \"anger\"\n    label, and is having a harder problem with other, more benign text\n    samples.\n2.  The Metadata Segment Performance check has found that we have\n    predicting correct results on new users from the Americas. That's 5%\n    of our dataset so we better investigate that further.\n\nYou\\'ll note that these two issues occur only in the test data, and so\nthe results of these checks for the training data appear in the\n\\\"Passed\\\" tab.\n\nModel Eval \\#3: Prediction Drift\n================================\n\nWe note that the Prediction Drift (here in the \"Passed\" tab) shows no\nissue. Given that we already know that there is significant Label Drift,\nthis means we have Concept Drift - the labels corresponding to our\nsamples have changed, while the model continues to predict the same\nlabels. You can learn more about the different types of drift and how\ndeepchecks detects them in our\n`Drift Guide <drift_user_guide>`{.interpreted-text role=\"ref\"}.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running Individual Checks\n=========================\n\nChecks can also be run individually. In this section, we\\'ll show two of\nthe more interesting checks and how you can run them stand-alone and add\nconditions to them. You can learn more about customizing suites, checks\nand conditions in our\n`Customizations Guide <general__customizations>`{.interpreted-text\nrole=\"ref\"}.\n\nEmbeddings Drift\n----------------\n\nIn order to run the\n`Embeddings Drift <nlp__embeddings_drift>`{.interpreted-text role=\"ref\"}\ncheck you must have text embeddings loaded to both datasets. You can\nread more about using embeddings in deepchecks NLP in our\n`Embeddings Guide <nlp__embeddings_guide>`{.interpreted-text\nrole=\"ref\"}.\n\nIn this example, we have the embeddings already pre-calculated:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.datasets.classification.tweet_emotion import load_embeddings\n\ntrain_embeddings, test_embeddings = load_embeddings()\n\ntrain.set_embeddings(train_embeddings)\ntest.set_embeddings(test_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also calculate the embeddings using deepchecks, either using an\nopen-source sentence-transformer or using Open AI's embedding API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# train.calculate_builtin_embeddings()\n# test.calculate_builtin_embeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.checks import TextEmbeddingsDrift\n\ncheck = TextEmbeddingsDrift()\nres = check.run(train, test)\nres.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can see some clusters that distinctly contain more samples from\ntrain or more sample for test. For example, if we look at the greenish\ncluster in the middle (by hovering on the samples and reading the\ntweets) we see it's full of inspirational quotes and sayings, and\nbelongs mostly to the test dataset. That is the source of the drastic\nincrease in optimistic labels!\n\nThere are of course also other note-worthy clusters, such as the\ngreenish cluster on the right that contains tweets about a terror attack\nin Bangladesh, which belongs solely to the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Under Annotated Segments\n========================\n\nAnother note-worthy segment is the\n`Under Annotated Segments <nlp__under_annotated_property_segments>`{.interpreted-text\nrole=\"ref\"} check, which explores our data and automatically identifies\nsegments where the data is under-annotated - meaning that the ratio of\nmissing labels is higher. To this check we'll also add a condition that\nwill fail in case that an under-annotated segment of significant size is\nfound.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.checks import UnderAnnotatedPropertySegments\ntest_under = tweet_emotion.load_under_annotated_data()\n\ncheck = UnderAnnotatedPropertySegments(\n    segment_minimum_size_ratio=0.1\n).add_condition_segments_relative_performance_greater_than()\n\ncheck.run(test_under)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For example, here the check detected that we have a lot of lacking\nannotations for samples that are informal and not very fluent. May it be\nthe case that our annotators have a problem annotating these samples and\nprefer not to deal with them? If these samples are important for use, we\nmay have to put special focus on annotating this segment.\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nYou can find the full list of available NLP checks in the\n`nlp.checks api documentation \u05bf\n<deepchecks.nlp.checks>`{.interpreted-text role=\"mod\"}.\n:::\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}