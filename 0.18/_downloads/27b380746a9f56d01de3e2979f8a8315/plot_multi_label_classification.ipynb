{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Multi Label Classification Quickstart {#nlp__multilabel_quickstart}\n=====================================\n\nDeepchecks NLP tests your models during model development/research and\nbefore deploying to production. Using our testing package reduces model\nfailures and saves tests development time. In this quickstart guide, you\nwill learn how to use the deepchecks NLP package to analyze and evaluate\na text multi label classification task. If you are interested in a\nregular multiclass classification task, you can refer to our\n`Multiclass Quickstart <nlp__multiclass_quickstart>`{.interpreted-text\nrole=\"ref\"}. We will cover the following:\n\n1.  [Creating a TextData object and auto calculating\n    properties](#setting-up)\n2.  [Running the built-in\n    suites](#running-the-deepchecks-default-suites)\n3.  [Running individual checks](#running-individual-checks)\n\nTo run deepchecks for NLP, you need the following for both your train\nand test data:\n\n1.  Your text data - a list of strings, each string is a single sample\n    (can be a sentence, paragraph, document, etc.).\n2.  Your labels and prediction in the\n    `correct format <nlp__supported_text_classification>`{.interpreted-text\n    role=\"ref\"} (Optional).\n3.  `Metadata <nlp__metadata_guide>`{.interpreted-text role=\"ref\"},\n    `Properties <nlp__properties_guide>`{.interpreted-text role=\"ref\"}\n    or `Embeddings <nlp__embeddings_guide>`{.interpreted-text\n    role=\"ref\"} for the provided text data (Optional).\n\nIf you don\\'t have deepchecks installed yet:\n\n``` {.python}\nimport sys\n!{sys.executable} -m pip install 'deepchecks[nlp]' -U --quiet #--user\n```\n\nSome properties calculated by `deepchecks.nlp` require additional\npackages to be installed. You can install them by running:\n\n``` {.python}\nimport sys\n!{sys.executable} -m pip install 'deepchecks[nlp-properties]' -U --quiet #--user\n```\n\nSetting Up\n----------\n\n### Load Data\n\nFor the purpose of this guide, we\\'ll use a small subset of the [just\ndance](https://www.kaggle.com/datasets/renatojmsantos/just-dance-on-youtube)\ncomment analysis dataset. A dataset containing comments, metadata and\nlabels for a multilabel category classification use case on youtube\ncomments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp import TextData\nfrom deepchecks.nlp.datasets.classification import just_dance_comment_analysis\n\ndata = just_dance_comment_analysis.load_data(data_format='DataFrame',\n                                             as_train_test=False)\nmetadata_cols = ['likes', 'dateComment']\ndata.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create TextData Objects\n=======================\n\nDeepchecks\\' `TextData <nlp__textdata_object>`{.interpreted-text\nrole=\"ref\"} object contains the text samples, labels, and possibly also\nproperties and metadata. It stores cache to save time between repeated\ncomputations and contains functionalities for input validations and\nsampling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "label_cols = data.drop(columns=['originalText'] + metadata_cols)\nclass_names = label_cols.columns.to_list()\ndataset = TextData(data['originalText'], label=label_cols.to_numpy().astype(int),\n                   task_type='text_classification',\n                   metadata=data[metadata_cols], categorical_metadata=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculating Properties\n======================\n\nSome of deepchecks\\' checks use properties of the text samples for\nvarious calculations. Deepcheck has a wide variety of such properties,\nsome simple and some that rely on external models and are more heavy to\nrun. In order for deepchecks\\' checks to be able to access the\nproperties, they must be stored within the\n`TextData <nlp__textdata_object>`{.interpreted-text role=\"ref\"} object.\nYou can read more about properties in the\n`Property Guide <nlp__properties_guide>`{.interpreted-text role=\"ref\"}.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# properties can be either calculated directly by Deepchecks\n# or imported from other sources in appropriate format\n\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# dataset.calculate_builtin_properties(include_long_calculation_properties=True, device=device)\n\nproperties = just_dance_comment_analysis.load_properties(as_train_test=False)\ndataset.set_properties(properties, categorical_properties=['Language'])\ndataset.properties.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the deepchecks default suites\n=====================================\n\nDeepchecks comes with a set of pre-built suites that can be used to run\na set of checks on your data, alongside with their default conditions\nand thresholds. You can read more about customizing and creating your\nown suites in the\n`Customizations Guide <general__customizations>`{.interpreted-text\nrole=\"ref\"}. In this guide we\\'ll be using 3 suites - the data integrity\nsuite, the train test validation suite and the model evaluation suite.\nYou can also run all the checks at once using the\n`full_suite <deepchecks.nlp.suites>`{.interpreted-text role=\"mod\"}.\n\nData Integrity\n--------------\n\nWe will start by doing preliminary integrity check to validate the text\nformatting. It is recommended to do this step before your train and\ntest/validation splits and model training as it may imply additional\ndata engineering is required.\n\nWe\\'ll do that using the\n`data_integrity <deepchecks.nlp.suites>`{.interpreted-text role=\"mod\"}\npre-built suite. Note that we are limiting the number of samples to 1000\nin order to get quick high level overview of potential issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.suites import data_integrity\n\ndata_integrity_suite = data_integrity(n_samples=1000)\ndata_integrity_suite.run(dataset, model_classes=class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Integrity \\#1: Unknown Tokens\n=============================\n\nFirst up (in the \"Didn't Pass\" tab) we see that the Unknown Tokens check\nhas returned a problem.\n\nLooking at the result, we can see that it assumed (by default) that\nwe're going to use the bert-base-uncased tokenizer for our NLP model,\nand that if that's the case there are many words in the dataset that\ncontain characters (specifically here emojis) that are unrecognized by\nthe tokenizer. This is an important insight, as bert tokenizers are very\ncommon.\n\nIntegrity \\#2: Conflicting Labels\n=================================\n\nLooking at the Conflicting Labels check result (in the \"Didn't Pass\"\ntab) we can see that there are 2 occurrences of duplicate samples that\nhave different labels. This may suggest a more severe labeling error in\nthe dataset which we would want to explore further.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Test Validation\n=====================\n\nThe next suite, the\n`train_test_validation <deepchecks.nlp.suites>`{.interpreted-text\nrole=\"mod\"} suite serves to validate our split and compare the two\ndataset. These splits can be either you training and val / test sets, in\nwhich case you\\'d want to run this suite after the split was made but\nbefore training, or for example your training and inference data, in\nwhich case the suite is useful for validating that the inference data is\nsimilar enough to the training data.\n\nTo run this suite we\\'ll split the data into train and test/validation\nsets. We\\'ll use a predefined split based on comment dates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.suites import train_test_validation\n\ntrain_ds, test_ds = just_dance_comment_analysis.load_data(\n    data_format='TextData', as_train_test=True,\n    include_embeddings=True, include_properties=True)\ntrain_test_validation(n_samples=1000).run(train_ds, test_ds,\n                                          model_classes=class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Test Validation \\#1: Properties Drift\n===========================================\n\nBased on the different properties we have calculated for the dataset, we\ncan now search for properties whose distribution changes between the\ntrain and test datasets. Changes like this are especially important to\nlook for when monitoring your model over time, as data drift is one of\nthe top reasons why machine learning model's performance degrades over\ntime.\n\nIn our case, we can see that the \"% Special Characters\" and the\n\\\"Formality\\\" property have different distributions between train and\ntest. Drilling further into the results, we can see that the language of\nthe comments in the test set is much less formal and includes more\nspecial characters (possibly emojis?) than the train set. Since this\nchange is quite significant, we may want to consider adding more\ninformal comments containing special characters to the train set before\ntraining (or retraining) our model.\n\nTrain Test Validation \\#2: Embedding Drift\n==========================================\n\nSimilarly to the properties drift, we can also look for embedding drift\nbetween the train and test datasets. The benefit of using embedding on\ntop of the properties is that they are able to detect semantic changes\nin the data.\n\nIn our case, we see there are significant semantic differences between\nthe train and test sets. Specifically, we can see some clusters that\ndistinctly contain more samples from train or more samples from the\ntrain dataset or more sample from the test dataset. By hovering over the\nclusters we can read the user comments understand what is the difference\nbetween the clusters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Evaluation\n================\n\nThe suite below, the\n`model_evaluation <deepchecks.nlp.suites>`{.interpreted-text role=\"mod\"}\nsuite, is designed to be run after a model has been trained and requires\nmodel predictions which can be supplied via the relevant arguments in\nthe `run` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_preds, test_preds = just_dance_comment_analysis.\\\n    load_precalculated_predictions(pred_format='predictions',\n                                   as_train_test=True)\ntrain_probas, test_probas = just_dance_comment_analysis.\\\n    load_precalculated_predictions(pred_format='probabilities',\n                                   as_train_test=True)\n\nfrom deepchecks.nlp.suites import model_evaluation\n\nsuite = model_evaluation(n_samples=1000)\nresult = suite.run(train_ds, test_ds,\n                   train_predictions=train_preds,\n                   test_predictions=test_preds,\n                   train_probabilities=train_probas,\n                   test_probabilities=test_probas,\n                   model_classes=class_names)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Eval \\#1: Train Test Performance\n======================================\n\nWe can immediately see in the \\\"Didn\\'t Pass\\\" tab that there has been\nsignificant degradation in the Recall on class \"Pain and Discomfort\".\nMoreover, it seems there is a general deterioration in our model\nperformance on the test set compared to the train set. This can be\nexplained based on the data drift we saw in the previous suite.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running Individual Checks\n=========================\n\nChecks can also be run individually as well as within a suite. You can\nlearn more about customizing suites, checks and conditions in our\n`Customizations Guide <general__customizations>`{.interpreted-text\nrole=\"ref\"}. In this section, we\\'ll show you how to do that while\nshowcasing one of our most interesting checks -\n`PropertySegmentPerformance\n<nlp__property_segments_performance>`{.interpreted-text role=\"ref\"}.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.checks import PropertySegmentsPerformance\n\ncheck = PropertySegmentsPerformance(segment_minimum_size_ratio=0.05)\ncheck = check.add_condition_segments_relative_performance_greater_than(0.1)\nresult = check.run(test_ds, probabilities=test_probas)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the display we can see some distinct property based segments that our\nmodel under performs on.\n\nBy reviewing the results we can see that our model is performing poorly\non samples that have a low level of Subjectivity, by looking at the\n\\\"Subjectivity vs Average Sentence Length\\\" tab We can see that the\nproblem is even more severe on samples containing long sentences.\n\nIn addition to the visual display, most checks also return detailed data\ndescribing the results. This data can be used for further analysis,\ncreate custom visualizations or to set custom conditions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.value['weak_segments_list'].head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can find the full list of available NLP checks in the\n`nlp.checks api documentation \u05bf <deepchecks.nlp.checks>`{.interpreted-text\nrole=\"mod\"}.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}