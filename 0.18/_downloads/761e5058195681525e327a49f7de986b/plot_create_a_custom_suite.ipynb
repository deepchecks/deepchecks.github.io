{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a Custom Suite {#create_custom_suite}\n=====================\n\nA suite is a list of checks that will run one after the other, and its\nresults will be displayed together.\n\nTo customize a suite, we can either:\n\n-   [Create new custom suites](#create-a-new-suite), by choosing the\n    checks (and the optional conditions) that we want the suite to have.\n-   [Modify a built-in suite](#modify-an-existing-suite) by adding\n    and/or removing checks and conditions, to adapt it to our needs.\n\nCreate a New Suite\n------------------\n\nLet\\'s say we want to create our custom suite, mainly with various\nperformance checks, including\n`PerformanceReport(), TrainTestDifferenceOverfit()` and several more.\n\nFor assistance in understanding which checks are implemented and can be\nincluded, we suggest using any of:\n\n-   `API Reference </api/index>`{.interpreted-text role=\"doc\"}\n-   `Tabular checks <tabular__checks_gallery>`{.interpreted-text\n    role=\"ref\"}\n-   `Vision checks <vision__checks_gallery>`{.interpreted-text\n    role=\"ref\"}\n-   `NLP checks <nlp__checks_gallery>`{.interpreted-text role=\"ref\"}\n-   Built-in suites (by printing them to see which checks they include)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer, precision_score, recall_score\n\nfrom deepchecks.tabular import Suite\n# importing all existing checks for demonstration simplicity\nfrom deepchecks.tabular.checks import *\n\n# The Suite's first argument is its name, and then all of the check objects.\n# Some checks can receive arguments when initialized (all check arguments have default values)\n# Each check can have an optional condition(/s)\n# Multiple conditions can be applied subsequentially\nnew_custom_suite = Suite('Simple Suite For Model Performance',\n                         ModelInfo(),\n                         # use custom scorers for performance report:\n                         TrainTestPerformance().add_condition_train_test_relative_degradation_less_than(threshold=0.15)\\\n                         .add_condition_test_performance_greater_than(0.8),\n                         ConfusionMatrixReport(),\n                         SimpleModelComparison(strategy='most_frequent',\n                                               scorers={'Recall (Multiclass)': make_scorer(recall_score, average=None),\n                                                        'Precision (Multiclass)': make_scorer(precision_score, average=None)}\n                                               ).add_condition_gain_greater_than(0.3)\n                         )\n\n# The scorers' parameter can also be passed to the suite in order to override the scorers of all the checks\n# in the suite. See :ref:`metrics_user_guide` for further details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s see the suite:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_custom_suite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*TIP: the auto-complete may not work from inside a new suite definition,\nso if you want to use the auto-complete to see the arguments a check\nreceive or the built-in conditions it has, try doing it outside of the\nsuite\\'s initialization.*\n\n-   For example, to see a check\\'s built-in conditions, type in a new\n    cell: `NameOfDesiredCheck().add_condition_` and then check the\n    auto-complete suggestions (using Shift + Tab), to discover the\n    built-in checks.\\*\n\nAdditional Notes about Conditions in a Suite\n============================================\n\n-   Checks in the built-in suites come with pre-defined conditions, and\n    when building your custom suite you should choose which conditions\n    to add.\n-   Most check classes have built-in methods for adding conditions.\n    These apply to the naming convention `add_condition_...`, which\n    enables adding a condition logic to parse the check\\'s results.\n-   Each check instance can have several conditions or none. Each\n    condition will be evaluated separately.\n-   The pass (\u2713) / fail (\u2716) / insight (!) status of the conditions,\n    along with the condition\\'s name and extra info will be displayed in\n    the suite\\'s Conditions Summary.\n-   Most conditions have configurable arguments that can be passed to\n    the condition while adding it.\n-   For more info about conditions, check out `Configure a Condition\n    <plot_configure_check_conditions>`{.interpreted-text role=\"doc\"}.\n\nRun the Suite\n-------------\n\nThis is simply done by calling the `run()` method of the suite.\n\nTo see that in action, we\\'ll need datasets and a model.\n\nLet\\'s quickly load a dataset and train a simple model for the sake of\nthis demo\n\nLoad Datasets and Train a Simple Model\n======================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n# General imports\nimport pandas as pd\n\nnp.random.seed(22)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nfrom deepchecks.tabular.datasets.classification import iris\n\n# Load pre-split Datasets\ntrain_dataset, test_dataset = iris.load_data(as_train_test=True)\nlabel_col = 'target'\n\n# Train Model\nrf_clf = RandomForestClassifier()\nrf_clf.fit(train_dataset.data[train_dataset.features],\n           train_dataset.data[train_dataset.label_name]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run Suite\n=========\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_custom_suite.run(model=rf_clf, train_dataset=train_dataset, test_dataset=test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modify an Existing Suite\n========================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.suites import train_test_validation\n\ncustomized_suite = train_test_validation()\n\n# let's check what it has:\ncustomized_suite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# and modify it by removing a check by index:\ncustomized_suite.remove(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.tabular.checks import UnusedFeatures\n\n# and add a new check with a condition:\ncustomized_suite.add(\n    UnusedFeatures().add_condition_number_of_high_variance_unused_features_less_or_equal())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# lets remove all condition for the FeatureLabelCorrelationChange:\ncustomized_suite[3].clean_conditions()\n\n# and update the suite's name:\ncustomized_suite.name = 'New Data Leakage Suite'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# and now we can run our modified suite:\ncustomized_suite.run(train_dataset, test_dataset, rf_clf)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}