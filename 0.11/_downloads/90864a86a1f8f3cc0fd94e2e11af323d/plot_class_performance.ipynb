{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Class Performance {#plot_vision_class_performance}\n=================\n\nThis notebooks provides an overview for using and understanding the\nclass performance check.\n\n**Structure:**\n\n-   [What is the purpose of the\n    check?](#what-is-the-purpose-of-the-check)\n-   [Classification](#classification-performance-report)\n    -   [Generate Dataset](#generate-dataset)\n    -   [Run the check](#run-the-check)\n-   [Object Detection](#object-detection-class-performance)\n    -   [Generate data & model](#id1)\n    -   [Run the check](#id2)\n\nWhat Is the Purpose of the Check?\n---------------------------------\n\nThe class performance check evaluates several metrics on the given model\nand data and returns all of the results in a single check. The check\nuses the following default metrics:\n\n  Task Type          Property name\n  ------------------ ----------------------------------------------------------------------------------------------------------------------------\n  Classification     Precision\n  Classification     Recall\n  Object Detection   [Average Precision](https://manalelaidouni.github.io/Evaluating-Object-Detection-Models-Guide-to-Performance-Metrics.html)\n  Object Detection   [Average Recall](https://manalelaidouni.github.io/Evaluating-Object-Detection-Models-Guide-to-Performance-Metrics.html)\n\nIn addition to the default metrics, the check supports custom metrics,\nas detailed in the\n`Metrics Guide </user-guide/general/metrics_guide>`{.interpreted-text\nrole=\"doc\"}. These can be passed as a list using the scorers parameter\nof the check, which will override the default metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports\n=======\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nIn this example, we use the pytorch version of the mnist dataset and\nmodel. In order to run this example using tensorflow, please change the\nimport statements to:\n\nfrom deepchecks.vision.datasets.classification import mnist\\_tensorflow\nas mnist\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.checks import ClassPerformance\nfrom deepchecks.vision.datasets.classification import mnist_torch as mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classification Performance Report\n=================================\n\nGenerate Dataset\n----------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_ds = mnist.load_dataset(train=True, object_type='VisionData')\ntest_ds = mnist.load_dataset(train=False, object_type='VisionData')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = ClassPerformance()\nresult = check.run(train_ds, test_ds)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To display the results in an IDE like PyCharm, you can use the following\ncode:\n\n``` {.python}\nresult.show_in_window()\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result will be displayed in a new window.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Object Detection Class Performance\n==================================\n\nFor object detection tasks - the default metric that is being calculated\nit the Average Precision. The definition of the Average Precision is\nidentical to how the COCO dataset defined it - mean of the average\nprecision per class, over the range \\[0.5, 0.95, 0.05\\] of IoU\nthresholds.\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nIn this example, we use the pytorch version of the coco dataset and\nmodel. In order to run this example using tensorflow, please change the\nimport statements to:\n\nfrom deepchecks.vision.datasets.detection import coco\\_tensorflow as\ncoco\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.datasets.detection import coco_torch as coco"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate Dataset\n================\n\nWe generate a sample dataset of 128 images from the [COCO\ndataset](https://cocodataset.org/#home), and using the [YOLOv5\nmodel](https://github.com/ultralytics/yolov5).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_ds = coco.load_dataset(train=True, object_type='VisionData')\ntest_ds = coco.load_dataset(train=False, object_type='VisionData')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = ClassPerformance(show_only='best')\nresult = check.run(train_ds, test_ds)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you have a GPU, you can speed up this check by calling:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# check.run(train_ds, test_ds, yolo, device=<your GPU>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To display the results in an IDE like PyCharm, you can use the following\ncode:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# result.show_in_window()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result will be displayed in a new window.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a Condition\n==================\n\nWe can also define a condition to validate that our model performance is\nabove a certain threshold. The condition is defined as a function that\ntakes the results of the check as input and returns a ConditionResult\nobject.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check = ClassPerformance(show_only='worst')\ncheck.add_condition_test_performance_greater_than(0.2)\nresult = check.run(train_ds, test_ds)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We detected that for several classes our model performance is below the\nthreshold.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}