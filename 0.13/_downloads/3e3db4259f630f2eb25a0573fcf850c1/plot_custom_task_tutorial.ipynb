{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Custom Task Tutorial\n====================\n\nComputer vision is an umbrella term for a wide spectrum of objectives\nmodels are trained for. These objective reflect on the structure of the\ndata and the possible actions on it.\n\nThe first step before running any Deepchecks checks is to create an\nimplementation of\n`deepchecks.vision.vision_data.vision_data.VisionData`{.interpreted-text\nrole=\"class\"}. Each implementation represents and standardize a computer\nvision task and allows to run a more complex checks which relates to the\ngiven task\\'s characteristics. There are default base classes for a few\nknown tasks like classification, object detection, and semantic\nsegmentation however not all tasks have a base implementation, meaning\nyou will have to create your own task.\n\nWhen creating your own task you will be limited to run checks which are\nagnostic to the specific task type. For example performance checks that\nuses IOU works only on object detection tasks, since they need to know\nthe exact bounding box format in order to run, while other checks that\nuses `/user-guide/vision/vision_properties`{.interpreted-text\nrole=\"doc\"} or custom metrics are agnostic to the task type.\n\nIn this guide we will implement a custom instance segmentation task and\nrun checks on it. Note that instance segmentation is different from\nsemantic segmentation, which is currently supported in Deepchecks.\n\n1.  [Defining the Data](#defining-the-data)\n2.  [Implement Custom Task](#implement-custom-task)\n3.  [Implement Custom Properties](#implement-custom-properties)\n4.  [Implement Custom Metric](#implement-custom-metric)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the Data\n=================\n\nFirst we will define a [PyTorch\nDataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\nof COCO-128 segmentation task. This part represents your own code, and\nis not yet Deepchecks related.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import contextlib\nimport os\nimport typing as t\nfrom pathlib import Path\n\nimport albumentations as A\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torchvision.transforms.functional as F\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom PIL import Image, ImageDraw\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import VisionDataset\nfrom torchvision.datasets.utils import download_and_extract_archive\nfrom torchvision.utils import draw_segmentation_masks\n\n\nclass CocoInstanceSegmentationDataset(VisionDataset):\n    \"\"\"An instance of PyTorch VisionData the represents the COCO128-segments dataset.\n\n    Parameters\n    ----------\n    root : str\n        Path to the root directory of the dataset.\n    name : str\n        Name of the dataset.\n    train : bool\n        if `True` train dataset, otherwise test dataset\n    transforms : Callable, optional\n        A function/transform that takes in an PIL image and returns a transformed version.\n        E.g, transforms.RandomCrop\n    \"\"\"\n\n    TRAIN_FRACTION = 0.5\n\n    def __init__(self, root: str, name: str, train: bool = True, transforms: t.Optional[t.Callable] = None, ) -> None:\n        super().__init__(root, transforms=transforms)\n\n        self.train = train\n        self.root = Path(root).absolute()\n        self.images_dir = Path(root) / 'images' / name\n        self.labels_dir = Path(root) / 'labels' / name\n\n        images: t.List[Path] = sorted(self.images_dir.glob('./*.jpg'))\n        labels: t.List[t.Optional[Path]] = []\n\n        for image in images:\n            label = self.labels_dir / f'{image.stem}.txt'\n            labels.append(label if label.exists() else None)\n\n        assert len(images) != 0, 'Did not find folder with images or it was empty'\n        assert not all(l is None for l in labels), 'Did not find folder with labels or it was empty'\n\n        train_len = int(self.TRAIN_FRACTION * len(images))\n\n        if self.train is True:\n            self.images = images[0:train_len]\n            self.labels = labels[0:train_len]\n        else:\n            self.images = images[train_len:]\n            self.labels = labels[train_len:]\n\n    def __getitem__(self, idx: int) -> t.Tuple[Image.Image, np.ndarray]:\n        \"\"\"Get the image and label at the given index.\"\"\"\n        image = Image.open(str(self.images[idx]))\n        label_file = self.labels[idx]\n\n        masks = []\n        if label_file is not None:\n            for label_str in label_file.open('r').read().strip().splitlines():\n                label = np.array(label_str.split(), dtype=np.float32)\n                class_id = int(label[0])\n                # Transform normalized coordinates to un-normalized\n                coordinates = (label[1:].reshape(-1, 2) * np.array([image.width, image.height])).reshape(-1).tolist()\n                # Create mask image\n                mask = Image.new('L', (image.width, image.height), 0)\n                ImageDraw.Draw(mask).polygon(coordinates, outline=1, fill=1)\n                # Add to list\n                masks.append(np.array(mask, dtype=bool))\n\n        if self.transforms is not None:\n            # Albumentations accepts images as numpy\n            transformed = self.transforms(image=np.array(image), masks=masks)\n            image = transformed['image']\n            masks = transformed['masks']\n            # Transform masks to tensor of (num_masks, H, W)\n            if masks:\n                if isinstance(masks[0], np.ndarray):\n                    masks = [torch.from_numpy(m) for m in masks]\n                masks = torch.stack(masks)\n            else:\n                masks = torch.empty((0, 3))\n\n        return image, masks\n\n    def __len__(self):\n        return len(self.images)\n\n    @classmethod\n    def load_or_download(cls, root: Path, train: bool) -> 'CocoInstanceSegmentationDataset':\n        extract_dir = root / 'coco128segments'\n        coco_dir = root / 'coco128segments' / 'coco128-seg'\n        folder = 'train2017'\n\n        if not coco_dir.exists():\n            url = 'https://ultralytics.com/assets/coco128-segments.zip'\n\n            with open(os.devnull, 'w', encoding='utf8') as f, contextlib.redirect_stdout(f):\n                download_and_extract_archive(url, download_root=str(root), extract_root=str(extract_dir))\n\n            try:\n                # remove coco128's README.txt so that it does not come in docs\n                os.remove(\"coco128/README.txt\")\n            except:\n                pass\n        return CocoInstanceSegmentationDataset(coco_dir, folder, train=train, transforms=A.Compose([ToTensorV2()]))\n\n\n# Download and load the datasets\ntrain_ds = CocoInstanceSegmentationDataset.load_or_download(Path('.'), train=True)\ntest_ds = CocoInstanceSegmentationDataset.load_or_download(Path('.'), train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizing that we loaded our datasets correctly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "masked_images = [draw_segmentation_masks(train_ds[i][0], masks=train_ds[i][1], alpha=0.7) for i in range(5)]\n\nfig, axs = plt.subplots(ncols=len(masked_images), figsize=(20, 20))\nfor i, img in enumerate(masked_images):\n    img = img.detach()\n    img = F.to_pil_image(img)\n    axs[i].imshow(np.asarray(img))\n    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\nfig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implementing the VisionData class\n=================================\n\nThe checks in the package validate the data by calculating various\nquantities over the data, labels and predictions (when available). In\norder to do that, those must be in a pre-defined format, according to\nthe task type.\n\nIn the following example we\\'re using pytorch. To see how this can be\ndone using tensorflow or a generic generator, please refer to\n`creating VisionData guide </user-guide/vision/VisionData>`{.interpreted-text\nrole=\"doc\"}.\n\nFor pytorch, we will use our DataLoader, but we\\'ll create a new collate\nfunction for it, that transforms the batch to the correct format. Then,\nwe\\'ll create a\n`deepchecks.vision.vision_data.vision_data.VisionData`{.interpreted-text\nrole=\"class\"} object, that will hold the data loader.\n\nFor a custom task, only the images have a pre-defined format while the\nlabels and predictions can arrive in any format. To learn more about the\nexpected formats for the different tasks please visit\n`supported tasks and formats guide </user-guide/vision/supported_tasks_and_formats>`{.interpreted-text\nrole=\"doc\"}.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision import VisionData, BatchOutputFormat\n\n\ndef deepchecks_collate_fn(batch) -> BatchOutputFormat:\n    \"\"\"Return a batch of images, labels and predictions for a batch of data. The expected format is a dictionary with\n    the following keys: 'images', 'labels' and 'predictions', each value is in the deepchecks format for the task.\n    You can also use the BatchOutputFormat class to create the output.\n    \"\"\"\n    # batch received as iterable of tuples of (image, label) and transformed to tuple of iterables of images and labels:\n    batch = tuple(zip(*batch))\n\n    images = [tensor.numpy().transpose((1, 2, 0)) for tensor in batch[0]]\n    labels = batch[1]\n    return BatchOutputFormat(images=images, labels=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The label\\_map is a dictionary that maps the class id to the class name,\nfor display purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "LABEL_MAP = {0: 'background', 1: 'airplane', 2: 'bicycle', 3: 'bird', 4: 'boat', 5: 'bottle', 6: 'bus', 7: 'car',\n             8: 'cat', 9: 'chair', 10: 'cow', 11: 'dining table', 12: 'dog', 13: 'horse', 14: 'motorcycle',\n             15: 'person', 16: 'potted plant', 17: 'sheep', 18: 'couch', 19: 'train', 20: 'tv'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our updated collate function, we can create the\ndataloader in the deepchecks format, and use it to create a VisionData\nobject. For custom tasks, we set the task type to \\'other\\':\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset=train_ds, batch_size=16, shuffle=False, collate_fn=deepchecks_collate_fn)\ntest_loader = DataLoader(dataset=test_ds, batch_size=16, shuffle=False, collate_fn=deepchecks_collate_fn)\n\ntrain_vision_data = VisionData(batch_loader=train_loader, task_type='other', label_map=LABEL_MAP)\ntest_vision_data = VisionData(batch_loader=test_loader, task_type='other', label_map=LABEL_MAP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running Checks\n==============\n\nAfter the vision data objects were created, we can run checks on them.\nFor custom tasks, since the images are in the standard Deepchecks\nformat, we can run image based checks without additional effort. Let\\'s\nrun the ImagePropertyDrift check with our task:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.checks import ImagePropertyDrift\n\nresult = ImagePropertyDrift().run(train_vision_data, test_vision_data)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now in order to run additional checks, we\\'ll need to define custom\nproperties or metrics.\n\nImplement Custom Properties\n===========================\n\nIn order to run checks that are using label or prediction properties\nwe\\'ll have to implement a custom\n`property </user-guide/vision/vision_properties>`{.interpreted-text\nrole=\"doc\"}. We\\'ll write label properties and run the label drift\ncheck.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.checks import LabelDrift\n\n\ndef number_of_detections(labels) -> t.List[int]:\n    \"\"\"Return a list containing the number of detections per sample in batch.\"\"\"\n    return [masks_per_image.shape[0] for masks_per_image in labels]\n\n\n# We will pass this object as parameter to checks that are using label properties\nlabel_properties = [{'name': '# Detections per image', 'method': number_of_detections, 'output_type': 'numerical'}]\ncheck = LabelDrift(label_properties=label_properties)\nresult = check.run(train_vision_data, test_vision_data)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implement Custom Metric\n=======================\n\nSome checks test the model performance and requires a metric in order to\nrun. When using a custom task you will also have to create a custom\nmetric in order for those checks to work, since the Deepchecks\\'\nbuilt-in metrics don\\'t know to handle custom data formats. See\n`link <metrics_guide__custom_metrics>`{.interpreted-text role=\"ref\"} for\nadditional information on how to create a custom metric.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}