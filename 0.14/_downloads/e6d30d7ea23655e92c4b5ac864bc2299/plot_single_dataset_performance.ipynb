{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Single Dataset Performance {#nlp__single_dataset_performance}\n==========================\n\nThis notebook provides an overview for using and understanding the\nsingle dataset performance check for NLP tasks.\n\n**Structure:**\n\n-   [What is the purpose of the\n    check?](#what-is-the-purpose-of-the-check)\n-   [Generate data & model](#generate-data-model)\n-   [Run the check](#run-the-check)\n-   [Define a condition](#define-a-condition)\n\nWhat is the purpose of the check?\n---------------------------------\n\nThis check is designed for evaluating a model\\'s performance on a\nlabeled dataset based on a scorer or multiple scorers.\n\nFor Text Classification tasks the supported metrics are sklearn scorers.\nYou may use any of the existing sklearn scorers or create your own. For\nmore information about the supported sklearn scorers, defining your own\nmetrics and to learn how to use metrics for other supported task types,\nsee the `metrics_user_guide`{.interpreted-text role=\"ref\"}.\n\nThe default scorers that are used for are F1, Precision, and Recall for\nClassification, and F1 Macro, Recall Macro and Precision Macro for Token\nClassification. See more about the supported task types at the\n`nlp__supported_tasks`{.interpreted-text role=\"ref\"} guide.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate data & model\n=====================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.datasets.classification.tweet_emotion import load_data, load_precalculated_predictions\n\n_, test_dataset = load_data(data_format='TextData')\n_, test_probas = load_precalculated_predictions(pred_format='probabilities')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the check\n=============\n\nYou can select which scorers to use by passing either a list or a dict\nof scorers to the check, see `metrics_user_guide`{.interpreted-text\nrole=\"ref\"} for additional details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.nlp.checks import SingleDatasetPerformance\n\ncheck = SingleDatasetPerformance(scorers=['recall_per_class', 'precision_per_class', 'f1_macro', 'f1_micro'])\nresult = check.run(dataset=test_dataset, probabilities=test_probas)\nresult.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a condition\n==================\n\nWe can define on our check a condition to validate that the different\nmetric scores are above a certain threshold. Using the `class_mode`\nargument we can define select a sub set of the classes to use for the\ncondition.\n\nLet\\'s add a condition to the check and see what happens when it fails:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check.add_condition_greater_than(threshold=0.85, class_mode='all')\nresult = check.run(dataset=test_dataset, probabilities=test_probas)\nresult.show(show_additional_outputs=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We detected that the Recall score is below specified threshold in at\nleast one of the classes.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}